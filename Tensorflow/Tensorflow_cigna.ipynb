{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tensorflow_cigna.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fyyn7W5nxFFQ",
        "colab_type": "text"
      },
      "source": [
        "Tensorflow 初探"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OdtL-aFbg_Rp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oOqxIl0Qhcrf",
        "colab_type": "code",
        "outputId": "abeb6a75-62b3-4ae8-e9be-703759748bb3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 76
        }
      },
      "source": [
        "print(tf.__version__)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.13.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IGbQ1u0diGze",
        "colab_type": "code",
        "outputId": "80245a1f-36c1-4368-a301-f238661a251e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 96
        }
      },
      "source": [
        "tf.test.gpu_device_name()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/device:GPU:0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2IG9RXSNjqA9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def NN(m1,m2,w1,w2,b):\n",
        "  z = m1 * w1 + m2 * w2 + b\n",
        "  return sigmoid(z)\n",
        "\n",
        "def sigmoid(x):\n",
        "  return 1/(1+np.exp(-x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dJFE7oiniHYH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MlYL6Mu-iAWo",
        "colab_type": "code",
        "outputId": "69a274d2-a775-428d-f87a-960e1c276d6a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "w1=np.random.randn()\n",
        "w2=np.random.randn()\n",
        "b=np.random.randn()\n",
        "phrases = [\"看起來像\",\"我猜是\",\"我想是\",\"可能是\",\"看起來像是\"]\n",
        "data = [[3,1.5,1],[2,1,0],[4,1.5,1],[3.5,5,0],[2.0,5,1]]\n",
        "rand_data = data[np.random.randint(len(data))]\n",
        "m1 = rand_data[0]\n",
        "m2 = rand_data[1]\n",
        "prediction = NN(m1,m2,w1,w2,b)\n",
        "prediction_text = ['藍色','紅色'][int(np.round(prediction))]\n",
        "phrase = np.random.choice(phrases) + \"\" + prediction_text\n",
        "o = '這個' + phrase + '真的是' + ['藍色','紅色'][rand_data[2]]\n",
        "o"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'這個看起來像藍色真的是紅色'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ep5fVitJhczY",
        "colab_type": "code",
        "outputId": "d50d05ea-9a46-4ab2-9c23-265ba18aceb9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 270
        }
      },
      "source": [
        "#Example NN\n",
        "n_features = 3\n",
        "n_dense_neurons = 3\n",
        "tf.set_random_seed(101)\n",
        "\n",
        "x = tf.placeholder(tf.float32, (None, n_features))\n",
        "b = tf.Variable(tf.zeros([n_dense_neurons]))\n",
        "W = tf.Variable(tf.random_normal([n_features,n_dense_neurons]))\n",
        "xW = tf.matmul(x,W)\n",
        "z = tf.add(xW, b)\n",
        "a = tf.sigmoid(z)\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "with tf.Session() as sess:\n",
        "  sess.run(init)\n",
        "  layer_out = sess.run(a,feed_dict = {x : np.random.random([5,n_features])})\n",
        "print(layer_out)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "[[0.37180263 0.4848752  0.40013963]\n",
            " [0.37374812 0.5469098  0.48332396]\n",
            " [0.29870242 0.51684254 0.45256433]\n",
            " [0.3235687  0.45716134 0.43859345]\n",
            " [0.43333548 0.42000324 0.42359108]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hn2ONXskhc17",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = tf.constant([[1,0,1,0],[1,0,1,1],[0,1,0,1]], dtype=tf.float32)\n",
        "y = tf.constant([[1],[1],[0]],dtype=tf.float32)\n",
        "def sigmoid(x):\n",
        "  return 1/(1+tf.exp(-x))\n",
        "def derivatives_sigmoid(x):\n",
        "    return x * (1 - x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NeJMmK1xqDpD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epoch = 1000\n",
        "lr = 0.1\n",
        "inputlayer_neurons = X.shape[1]\n",
        "hiddenlayer_neurons = 10\n",
        "output_neurons = 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y26cV8p2qDq-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "wh = tf.random_normal(shape=[int(inputlayer_neurons),int(hiddenlayer_neurons)],\n",
        "                      mean=0.0,stddev=1.0,\n",
        "                      dtype=tf.float32,seed=None,name=None)\n",
        "bh =  tf.random_normal(shape=[1,int(hiddenlayer_neurons)],\n",
        "                      mean=0.0,stddev=1.0,\n",
        "                      dtype=tf.float32,seed=None,name=None)\n",
        "wout =  tf.random_normal(shape=[int(hiddenlayer_neurons),int(output_neurons)],\n",
        "                      mean=0.0,stddev=1.0,\n",
        "                      dtype=tf.float32,seed=None,name=None)\n",
        "bout =  tf.random_normal(shape=[1,int(output_neurons)],\n",
        "                      mean=0.0,stddev=1.0,\n",
        "                      dtype=tf.float32,seed=None,name=None)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NATrfKTxqDt8",
        "colab_type": "code",
        "outputId": "f908ee1a-a22e-479c-db32-2c2cd1f7d303",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        }
      },
      "source": [
        "for i in range(epoch):\n",
        "  hidden_layer_input1 = tf.matmul(X, wh)\n",
        "  hidden_layer_input = hidden_layer_input1 + bh\n",
        "  hidden_layer_activations = sigmoid(hidden_layer_input)\n",
        "  output_layer_input1 = tf.matmul(hidden_layer_activations, wout)\n",
        "  output_layer_input = output_layer_input1 + bout\n",
        "  output = sigmoid(output_layer_input)\n",
        "  \n",
        "  E= y-output\n",
        "  back_output_layer = derivatives_sigmoid(output)\n",
        "  back_hidden_layer = derivatives_sigmoid(hidden_layer_activations)\n",
        "  d_output = E * back_output_layer\n",
        "  Error_at_hidden_layer = tf.matmul(d_output, tf.transpose(wout))\n",
        "  d_hiddenlayer = Error_at_hidden_layer * back_hidden_layer\n",
        "  wout += tf.matmul(tf.transpose(hidden_layer_activations), d_output) * lr \n",
        "  bout += tf.reduce_sum(d_output) * lr\n",
        "  wh += tf.matmul(tf.transpose(X), d_hiddenlayer) * lr\n",
        "  bh += tf.reduce_sum(d_output)*lr\n",
        "  \n",
        "sess = tf.Session()\n",
        "print('Actual :\\n', sess.run(y), '\\n')\n",
        "print('Predicted :\\n', sess.run(output), '\\n')\n",
        "  \n",
        "  "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Actual :\n",
            " [[1.]\n",
            " [1.]\n",
            " [0.]] \n",
            "\n",
            "Predicted :\n",
            " [[0.9697147 ]\n",
            " [0.92694074]\n",
            " [0.09504487]] \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sT89A9QU4I3w",
        "colab_type": "code",
        "outputId": "afffa3ee-6a20-40af-c289-c4ad5f532906",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 692
        }
      },
      "source": [
        "!pip install tensorflow==2.0.0-rc0 \n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==2.0.0-rc0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fb/4b/77f0965ec7e8a76d3dcd6a22ca8bbd2b934cd92c4ded43fef6bea5ff3258/tensorflow-2.0.0rc0-cp36-cp36m-manylinux2010_x86_64.whl (86.3MB)\n",
            "\u001b[K     |████████████████████████████████| 86.3MB 444kB/s \n",
            "\u001b[?25hRequirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-rc0) (0.1.7)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-rc0) (1.16.4)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-rc0) (3.7.1)\n",
            "Collecting opt-einsum>=2.3.2 (from tensorflow==2.0.0-rc0)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c0/1a/ab5683d8e450e380052d3a3e77bb2c9dffa878058f583587c3875041fb63/opt_einsum-3.0.1.tar.gz (66kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 34.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-rc0) (1.12.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-rc0) (0.8.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-rc0) (1.1.0)\n",
            "Collecting tb-nightly<1.15.0a20190807,>=1.15.0a20190806 (from tensorflow==2.0.0-rc0)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bc/88/24b5fb7280e74c7cf65bde47c171547fd02afb3840cff41bcbe9270650f5/tb_nightly-1.15.0a20190806-py3-none-any.whl (4.3MB)\n",
            "\u001b[K     |████████████████████████████████| 4.3MB 42.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-rc0) (0.7.1)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-rc0) (1.11.2)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-rc0) (1.15.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-rc0) (1.0.8)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-rc0) (1.1.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-rc0) (0.33.4)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-rc0) (0.2.2)\n",
            "Collecting tf-estimator-nightly<1.14.0.dev2019080602,>=1.14.0.dev2019080601 (from tensorflow==2.0.0-rc0)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/21/28/f2a27a62943d5f041e4a6fd404b2d21cb7c59b2242a4e73b03d9ba166552/tf_estimator_nightly-1.14.0.dev2019080601-py2.py3-none-any.whl (501kB)\n",
            "\u001b[K     |████████████████████████████████| 501kB 61.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow==2.0.0-rc0) (41.2.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.15.0a20190807,>=1.15.0a20190806->tensorflow==2.0.0-rc0) (0.15.5)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.15.0a20190807,>=1.15.0a20190806->tensorflow==2.0.0-rc0) (3.1.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow==2.0.0-rc0) (2.8.0)\n",
            "Building wheels for collected packages: opt-einsum\n",
            "  Building wheel for opt-einsum (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for opt-einsum: filename=opt_einsum-3.0.1-cp36-none-any.whl size=58500 sha256=a4298c376426791461db08199b086d69e68d6c5e0692ddfa75c7236a2e46a630\n",
            "  Stored in directory: /root/.cache/pip/wheels/91/98/8d/10e3d4e04c959597a411b91acd3695e9e2d210e68ce3427aad\n",
            "Successfully built opt-einsum\n",
            "Installing collected packages: opt-einsum, tb-nightly, tf-estimator-nightly, tensorflow\n",
            "  Found existing installation: tensorflow 1.14.0\n",
            "    Uninstalling tensorflow-1.14.0:\n",
            "      Successfully uninstalled tensorflow-1.14.0\n",
            "Successfully installed opt-einsum-3.0.1 tb-nightly-1.15.0a20190806 tensorflow-2.0.0rc0 tf-estimator-nightly-1.14.0.dev2019080601\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iFdHofKzqDw5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pcx2ZmGJ4OJD",
        "colab_type": "code",
        "outputId": "bf62b5a9-4cdb-47ee-bb25-3a113192efc1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "tf.__version__"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.0.0-rc0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sjam3_iWqDy0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = load_iris()\n",
        "x_train, x_test, y_train, y_test= train_test_split(data.data, data.target, test_size=0.3, random_state=42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FAawHwqlqD03",
        "colab_type": "code",
        "outputId": "6e8e7b66-7ed9-4073-9810-c1ee2b1f6e6c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(500, activation=tf.nn.relu),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
        "])\n",
        "adam = tf.keras.optimizers.Adam(lr=0.001)\n",
        "model.compile(optimizer=adam,\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "#model.fit(x_train, y_train, epochs=50)\n",
        "history = model.fit(x_train, y_train, epochs=100, validation_split=0.3)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 73 samples, validate on 32 samples\n",
            "Epoch 1/100\n",
            "73/73 [==============================] - 0s 6ms/sample - loss: 2.2660 - accuracy: 0.0822 - val_loss: 1.8022 - val_accuracy: 0.4062\n",
            "Epoch 2/100\n",
            "73/73 [==============================] - 0s 225us/sample - loss: 1.7780 - accuracy: 0.2877 - val_loss: 1.4004 - val_accuracy: 0.4062\n",
            "Epoch 3/100\n",
            "73/73 [==============================] - 0s 225us/sample - loss: 1.3859 - accuracy: 0.4110 - val_loss: 1.1598 - val_accuracy: 0.7188\n",
            "Epoch 4/100\n",
            "73/73 [==============================] - 0s 209us/sample - loss: 1.1839 - accuracy: 0.4795 - val_loss: 1.0322 - val_accuracy: 0.5625\n",
            "Epoch 5/100\n",
            "73/73 [==============================] - 0s 219us/sample - loss: 1.0806 - accuracy: 0.5205 - val_loss: 0.9502 - val_accuracy: 0.5938\n",
            "Epoch 6/100\n",
            "73/73 [==============================] - 0s 206us/sample - loss: 0.9579 - accuracy: 0.6438 - val_loss: 0.8868 - val_accuracy: 0.5938\n",
            "Epoch 7/100\n",
            "73/73 [==============================] - 0s 218us/sample - loss: 0.9012 - accuracy: 0.7260 - val_loss: 0.8348 - val_accuracy: 0.6250\n",
            "Epoch 8/100\n",
            "73/73 [==============================] - 0s 203us/sample - loss: 0.8493 - accuracy: 0.6849 - val_loss: 0.7955 - val_accuracy: 0.5938\n",
            "Epoch 9/100\n",
            "73/73 [==============================] - 0s 208us/sample - loss: 0.7911 - accuracy: 0.7397 - val_loss: 0.7618 - val_accuracy: 0.5938\n",
            "Epoch 10/100\n",
            "73/73 [==============================] - 0s 261us/sample - loss: 0.7540 - accuracy: 0.7260 - val_loss: 0.7268 - val_accuracy: 0.6250\n",
            "Epoch 11/100\n",
            "73/73 [==============================] - 0s 217us/sample - loss: 0.7177 - accuracy: 0.7123 - val_loss: 0.6955 - val_accuracy: 0.9062\n",
            "Epoch 12/100\n",
            "73/73 [==============================] - 0s 220us/sample - loss: 0.7105 - accuracy: 0.7671 - val_loss: 0.6750 - val_accuracy: 0.9688\n",
            "Epoch 13/100\n",
            "73/73 [==============================] - 0s 192us/sample - loss: 0.6505 - accuracy: 0.7945 - val_loss: 0.6585 - val_accuracy: 0.9688\n",
            "Epoch 14/100\n",
            "73/73 [==============================] - 0s 228us/sample - loss: 0.6457 - accuracy: 0.7397 - val_loss: 0.6383 - val_accuracy: 0.9062\n",
            "Epoch 15/100\n",
            "73/73 [==============================] - 0s 232us/sample - loss: 0.6181 - accuracy: 0.7671 - val_loss: 0.6344 - val_accuracy: 0.5938\n",
            "Epoch 16/100\n",
            "73/73 [==============================] - 0s 244us/sample - loss: 0.6059 - accuracy: 0.7808 - val_loss: 0.6371 - val_accuracy: 0.5938\n",
            "Epoch 17/100\n",
            "73/73 [==============================] - 0s 237us/sample - loss: 0.5650 - accuracy: 0.7534 - val_loss: 0.6220 - val_accuracy: 0.5938\n",
            "Epoch 18/100\n",
            "73/73 [==============================] - 0s 225us/sample - loss: 0.5973 - accuracy: 0.7123 - val_loss: 0.5961 - val_accuracy: 0.6250\n",
            "Epoch 19/100\n",
            "73/73 [==============================] - 0s 215us/sample - loss: 0.5580 - accuracy: 0.7945 - val_loss: 0.5688 - val_accuracy: 0.9062\n",
            "Epoch 20/100\n",
            "73/73 [==============================] - 0s 236us/sample - loss: 0.5571 - accuracy: 0.7397 - val_loss: 0.5488 - val_accuracy: 1.0000\n",
            "Epoch 21/100\n",
            "73/73 [==============================] - 0s 217us/sample - loss: 0.5058 - accuracy: 0.7945 - val_loss: 0.5323 - val_accuracy: 0.9688\n",
            "Epoch 22/100\n",
            "73/73 [==============================] - 0s 200us/sample - loss: 0.5175 - accuracy: 0.8082 - val_loss: 0.5151 - val_accuracy: 1.0000\n",
            "Epoch 23/100\n",
            "73/73 [==============================] - 0s 227us/sample - loss: 0.4879 - accuracy: 0.8082 - val_loss: 0.5023 - val_accuracy: 1.0000\n",
            "Epoch 24/100\n",
            "73/73 [==============================] - 0s 200us/sample - loss: 0.4561 - accuracy: 0.8356 - val_loss: 0.4925 - val_accuracy: 0.9375\n",
            "Epoch 25/100\n",
            "73/73 [==============================] - 0s 203us/sample - loss: 0.4357 - accuracy: 0.9452 - val_loss: 0.4842 - val_accuracy: 0.9375\n",
            "Epoch 26/100\n",
            "73/73 [==============================] - 0s 215us/sample - loss: 0.4338 - accuracy: 0.8493 - val_loss: 0.4744 - val_accuracy: 1.0000\n",
            "Epoch 27/100\n",
            "73/73 [==============================] - 0s 225us/sample - loss: 0.4452 - accuracy: 0.8493 - val_loss: 0.4673 - val_accuracy: 1.0000\n",
            "Epoch 28/100\n",
            "73/73 [==============================] - 0s 336us/sample - loss: 0.4813 - accuracy: 0.7397 - val_loss: 0.4605 - val_accuracy: 1.0000\n",
            "Epoch 29/100\n",
            "73/73 [==============================] - 0s 214us/sample - loss: 0.4323 - accuracy: 0.8904 - val_loss: 0.4534 - val_accuracy: 0.9688\n",
            "Epoch 30/100\n",
            "73/73 [==============================] - 0s 221us/sample - loss: 0.4186 - accuracy: 0.8904 - val_loss: 0.4447 - val_accuracy: 0.9688\n",
            "Epoch 31/100\n",
            "73/73 [==============================] - 0s 195us/sample - loss: 0.4022 - accuracy: 0.8767 - val_loss: 0.4346 - val_accuracy: 1.0000\n",
            "Epoch 32/100\n",
            "73/73 [==============================] - 0s 216us/sample - loss: 0.4290 - accuracy: 0.8493 - val_loss: 0.4301 - val_accuracy: 0.9375\n",
            "Epoch 33/100\n",
            "73/73 [==============================] - 0s 221us/sample - loss: 0.3930 - accuracy: 0.8630 - val_loss: 0.4254 - val_accuracy: 0.9375\n",
            "Epoch 34/100\n",
            "73/73 [==============================] - 0s 232us/sample - loss: 0.4109 - accuracy: 0.8219 - val_loss: 0.4167 - val_accuracy: 1.0000\n",
            "Epoch 35/100\n",
            "73/73 [==============================] - 0s 236us/sample - loss: 0.3866 - accuracy: 0.9178 - val_loss: 0.4116 - val_accuracy: 1.0000\n",
            "Epoch 36/100\n",
            "73/73 [==============================] - 0s 230us/sample - loss: 0.4214 - accuracy: 0.8356 - val_loss: 0.4043 - val_accuracy: 1.0000\n",
            "Epoch 37/100\n",
            "73/73 [==============================] - 0s 235us/sample - loss: 0.3596 - accuracy: 0.9041 - val_loss: 0.4064 - val_accuracy: 0.9062\n",
            "Epoch 38/100\n",
            "73/73 [==============================] - 0s 196us/sample - loss: 0.3772 - accuracy: 0.8767 - val_loss: 0.3993 - val_accuracy: 0.9062\n",
            "Epoch 39/100\n",
            "73/73 [==============================] - 0s 245us/sample - loss: 0.3835 - accuracy: 0.8493 - val_loss: 0.3841 - val_accuracy: 1.0000\n",
            "Epoch 40/100\n",
            "73/73 [==============================] - 0s 270us/sample - loss: 0.3809 - accuracy: 0.8767 - val_loss: 0.3757 - val_accuracy: 1.0000\n",
            "Epoch 41/100\n",
            "73/73 [==============================] - 0s 270us/sample - loss: 0.3697 - accuracy: 0.9315 - val_loss: 0.3695 - val_accuracy: 1.0000\n",
            "Epoch 42/100\n",
            "73/73 [==============================] - 0s 234us/sample - loss: 0.3713 - accuracy: 0.8630 - val_loss: 0.3631 - val_accuracy: 1.0000\n",
            "Epoch 43/100\n",
            "73/73 [==============================] - 0s 223us/sample - loss: 0.3531 - accuracy: 0.9041 - val_loss: 0.3592 - val_accuracy: 0.9688\n",
            "Epoch 44/100\n",
            "73/73 [==============================] - 0s 218us/sample - loss: 0.3508 - accuracy: 0.8767 - val_loss: 0.3543 - val_accuracy: 0.9688\n",
            "Epoch 45/100\n",
            "73/73 [==============================] - 0s 260us/sample - loss: 0.3516 - accuracy: 0.8630 - val_loss: 0.3483 - val_accuracy: 0.9688\n",
            "Epoch 46/100\n",
            "73/73 [==============================] - 0s 201us/sample - loss: 0.3348 - accuracy: 0.9041 - val_loss: 0.3409 - val_accuracy: 1.0000\n",
            "Epoch 47/100\n",
            "73/73 [==============================] - 0s 215us/sample - loss: 0.3138 - accuracy: 0.9178 - val_loss: 0.3366 - val_accuracy: 1.0000\n",
            "Epoch 48/100\n",
            "73/73 [==============================] - 0s 229us/sample - loss: 0.3502 - accuracy: 0.8767 - val_loss: 0.3344 - val_accuracy: 1.0000\n",
            "Epoch 49/100\n",
            "73/73 [==============================] - 0s 202us/sample - loss: 0.3277 - accuracy: 0.9041 - val_loss: 0.3256 - val_accuracy: 1.0000\n",
            "Epoch 50/100\n",
            "73/73 [==============================] - 0s 238us/sample - loss: 0.3162 - accuracy: 0.8767 - val_loss: 0.3225 - val_accuracy: 0.9688\n",
            "Epoch 51/100\n",
            "73/73 [==============================] - 0s 210us/sample - loss: 0.3306 - accuracy: 0.8904 - val_loss: 0.3204 - val_accuracy: 0.9688\n",
            "Epoch 52/100\n",
            "73/73 [==============================] - 0s 221us/sample - loss: 0.3279 - accuracy: 0.8767 - val_loss: 0.3134 - val_accuracy: 0.9688\n",
            "Epoch 53/100\n",
            "73/73 [==============================] - 0s 236us/sample - loss: 0.3163 - accuracy: 0.9452 - val_loss: 0.3039 - val_accuracy: 1.0000\n",
            "Epoch 54/100\n",
            "73/73 [==============================] - 0s 239us/sample - loss: 0.3150 - accuracy: 0.9315 - val_loss: 0.2988 - val_accuracy: 1.0000\n",
            "Epoch 55/100\n",
            "73/73 [==============================] - 0s 250us/sample - loss: 0.2899 - accuracy: 0.9041 - val_loss: 0.2942 - val_accuracy: 1.0000\n",
            "Epoch 56/100\n",
            "73/73 [==============================] - 0s 277us/sample - loss: 0.3320 - accuracy: 0.8493 - val_loss: 0.2905 - val_accuracy: 1.0000\n",
            "Epoch 57/100\n",
            "73/73 [==============================] - 0s 256us/sample - loss: 0.2820 - accuracy: 0.9452 - val_loss: 0.2858 - val_accuracy: 1.0000\n",
            "Epoch 58/100\n",
            "73/73 [==============================] - 0s 227us/sample - loss: 0.3277 - accuracy: 0.8904 - val_loss: 0.2814 - val_accuracy: 1.0000\n",
            "Epoch 59/100\n",
            "73/73 [==============================] - 0s 209us/sample - loss: 0.3219 - accuracy: 0.8904 - val_loss: 0.2869 - val_accuracy: 0.9688\n",
            "Epoch 60/100\n",
            "73/73 [==============================] - 0s 234us/sample - loss: 0.2891 - accuracy: 0.9315 - val_loss: 0.2877 - val_accuracy: 0.9688\n",
            "Epoch 61/100\n",
            "73/73 [==============================] - 0s 285us/sample - loss: 0.3057 - accuracy: 0.9041 - val_loss: 0.2726 - val_accuracy: 0.9688\n",
            "Epoch 62/100\n",
            "73/73 [==============================] - 0s 218us/sample - loss: 0.2784 - accuracy: 0.9315 - val_loss: 0.2644 - val_accuracy: 1.0000\n",
            "Epoch 63/100\n",
            "73/73 [==============================] - 0s 247us/sample - loss: 0.2938 - accuracy: 0.8904 - val_loss: 0.2628 - val_accuracy: 1.0000\n",
            "Epoch 64/100\n",
            "73/73 [==============================] - 0s 226us/sample - loss: 0.2838 - accuracy: 0.9178 - val_loss: 0.2591 - val_accuracy: 1.0000\n",
            "Epoch 65/100\n",
            "73/73 [==============================] - 0s 209us/sample - loss: 0.2566 - accuracy: 0.9178 - val_loss: 0.2543 - val_accuracy: 1.0000\n",
            "Epoch 66/100\n",
            "73/73 [==============================] - 0s 236us/sample - loss: 0.2699 - accuracy: 0.9041 - val_loss: 0.2486 - val_accuracy: 1.0000\n",
            "Epoch 67/100\n",
            "73/73 [==============================] - 0s 237us/sample - loss: 0.2704 - accuracy: 0.8904 - val_loss: 0.2444 - val_accuracy: 1.0000\n",
            "Epoch 68/100\n",
            "73/73 [==============================] - 0s 214us/sample - loss: 0.2491 - accuracy: 0.9452 - val_loss: 0.2399 - val_accuracy: 1.0000\n",
            "Epoch 69/100\n",
            "73/73 [==============================] - 0s 231us/sample - loss: 0.2673 - accuracy: 0.9452 - val_loss: 0.2358 - val_accuracy: 1.0000\n",
            "Epoch 70/100\n",
            "73/73 [==============================] - 0s 240us/sample - loss: 0.2752 - accuracy: 0.9178 - val_loss: 0.2317 - val_accuracy: 1.0000\n",
            "Epoch 71/100\n",
            "73/73 [==============================] - 0s 242us/sample - loss: 0.2614 - accuracy: 0.9452 - val_loss: 0.2354 - val_accuracy: 0.9688\n",
            "Epoch 72/100\n",
            "73/73 [==============================] - 0s 259us/sample - loss: 0.2491 - accuracy: 0.9315 - val_loss: 0.2284 - val_accuracy: 0.9688\n",
            "Epoch 73/100\n",
            "73/73 [==============================] - 0s 228us/sample - loss: 0.2565 - accuracy: 0.9178 - val_loss: 0.2214 - val_accuracy: 1.0000\n",
            "Epoch 74/100\n",
            "73/73 [==============================] - 0s 241us/sample - loss: 0.2586 - accuracy: 0.9315 - val_loss: 0.2197 - val_accuracy: 1.0000\n",
            "Epoch 75/100\n",
            "73/73 [==============================] - 0s 233us/sample - loss: 0.2546 - accuracy: 0.9315 - val_loss: 0.2167 - val_accuracy: 1.0000\n",
            "Epoch 76/100\n",
            "73/73 [==============================] - 0s 228us/sample - loss: 0.2593 - accuracy: 0.8767 - val_loss: 0.2167 - val_accuracy: 0.9688\n",
            "Epoch 77/100\n",
            "73/73 [==============================] - 0s 229us/sample - loss: 0.2536 - accuracy: 0.9315 - val_loss: 0.2234 - val_accuracy: 0.9688\n",
            "Epoch 78/100\n",
            "73/73 [==============================] - 0s 230us/sample - loss: 0.2710 - accuracy: 0.9041 - val_loss: 0.2117 - val_accuracy: 0.9688\n",
            "Epoch 79/100\n",
            "73/73 [==============================] - 0s 298us/sample - loss: 0.2250 - accuracy: 0.9315 - val_loss: 0.2035 - val_accuracy: 1.0000\n",
            "Epoch 80/100\n",
            "73/73 [==============================] - 0s 281us/sample - loss: 0.2614 - accuracy: 0.9041 - val_loss: 0.2067 - val_accuracy: 1.0000\n",
            "Epoch 81/100\n",
            "73/73 [==============================] - 0s 231us/sample - loss: 0.2366 - accuracy: 0.9041 - val_loss: 0.2078 - val_accuracy: 1.0000\n",
            "Epoch 82/100\n",
            "73/73 [==============================] - 0s 216us/sample - loss: 0.2486 - accuracy: 0.9315 - val_loss: 0.1950 - val_accuracy: 1.0000\n",
            "Epoch 83/100\n",
            "73/73 [==============================] - 0s 221us/sample - loss: 0.2460 - accuracy: 0.9452 - val_loss: 0.1966 - val_accuracy: 0.9688\n",
            "Epoch 84/100\n",
            "73/73 [==============================] - 0s 237us/sample - loss: 0.2151 - accuracy: 0.9452 - val_loss: 0.2026 - val_accuracy: 0.9688\n",
            "Epoch 85/100\n",
            "73/73 [==============================] - 0s 243us/sample - loss: 0.2229 - accuracy: 0.9315 - val_loss: 0.1935 - val_accuracy: 0.9688\n",
            "Epoch 86/100\n",
            "73/73 [==============================] - 0s 283us/sample - loss: 0.2180 - accuracy: 0.9178 - val_loss: 0.1885 - val_accuracy: 0.9688\n",
            "Epoch 87/100\n",
            "73/73 [==============================] - 0s 211us/sample - loss: 0.2194 - accuracy: 0.9452 - val_loss: 0.1854 - val_accuracy: 0.9688\n",
            "Epoch 88/100\n",
            "73/73 [==============================] - 0s 199us/sample - loss: 0.2451 - accuracy: 0.9178 - val_loss: 0.1806 - val_accuracy: 1.0000\n",
            "Epoch 89/100\n",
            "73/73 [==============================] - 0s 243us/sample - loss: 0.2112 - accuracy: 0.9726 - val_loss: 0.1793 - val_accuracy: 1.0000\n",
            "Epoch 90/100\n",
            "73/73 [==============================] - 0s 247us/sample - loss: 0.2223 - accuracy: 0.9452 - val_loss: 0.1764 - val_accuracy: 1.0000\n",
            "Epoch 91/100\n",
            "73/73 [==============================] - 0s 240us/sample - loss: 0.2055 - accuracy: 0.9452 - val_loss: 0.1739 - val_accuracy: 1.0000\n",
            "Epoch 92/100\n",
            "73/73 [==============================] - 0s 251us/sample - loss: 0.2333 - accuracy: 0.9178 - val_loss: 0.1733 - val_accuracy: 1.0000\n",
            "Epoch 93/100\n",
            "73/73 [==============================] - 0s 242us/sample - loss: 0.2228 - accuracy: 0.9452 - val_loss: 0.1745 - val_accuracy: 0.9688\n",
            "Epoch 94/100\n",
            "73/73 [==============================] - 0s 211us/sample - loss: 0.2100 - accuracy: 0.9315 - val_loss: 0.1743 - val_accuracy: 0.9688\n",
            "Epoch 95/100\n",
            "73/73 [==============================] - 0s 303us/sample - loss: 0.2013 - accuracy: 0.9589 - val_loss: 0.1663 - val_accuracy: 1.0000\n",
            "Epoch 96/100\n",
            "73/73 [==============================] - 0s 279us/sample - loss: 0.1939 - accuracy: 0.9452 - val_loss: 0.1622 - val_accuracy: 1.0000\n",
            "Epoch 97/100\n",
            "73/73 [==============================] - 0s 207us/sample - loss: 0.2050 - accuracy: 0.9452 - val_loss: 0.1631 - val_accuracy: 1.0000\n",
            "Epoch 98/100\n",
            "73/73 [==============================] - 0s 302us/sample - loss: 0.1823 - accuracy: 0.9589 - val_loss: 0.1731 - val_accuracy: 0.9688\n",
            "Epoch 99/100\n",
            "73/73 [==============================] - 0s 230us/sample - loss: 0.2232 - accuracy: 0.9315 - val_loss: 0.1806 - val_accuracy: 0.9688\n",
            "Epoch 100/100\n",
            "73/73 [==============================] - 0s 228us/sample - loss: 0.1933 - accuracy: 0.9589 - val_loss: 0.1579 - val_accuracy: 0.9688\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VbEQVkwDqD3r",
        "colab_type": "code",
        "outputId": "3b2c04ff-c621-4f85-ed95-ed1941e85b32",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "source": [
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model Loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Training','Testing'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nsVAwxhjjY6FgjDHGx0LBGGOMz/8H10xa7dJAEc0AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x6sjQjOEqD7L",
        "colab_type": "code",
        "outputId": "db9ebd60-ba3b-4221-a39e-420969c9d8ed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "model.evaluate(x_test, y_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r45/1 [======================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 0s 1ms/sample - loss: 0.1059 - accuracy: 1.0000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.13053987473249434, 1.0]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f06-I3Xchc4U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred = model.predict_classes(x_test[0:3])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cg5PdZV1hc59",
        "colab_type": "code",
        "outputId": "882276c1-1274-462f-db02-5328adee5de3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(y_pred)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1 0 2]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3qfJXNJ8hc7a",
        "colab_type": "code",
        "outputId": "e907a1a5-84e5-4832-e5ad-8a5d3ce3f4b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "x_test[0:3],y_test[0:3]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[6.1, 2.8, 4.7, 1.2],\n",
              "        [5.7, 3.8, 1.7, 0.3],\n",
              "        [7.7, 2.6, 6.9, 2.3]]), array([1, 0, 2]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R2AnkaC7hc-b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CbNinEcXhdAF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import sklearn\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import preprocessing"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4lFbH0Um_3Uc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df=pd.DataFrame(data.data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V3MoNF6FAW9g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_df = df[[0,1,2]]\n",
        "target_df = df[3]\n",
        "X_train, X_test, y_train, y_test = train_test_split(data_df, target_df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R5dNu66VAr8u",
        "colab_type": "code",
        "outputId": "113c48f5-40cc-4694-b274-8da6d7d525e8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Dense(1000, input_dim=3, kernel_initializer='normal', activation='relu'),\n",
        "  tf.keras.layers.Dense(150, activation='relu'),\n",
        "  tf.keras.layers.Dense(50, activation='relu'),\n",
        "  tf.keras.layers.Dense(1, activation='linear')\n",
        "])\n",
        "adam = tf.keras.optimizers.Adam(lr=0.0001)\n",
        "model.compile(loss='mse', optimizer=adam, metrics=['mse','mae'])\n",
        "history = model.fit(X_train, y_train, epochs=300, validation_split=0.4)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0824 09:07:49.040391 140072041154432 training.py:510] Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 67 samples, validate on 45 samples\n",
            "Epoch 1/300\n",
            "67/67 [==============================] - 0s 2ms/sample - loss: 1.6802 - mse: 1.6802 - mae: 1.0194 - val_loss: 0.9573 - val_mse: 0.9573 - val_mae: 0.7996\n",
            "Epoch 2/300\n",
            "67/67 [==============================] - 0s 226us/sample - loss: 1.0520 - mse: 1.0520 - mae: 0.8058 - val_loss: 0.5689 - val_mse: 0.5689 - val_mae: 0.6387\n",
            "Epoch 3/300\n",
            "67/67 [==============================] - 0s 212us/sample - loss: 0.6560 - mse: 0.6560 - mae: 0.6859 - val_loss: 0.3504 - val_mse: 0.3504 - val_mae: 0.5218\n",
            "Epoch 4/300\n",
            "67/67 [==============================] - 0s 207us/sample - loss: 0.4333 - mse: 0.4333 - mae: 0.5917 - val_loss: 0.2335 - val_mse: 0.2335 - val_mae: 0.4223\n",
            "Epoch 5/300\n",
            "67/67 [==============================] - 0s 206us/sample - loss: 0.3232 - mse: 0.3232 - mae: 0.5122 - val_loss: 0.1839 - val_mse: 0.1839 - val_mae: 0.3506\n",
            "Epoch 6/300\n",
            "67/67 [==============================] - 0s 215us/sample - loss: 0.2595 - mse: 0.2595 - mae: 0.4444 - val_loss: 0.1666 - val_mse: 0.1666 - val_mae: 0.3215\n",
            "Epoch 7/300\n",
            "67/67 [==============================] - 0s 230us/sample - loss: 0.2321 - mse: 0.2321 - mae: 0.4134 - val_loss: 0.1553 - val_mse: 0.1553 - val_mae: 0.3167\n",
            "Epoch 8/300\n",
            "67/67 [==============================] - 0s 205us/sample - loss: 0.2108 - mse: 0.2108 - mae: 0.3907 - val_loss: 0.1438 - val_mse: 0.1438 - val_mae: 0.3104\n",
            "Epoch 9/300\n",
            "67/67 [==============================] - 0s 192us/sample - loss: 0.1921 - mse: 0.1921 - mae: 0.3762 - val_loss: 0.1325 - val_mse: 0.1325 - val_mae: 0.3008\n",
            "Epoch 10/300\n",
            "67/67 [==============================] - 0s 204us/sample - loss: 0.1741 - mse: 0.1741 - mae: 0.3598 - val_loss: 0.1157 - val_mse: 0.1157 - val_mae: 0.2803\n",
            "Epoch 11/300\n",
            "67/67 [==============================] - 0s 248us/sample - loss: 0.1545 - mse: 0.1545 - mae: 0.3403 - val_loss: 0.0964 - val_mse: 0.0964 - val_mae: 0.2551\n",
            "Epoch 12/300\n",
            "67/67 [==============================] - 0s 218us/sample - loss: 0.1344 - mse: 0.1344 - mae: 0.3179 - val_loss: 0.0811 - val_mse: 0.0811 - val_mae: 0.2307\n",
            "Epoch 13/300\n",
            "67/67 [==============================] - 0s 232us/sample - loss: 0.1192 - mse: 0.1192 - mae: 0.2986 - val_loss: 0.0709 - val_mse: 0.0709 - val_mae: 0.2106\n",
            "Epoch 14/300\n",
            "67/67 [==============================] - 0s 222us/sample - loss: 0.1102 - mse: 0.1102 - mae: 0.2806 - val_loss: 0.0651 - val_mse: 0.0651 - val_mae: 0.2001\n",
            "Epoch 15/300\n",
            "67/67 [==============================] - 0s 293us/sample - loss: 0.1064 - mse: 0.1064 - mae: 0.2695 - val_loss: 0.0617 - val_mse: 0.0617 - val_mae: 0.1921\n",
            "Epoch 16/300\n",
            "67/67 [==============================] - 0s 218us/sample - loss: 0.1003 - mse: 0.1003 - mae: 0.2563 - val_loss: 0.0565 - val_mse: 0.0565 - val_mae: 0.1805\n",
            "Epoch 17/300\n",
            "67/67 [==============================] - 0s 203us/sample - loss: 0.0912 - mse: 0.0912 - mae: 0.2404 - val_loss: 0.0496 - val_mse: 0.0496 - val_mae: 0.1684\n",
            "Epoch 18/300\n",
            "67/67 [==============================] - 0s 261us/sample - loss: 0.0791 - mse: 0.0791 - mae: 0.2257 - val_loss: 0.0443 - val_mse: 0.0443 - val_mae: 0.1605\n",
            "Epoch 19/300\n",
            "67/67 [==============================] - 0s 209us/sample - loss: 0.0691 - mse: 0.0691 - mae: 0.2180 - val_loss: 0.0425 - val_mse: 0.0425 - val_mae: 0.1616\n",
            "Epoch 20/300\n",
            "67/67 [==============================] - 0s 195us/sample - loss: 0.0631 - mse: 0.0631 - mae: 0.2131 - val_loss: 0.0411 - val_mse: 0.0411 - val_mae: 0.1597\n",
            "Epoch 21/300\n",
            "67/67 [==============================] - 0s 193us/sample - loss: 0.0592 - mse: 0.0592 - mae: 0.2067 - val_loss: 0.0392 - val_mse: 0.0392 - val_mae: 0.1550\n",
            "Epoch 22/300\n",
            "67/67 [==============================] - 0s 256us/sample - loss: 0.0551 - mse: 0.0551 - mae: 0.1968 - val_loss: 0.0376 - val_mse: 0.0376 - val_mae: 0.1501\n",
            "Epoch 23/300\n",
            "67/67 [==============================] - 0s 203us/sample - loss: 0.0516 - mse: 0.0516 - mae: 0.1880 - val_loss: 0.0365 - val_mse: 0.0365 - val_mae: 0.1465\n",
            "Epoch 24/300\n",
            "67/67 [==============================] - 0s 203us/sample - loss: 0.0489 - mse: 0.0489 - mae: 0.1797 - val_loss: 0.0357 - val_mse: 0.0357 - val_mae: 0.1431\n",
            "Epoch 25/300\n",
            "67/67 [==============================] - 0s 216us/sample - loss: 0.0468 - mse: 0.0468 - mae: 0.1734 - val_loss: 0.0352 - val_mse: 0.0352 - val_mae: 0.1414\n",
            "Epoch 26/300\n",
            "67/67 [==============================] - 0s 240us/sample - loss: 0.0449 - mse: 0.0449 - mae: 0.1680 - val_loss: 0.0350 - val_mse: 0.0350 - val_mae: 0.1399\n",
            "Epoch 27/300\n",
            "67/67 [==============================] - 0s 227us/sample - loss: 0.0437 - mse: 0.0437 - mae: 0.1637 - val_loss: 0.0348 - val_mse: 0.0348 - val_mae: 0.1370\n",
            "Epoch 28/300\n",
            "67/67 [==============================] - 0s 175us/sample - loss: 0.0430 - mse: 0.0430 - mae: 0.1585 - val_loss: 0.0351 - val_mse: 0.0351 - val_mae: 0.1376\n",
            "Epoch 29/300\n",
            "67/67 [==============================] - 0s 180us/sample - loss: 0.0414 - mse: 0.0414 - mae: 0.1570 - val_loss: 0.0364 - val_mse: 0.0364 - val_mae: 0.1438\n",
            "Epoch 30/300\n",
            "67/67 [==============================] - 0s 227us/sample - loss: 0.0405 - mse: 0.0405 - mae: 0.1601 - val_loss: 0.0382 - val_mse: 0.0382 - val_mae: 0.1502\n",
            "Epoch 31/300\n",
            "67/67 [==============================] - 0s 212us/sample - loss: 0.0398 - mse: 0.0398 - mae: 0.1603 - val_loss: 0.0387 - val_mse: 0.0387 - val_mae: 0.1515\n",
            "Epoch 32/300\n",
            "67/67 [==============================] - 0s 225us/sample - loss: 0.0395 - mse: 0.0395 - mae: 0.1595 - val_loss: 0.0396 - val_mse: 0.0396 - val_mae: 0.1541\n",
            "Epoch 33/300\n",
            "67/67 [==============================] - 0s 227us/sample - loss: 0.0398 - mse: 0.0398 - mae: 0.1607 - val_loss: 0.0401 - val_mse: 0.0401 - val_mae: 0.1552\n",
            "Epoch 34/300\n",
            "67/67 [==============================] - 0s 233us/sample - loss: 0.0390 - mse: 0.0390 - mae: 0.1584 - val_loss: 0.0380 - val_mse: 0.0380 - val_mae: 0.1477\n",
            "Epoch 35/300\n",
            "67/67 [==============================] - 0s 210us/sample - loss: 0.0379 - mse: 0.0379 - mae: 0.1511 - val_loss: 0.0369 - val_mse: 0.0369 - val_mae: 0.1434\n",
            "Epoch 36/300\n",
            "67/67 [==============================] - 0s 219us/sample - loss: 0.0386 - mse: 0.0386 - mae: 0.1472 - val_loss: 0.0370 - val_mse: 0.0370 - val_mae: 0.1427\n",
            "Epoch 37/300\n",
            "67/67 [==============================] - 0s 210us/sample - loss: 0.0392 - mse: 0.0392 - mae: 0.1463 - val_loss: 0.0372 - val_mse: 0.0372 - val_mae: 0.1445\n",
            "Epoch 38/300\n",
            "67/67 [==============================] - 0s 235us/sample - loss: 0.0381 - mse: 0.0381 - mae: 0.1476 - val_loss: 0.0380 - val_mse: 0.0380 - val_mae: 0.1476\n",
            "Epoch 39/300\n",
            "67/67 [==============================] - 0s 211us/sample - loss: 0.0371 - mse: 0.0371 - mae: 0.1478 - val_loss: 0.0391 - val_mse: 0.0391 - val_mae: 0.1516\n",
            "Epoch 40/300\n",
            "67/67 [==============================] - 0s 211us/sample - loss: 0.0367 - mse: 0.0367 - mae: 0.1491 - val_loss: 0.0415 - val_mse: 0.0415 - val_mae: 0.1574\n",
            "Epoch 41/300\n",
            "67/67 [==============================] - 0s 174us/sample - loss: 0.0381 - mse: 0.0381 - mae: 0.1535 - val_loss: 0.0439 - val_mse: 0.0439 - val_mae: 0.1634\n",
            "Epoch 42/300\n",
            "67/67 [==============================] - 0s 253us/sample - loss: 0.0388 - mse: 0.0388 - mae: 0.1552 - val_loss: 0.0442 - val_mse: 0.0442 - val_mae: 0.1642\n",
            "Epoch 43/300\n",
            "67/67 [==============================] - 0s 227us/sample - loss: 0.0387 - mse: 0.0387 - mae: 0.1546 - val_loss: 0.0431 - val_mse: 0.0431 - val_mae: 0.1615\n",
            "Epoch 44/300\n",
            "67/67 [==============================] - 0s 269us/sample - loss: 0.0374 - mse: 0.0374 - mae: 0.1509 - val_loss: 0.0406 - val_mse: 0.0406 - val_mae: 0.1561\n",
            "Epoch 45/300\n",
            "67/67 [==============================] - 0s 250us/sample - loss: 0.0362 - mse: 0.0362 - mae: 0.1466 - val_loss: 0.0387 - val_mse: 0.0387 - val_mae: 0.1513\n",
            "Epoch 46/300\n",
            "67/67 [==============================] - 0s 245us/sample - loss: 0.0359 - mse: 0.0359 - mae: 0.1450 - val_loss: 0.0382 - val_mse: 0.0382 - val_mae: 0.1500\n",
            "Epoch 47/300\n",
            "67/67 [==============================] - 0s 230us/sample - loss: 0.0356 - mse: 0.0356 - mae: 0.1438 - val_loss: 0.0387 - val_mse: 0.0387 - val_mae: 0.1521\n",
            "Epoch 48/300\n",
            "67/67 [==============================] - 0s 200us/sample - loss: 0.0352 - mse: 0.0352 - mae: 0.1438 - val_loss: 0.0387 - val_mse: 0.0387 - val_mae: 0.1521\n",
            "Epoch 49/300\n",
            "67/67 [==============================] - 0s 204us/sample - loss: 0.0351 - mse: 0.0351 - mae: 0.1439 - val_loss: 0.0384 - val_mse: 0.0384 - val_mae: 0.1511\n",
            "Epoch 50/300\n",
            "67/67 [==============================] - 0s 210us/sample - loss: 0.0351 - mse: 0.0351 - mae: 0.1443 - val_loss: 0.0377 - val_mse: 0.0377 - val_mae: 0.1490\n",
            "Epoch 51/300\n",
            "67/67 [==============================] - 0s 189us/sample - loss: 0.0349 - mse: 0.0349 - mae: 0.1443 - val_loss: 0.0367 - val_mse: 0.0367 - val_mae: 0.1459\n",
            "Epoch 52/300\n",
            "67/67 [==============================] - 0s 194us/sample - loss: 0.0349 - mse: 0.0349 - mae: 0.1439 - val_loss: 0.0362 - val_mse: 0.0362 - val_mae: 0.1448\n",
            "Epoch 53/300\n",
            "67/67 [==============================] - 0s 198us/sample - loss: 0.0348 - mse: 0.0348 - mae: 0.1440 - val_loss: 0.0359 - val_mse: 0.0359 - val_mae: 0.1440\n",
            "Epoch 54/300\n",
            "67/67 [==============================] - 0s 200us/sample - loss: 0.0347 - mse: 0.0347 - mae: 0.1437 - val_loss: 0.0355 - val_mse: 0.0355 - val_mae: 0.1425\n",
            "Epoch 55/300\n",
            "67/67 [==============================] - 0s 216us/sample - loss: 0.0345 - mse: 0.0345 - mae: 0.1424 - val_loss: 0.0343 - val_mse: 0.0343 - val_mae: 0.1368\n",
            "Epoch 56/300\n",
            "67/67 [==============================] - 0s 194us/sample - loss: 0.0352 - mse: 0.0352 - mae: 0.1408 - val_loss: 0.0340 - val_mse: 0.0340 - val_mae: 0.1347\n",
            "Epoch 57/300\n",
            "67/67 [==============================] - 0s 250us/sample - loss: 0.0350 - mse: 0.0350 - mae: 0.1403 - val_loss: 0.0342 - val_mse: 0.0342 - val_mae: 0.1370\n",
            "Epoch 58/300\n",
            "67/67 [==============================] - 0s 179us/sample - loss: 0.0343 - mse: 0.0343 - mae: 0.1414 - val_loss: 0.0341 - val_mse: 0.0341 - val_mae: 0.1365\n",
            "Epoch 59/300\n",
            "67/67 [==============================] - 0s 185us/sample - loss: 0.0346 - mse: 0.0346 - mae: 0.1417 - val_loss: 0.0345 - val_mse: 0.0345 - val_mae: 0.1381\n",
            "Epoch 60/300\n",
            "67/67 [==============================] - 0s 258us/sample - loss: 0.0338 - mse: 0.0338 - mae: 0.1417 - val_loss: 0.0373 - val_mse: 0.0373 - val_mae: 0.1486\n",
            "Epoch 61/300\n",
            "67/67 [==============================] - 0s 215us/sample - loss: 0.0343 - mse: 0.0343 - mae: 0.1459 - val_loss: 0.0399 - val_mse: 0.0399 - val_mae: 0.1555\n",
            "Epoch 62/300\n",
            "67/67 [==============================] - 0s 215us/sample - loss: 0.0354 - mse: 0.0354 - mae: 0.1481 - val_loss: 0.0402 - val_mse: 0.0402 - val_mae: 0.1559\n",
            "Epoch 63/300\n",
            "67/67 [==============================] - 0s 230us/sample - loss: 0.0347 - mse: 0.0347 - mae: 0.1455 - val_loss: 0.0372 - val_mse: 0.0372 - val_mae: 0.1481\n",
            "Epoch 64/300\n",
            "67/67 [==============================] - 0s 229us/sample - loss: 0.0332 - mse: 0.0332 - mae: 0.1396 - val_loss: 0.0357 - val_mse: 0.0357 - val_mae: 0.1420\n",
            "Epoch 65/300\n",
            "67/67 [==============================] - 0s 230us/sample - loss: 0.0339 - mse: 0.0339 - mae: 0.1367 - val_loss: 0.0358 - val_mse: 0.0358 - val_mae: 0.1394\n",
            "Epoch 66/300\n",
            "67/67 [==============================] - 0s 215us/sample - loss: 0.0367 - mse: 0.0367 - mae: 0.1418 - val_loss: 0.0354 - val_mse: 0.0354 - val_mae: 0.1383\n",
            "Epoch 67/300\n",
            "67/67 [==============================] - 0s 196us/sample - loss: 0.0350 - mse: 0.0350 - mae: 0.1386 - val_loss: 0.0351 - val_mse: 0.0351 - val_mae: 0.1410\n",
            "Epoch 68/300\n",
            "67/67 [==============================] - 0s 201us/sample - loss: 0.0329 - mse: 0.0329 - mae: 0.1383 - val_loss: 0.0379 - val_mse: 0.0379 - val_mae: 0.1504\n",
            "Epoch 69/300\n",
            "67/67 [==============================] - 0s 236us/sample - loss: 0.0346 - mse: 0.0346 - mae: 0.1457 - val_loss: 0.0444 - val_mse: 0.0444 - val_mae: 0.1672\n",
            "Epoch 70/300\n",
            "67/67 [==============================] - 0s 200us/sample - loss: 0.0376 - mse: 0.0376 - mae: 0.1517 - val_loss: 0.0452 - val_mse: 0.0452 - val_mae: 0.1684\n",
            "Epoch 71/300\n",
            "67/67 [==============================] - 0s 199us/sample - loss: 0.0371 - mse: 0.0371 - mae: 0.1499 - val_loss: 0.0400 - val_mse: 0.0400 - val_mae: 0.1550\n",
            "Epoch 72/300\n",
            "67/67 [==============================] - 0s 199us/sample - loss: 0.0344 - mse: 0.0344 - mae: 0.1432 - val_loss: 0.0353 - val_mse: 0.0353 - val_mae: 0.1416\n",
            "Epoch 73/300\n",
            "67/67 [==============================] - 0s 193us/sample - loss: 0.0331 - mse: 0.0331 - mae: 0.1371 - val_loss: 0.0348 - val_mse: 0.0348 - val_mae: 0.1375\n",
            "Epoch 74/300\n",
            "67/67 [==============================] - 0s 283us/sample - loss: 0.0358 - mse: 0.0358 - mae: 0.1405 - val_loss: 0.0346 - val_mse: 0.0346 - val_mae: 0.1370\n",
            "Epoch 75/300\n",
            "67/67 [==============================] - 0s 292us/sample - loss: 0.0344 - mse: 0.0344 - mae: 0.1388 - val_loss: 0.0343 - val_mse: 0.0343 - val_mae: 0.1381\n",
            "Epoch 76/300\n",
            "67/67 [==============================] - 0s 198us/sample - loss: 0.0325 - mse: 0.0325 - mae: 0.1367 - val_loss: 0.0354 - val_mse: 0.0354 - val_mae: 0.1434\n",
            "Epoch 77/300\n",
            "67/67 [==============================] - 0s 214us/sample - loss: 0.0324 - mse: 0.0324 - mae: 0.1386 - val_loss: 0.0354 - val_mse: 0.0354 - val_mae: 0.1442\n",
            "Epoch 78/300\n",
            "67/67 [==============================] - 0s 176us/sample - loss: 0.0325 - mse: 0.0325 - mae: 0.1397 - val_loss: 0.0359 - val_mse: 0.0359 - val_mae: 0.1464\n",
            "Epoch 79/300\n",
            "67/67 [==============================] - 0s 231us/sample - loss: 0.0328 - mse: 0.0328 - mae: 0.1413 - val_loss: 0.0357 - val_mse: 0.0357 - val_mae: 0.1459\n",
            "Epoch 80/300\n",
            "67/67 [==============================] - 0s 217us/sample - loss: 0.0324 - mse: 0.0324 - mae: 0.1389 - val_loss: 0.0333 - val_mse: 0.0333 - val_mae: 0.1355\n",
            "Epoch 81/300\n",
            "67/67 [==============================] - 0s 190us/sample - loss: 0.0333 - mse: 0.0333 - mae: 0.1378 - val_loss: 0.0331 - val_mse: 0.0331 - val_mae: 0.1315\n",
            "Epoch 82/300\n",
            "67/67 [==============================] - 0s 184us/sample - loss: 0.0346 - mse: 0.0346 - mae: 0.1384 - val_loss: 0.0328 - val_mse: 0.0328 - val_mae: 0.1315\n",
            "Epoch 83/300\n",
            "67/67 [==============================] - 0s 181us/sample - loss: 0.0337 - mse: 0.0337 - mae: 0.1379 - val_loss: 0.0329 - val_mse: 0.0329 - val_mae: 0.1342\n",
            "Epoch 84/300\n",
            "67/67 [==============================] - 0s 244us/sample - loss: 0.0326 - mse: 0.0326 - mae: 0.1376 - val_loss: 0.0343 - val_mse: 0.0343 - val_mae: 0.1414\n",
            "Epoch 85/300\n",
            "67/67 [==============================] - 0s 235us/sample - loss: 0.0314 - mse: 0.0314 - mae: 0.1373 - val_loss: 0.0374 - val_mse: 0.0374 - val_mae: 0.1506\n",
            "Epoch 86/300\n",
            "67/67 [==============================] - 0s 208us/sample - loss: 0.0330 - mse: 0.0330 - mae: 0.1427 - val_loss: 0.0389 - val_mse: 0.0389 - val_mae: 0.1535\n",
            "Epoch 87/300\n",
            "67/67 [==============================] - 0s 209us/sample - loss: 0.0331 - mse: 0.0331 - mae: 0.1418 - val_loss: 0.0375 - val_mse: 0.0375 - val_mae: 0.1491\n",
            "Epoch 88/300\n",
            "67/67 [==============================] - 0s 215us/sample - loss: 0.0323 - mse: 0.0323 - mae: 0.1381 - val_loss: 0.0369 - val_mse: 0.0369 - val_mae: 0.1471\n",
            "Epoch 89/300\n",
            "67/67 [==============================] - 0s 176us/sample - loss: 0.0319 - mse: 0.0319 - mae: 0.1365 - val_loss: 0.0373 - val_mse: 0.0373 - val_mae: 0.1481\n",
            "Epoch 90/300\n",
            "67/67 [==============================] - 0s 180us/sample - loss: 0.0320 - mse: 0.0320 - mae: 0.1369 - val_loss: 0.0373 - val_mse: 0.0373 - val_mae: 0.1482\n",
            "Epoch 91/300\n",
            "67/67 [==============================] - 0s 178us/sample - loss: 0.0318 - mse: 0.0318 - mae: 0.1357 - val_loss: 0.0368 - val_mse: 0.0368 - val_mae: 0.1474\n",
            "Epoch 92/300\n",
            "67/67 [==============================] - 0s 229us/sample - loss: 0.0317 - mse: 0.0317 - mae: 0.1348 - val_loss: 0.0369 - val_mse: 0.0369 - val_mae: 0.1479\n",
            "Epoch 93/300\n",
            "67/67 [==============================] - 0s 207us/sample - loss: 0.0316 - mse: 0.0316 - mae: 0.1347 - val_loss: 0.0367 - val_mse: 0.0367 - val_mae: 0.1475\n",
            "Epoch 94/300\n",
            "67/67 [==============================] - 0s 255us/sample - loss: 0.0315 - mse: 0.0315 - mae: 0.1343 - val_loss: 0.0367 - val_mse: 0.0367 - val_mae: 0.1470\n",
            "Epoch 95/300\n",
            "67/67 [==============================] - 0s 225us/sample - loss: 0.0313 - mse: 0.0313 - mae: 0.1342 - val_loss: 0.0361 - val_mse: 0.0361 - val_mae: 0.1452\n",
            "Epoch 96/300\n",
            "67/67 [==============================] - 0s 213us/sample - loss: 0.0309 - mse: 0.0309 - mae: 0.1332 - val_loss: 0.0346 - val_mse: 0.0346 - val_mae: 0.1406\n",
            "Epoch 97/300\n",
            "67/67 [==============================] - 0s 210us/sample - loss: 0.0310 - mse: 0.0310 - mae: 0.1322 - val_loss: 0.0339 - val_mse: 0.0339 - val_mae: 0.1379\n",
            "Epoch 98/300\n",
            "67/67 [==============================] - 0s 283us/sample - loss: 0.0306 - mse: 0.0306 - mae: 0.1320 - val_loss: 0.0349 - val_mse: 0.0349 - val_mae: 0.1418\n",
            "Epoch 99/300\n",
            "67/67 [==============================] - 0s 214us/sample - loss: 0.0309 - mse: 0.0309 - mae: 0.1347 - val_loss: 0.0361 - val_mse: 0.0361 - val_mae: 0.1459\n",
            "Epoch 100/300\n",
            "67/67 [==============================] - 0s 226us/sample - loss: 0.0313 - mse: 0.0313 - mae: 0.1363 - val_loss: 0.0358 - val_mse: 0.0358 - val_mae: 0.1456\n",
            "Epoch 101/300\n",
            "67/67 [==============================] - 0s 177us/sample - loss: 0.0311 - mse: 0.0311 - mae: 0.1362 - val_loss: 0.0368 - val_mse: 0.0368 - val_mae: 0.1487\n",
            "Epoch 102/300\n",
            "67/67 [==============================] - 0s 210us/sample - loss: 0.0317 - mse: 0.0317 - mae: 0.1378 - val_loss: 0.0376 - val_mse: 0.0376 - val_mae: 0.1509\n",
            "Epoch 103/300\n",
            "67/67 [==============================] - 0s 245us/sample - loss: 0.0318 - mse: 0.0318 - mae: 0.1375 - val_loss: 0.0368 - val_mse: 0.0368 - val_mae: 0.1482\n",
            "Epoch 104/300\n",
            "67/67 [==============================] - 0s 261us/sample - loss: 0.0312 - mse: 0.0312 - mae: 0.1354 - val_loss: 0.0362 - val_mse: 0.0362 - val_mae: 0.1462\n",
            "Epoch 105/300\n",
            "67/67 [==============================] - 0s 244us/sample - loss: 0.0307 - mse: 0.0307 - mae: 0.1331 - val_loss: 0.0353 - val_mse: 0.0353 - val_mae: 0.1434\n",
            "Epoch 106/300\n",
            "67/67 [==============================] - 0s 197us/sample - loss: 0.0305 - mse: 0.0305 - mae: 0.1315 - val_loss: 0.0358 - val_mse: 0.0358 - val_mae: 0.1442\n",
            "Epoch 107/300\n",
            "67/67 [==============================] - 0s 239us/sample - loss: 0.0305 - mse: 0.0305 - mae: 0.1319 - val_loss: 0.0363 - val_mse: 0.0363 - val_mae: 0.1453\n",
            "Epoch 108/300\n",
            "67/67 [==============================] - 0s 225us/sample - loss: 0.0306 - mse: 0.0306 - mae: 0.1327 - val_loss: 0.0357 - val_mse: 0.0357 - val_mae: 0.1440\n",
            "Epoch 109/300\n",
            "67/67 [==============================] - 0s 196us/sample - loss: 0.0306 - mse: 0.0306 - mae: 0.1327 - val_loss: 0.0336 - val_mse: 0.0336 - val_mae: 0.1379\n",
            "Epoch 110/300\n",
            "67/67 [==============================] - 0s 194us/sample - loss: 0.0300 - mse: 0.0300 - mae: 0.1309 - val_loss: 0.0331 - val_mse: 0.0331 - val_mae: 0.1366\n",
            "Epoch 111/300\n",
            "67/67 [==============================] - 0s 184us/sample - loss: 0.0298 - mse: 0.0298 - mae: 0.1312 - val_loss: 0.0335 - val_mse: 0.0335 - val_mae: 0.1395\n",
            "Epoch 112/300\n",
            "67/67 [==============================] - 0s 186us/sample - loss: 0.0298 - mse: 0.0298 - mae: 0.1323 - val_loss: 0.0336 - val_mse: 0.0336 - val_mae: 0.1416\n",
            "Epoch 113/300\n",
            "67/67 [==============================] - 0s 195us/sample - loss: 0.0302 - mse: 0.0302 - mae: 0.1345 - val_loss: 0.0331 - val_mse: 0.0331 - val_mae: 0.1402\n",
            "Epoch 114/300\n",
            "67/67 [==============================] - 0s 188us/sample - loss: 0.0296 - mse: 0.0296 - mae: 0.1326 - val_loss: 0.0315 - val_mse: 0.0315 - val_mae: 0.1307\n",
            "Epoch 115/300\n",
            "67/67 [==============================] - 0s 230us/sample - loss: 0.0308 - mse: 0.0308 - mae: 0.1331 - val_loss: 0.0318 - val_mse: 0.0318 - val_mae: 0.1301\n",
            "Epoch 116/300\n",
            "67/67 [==============================] - 0s 213us/sample - loss: 0.0314 - mse: 0.0314 - mae: 0.1338 - val_loss: 0.0318 - val_mse: 0.0318 - val_mae: 0.1309\n",
            "Epoch 117/300\n",
            "67/67 [==============================] - 0s 259us/sample - loss: 0.0306 - mse: 0.0306 - mae: 0.1327 - val_loss: 0.0317 - val_mse: 0.0317 - val_mae: 0.1316\n",
            "Epoch 118/300\n",
            "67/67 [==============================] - 0s 266us/sample - loss: 0.0293 - mse: 0.0293 - mae: 0.1307 - val_loss: 0.0326 - val_mse: 0.0326 - val_mae: 0.1386\n",
            "Epoch 119/300\n",
            "67/67 [==============================] - 0s 287us/sample - loss: 0.0298 - mse: 0.0298 - mae: 0.1348 - val_loss: 0.0342 - val_mse: 0.0342 - val_mae: 0.1449\n",
            "Epoch 120/300\n",
            "67/67 [==============================] - 0s 211us/sample - loss: 0.0301 - mse: 0.0301 - mae: 0.1362 - val_loss: 0.0316 - val_mse: 0.0316 - val_mae: 0.1350\n",
            "Epoch 121/300\n",
            "67/67 [==============================] - 0s 200us/sample - loss: 0.0301 - mse: 0.0301 - mae: 0.1342 - val_loss: 0.0307 - val_mse: 0.0307 - val_mae: 0.1298\n",
            "Epoch 122/300\n",
            "67/67 [==============================] - 0s 225us/sample - loss: 0.0298 - mse: 0.0298 - mae: 0.1330 - val_loss: 0.0317 - val_mse: 0.0317 - val_mae: 0.1331\n",
            "Epoch 123/300\n",
            "67/67 [==============================] - 0s 259us/sample - loss: 0.0292 - mse: 0.0292 - mae: 0.1312 - val_loss: 0.0327 - val_mse: 0.0327 - val_mae: 0.1367\n",
            "Epoch 124/300\n",
            "67/67 [==============================] - 0s 198us/sample - loss: 0.0292 - mse: 0.0292 - mae: 0.1310 - val_loss: 0.0337 - val_mse: 0.0337 - val_mae: 0.1409\n",
            "Epoch 125/300\n",
            "67/67 [==============================] - 0s 237us/sample - loss: 0.0293 - mse: 0.0293 - mae: 0.1319 - val_loss: 0.0331 - val_mse: 0.0331 - val_mae: 0.1395\n",
            "Epoch 126/300\n",
            "67/67 [==============================] - 0s 199us/sample - loss: 0.0291 - mse: 0.0291 - mae: 0.1316 - val_loss: 0.0322 - val_mse: 0.0322 - val_mae: 0.1363\n",
            "Epoch 127/300\n",
            "67/67 [==============================] - 0s 220us/sample - loss: 0.0290 - mse: 0.0290 - mae: 0.1310 - val_loss: 0.0310 - val_mse: 0.0310 - val_mae: 0.1309\n",
            "Epoch 128/300\n",
            "67/67 [==============================] - 0s 194us/sample - loss: 0.0296 - mse: 0.0296 - mae: 0.1323 - val_loss: 0.0306 - val_mse: 0.0306 - val_mae: 0.1281\n",
            "Epoch 129/300\n",
            "67/67 [==============================] - 0s 197us/sample - loss: 0.0296 - mse: 0.0296 - mae: 0.1322 - val_loss: 0.0310 - val_mse: 0.0310 - val_mae: 0.1299\n",
            "Epoch 130/300\n",
            "67/67 [==============================] - 0s 198us/sample - loss: 0.0293 - mse: 0.0293 - mae: 0.1313 - val_loss: 0.0308 - val_mse: 0.0308 - val_mae: 0.1287\n",
            "Epoch 131/300\n",
            "67/67 [==============================] - 0s 191us/sample - loss: 0.0293 - mse: 0.0293 - mae: 0.1313 - val_loss: 0.0321 - val_mse: 0.0321 - val_mae: 0.1337\n",
            "Epoch 132/300\n",
            "67/67 [==============================] - 0s 206us/sample - loss: 0.0289 - mse: 0.0289 - mae: 0.1293 - val_loss: 0.0334 - val_mse: 0.0334 - val_mae: 0.1384\n",
            "Epoch 133/300\n",
            "67/67 [==============================] - 0s 254us/sample - loss: 0.0290 - mse: 0.0290 - mae: 0.1292 - val_loss: 0.0338 - val_mse: 0.0338 - val_mae: 0.1400\n",
            "Epoch 134/300\n",
            "67/67 [==============================] - 0s 197us/sample - loss: 0.0290 - mse: 0.0290 - mae: 0.1292 - val_loss: 0.0318 - val_mse: 0.0318 - val_mae: 0.1340\n",
            "Epoch 135/300\n",
            "67/67 [==============================] - 0s 245us/sample - loss: 0.0286 - mse: 0.0286 - mae: 0.1299 - val_loss: 0.0310 - val_mse: 0.0310 - val_mae: 0.1317\n",
            "Epoch 136/300\n",
            "67/67 [==============================] - 0s 267us/sample - loss: 0.0287 - mse: 0.0287 - mae: 0.1309 - val_loss: 0.0306 - val_mse: 0.0306 - val_mae: 0.1305\n",
            "Epoch 137/300\n",
            "67/67 [==============================] - 0s 251us/sample - loss: 0.0287 - mse: 0.0287 - mae: 0.1312 - val_loss: 0.0301 - val_mse: 0.0301 - val_mae: 0.1289\n",
            "Epoch 138/300\n",
            "67/67 [==============================] - 0s 223us/sample - loss: 0.0288 - mse: 0.0288 - mae: 0.1317 - val_loss: 0.0305 - val_mse: 0.0305 - val_mae: 0.1311\n",
            "Epoch 139/300\n",
            "67/67 [==============================] - 0s 217us/sample - loss: 0.0286 - mse: 0.0286 - mae: 0.1315 - val_loss: 0.0318 - val_mse: 0.0318 - val_mae: 0.1360\n",
            "Epoch 140/300\n",
            "67/67 [==============================] - 0s 255us/sample - loss: 0.0285 - mse: 0.0285 - mae: 0.1302 - val_loss: 0.0329 - val_mse: 0.0329 - val_mae: 0.1390\n",
            "Epoch 141/300\n",
            "67/67 [==============================] - 0s 211us/sample - loss: 0.0286 - mse: 0.0286 - mae: 0.1300 - val_loss: 0.0329 - val_mse: 0.0329 - val_mae: 0.1388\n",
            "Epoch 142/300\n",
            "67/67 [==============================] - 0s 206us/sample - loss: 0.0286 - mse: 0.0286 - mae: 0.1295 - val_loss: 0.0328 - val_mse: 0.0328 - val_mae: 0.1381\n",
            "Epoch 143/300\n",
            "67/67 [==============================] - 0s 200us/sample - loss: 0.0287 - mse: 0.0287 - mae: 0.1293 - val_loss: 0.0328 - val_mse: 0.0328 - val_mae: 0.1382\n",
            "Epoch 144/300\n",
            "67/67 [==============================] - 0s 209us/sample - loss: 0.0285 - mse: 0.0285 - mae: 0.1292 - val_loss: 0.0353 - val_mse: 0.0353 - val_mae: 0.1457\n",
            "Epoch 145/300\n",
            "67/67 [==============================] - 0s 203us/sample - loss: 0.0294 - mse: 0.0294 - mae: 0.1305 - val_loss: 0.0344 - val_mse: 0.0344 - val_mae: 0.1434\n",
            "Epoch 146/300\n",
            "67/67 [==============================] - 0s 233us/sample - loss: 0.0285 - mse: 0.0285 - mae: 0.1288 - val_loss: 0.0316 - val_mse: 0.0316 - val_mae: 0.1336\n",
            "Epoch 147/300\n",
            "67/67 [==============================] - 0s 234us/sample - loss: 0.0282 - mse: 0.0282 - mae: 0.1292 - val_loss: 0.0301 - val_mse: 0.0301 - val_mae: 0.1271\n",
            "Epoch 148/300\n",
            "67/67 [==============================] - 0s 236us/sample - loss: 0.0295 - mse: 0.0295 - mae: 0.1314 - val_loss: 0.0294 - val_mse: 0.0294 - val_mae: 0.1215\n",
            "Epoch 149/300\n",
            "67/67 [==============================] - 0s 233us/sample - loss: 0.0322 - mse: 0.0322 - mae: 0.1357 - val_loss: 0.0290 - val_mse: 0.0290 - val_mae: 0.1230\n",
            "Epoch 150/300\n",
            "67/67 [==============================] - 0s 253us/sample - loss: 0.0304 - mse: 0.0304 - mae: 0.1352 - val_loss: 0.0316 - val_mse: 0.0316 - val_mae: 0.1386\n",
            "Epoch 151/300\n",
            "67/67 [==============================] - 0s 349us/sample - loss: 0.0292 - mse: 0.0292 - mae: 0.1352 - val_loss: 0.0331 - val_mse: 0.0331 - val_mae: 0.1433\n",
            "Epoch 152/300\n",
            "67/67 [==============================] - 0s 204us/sample - loss: 0.0289 - mse: 0.0289 - mae: 0.1335 - val_loss: 0.0307 - val_mse: 0.0307 - val_mae: 0.1325\n",
            "Epoch 153/300\n",
            "67/67 [==============================] - 0s 239us/sample - loss: 0.0282 - mse: 0.0282 - mae: 0.1306 - val_loss: 0.0303 - val_mse: 0.0303 - val_mae: 0.1290\n",
            "Epoch 154/300\n",
            "67/67 [==============================] - 0s 223us/sample - loss: 0.0286 - mse: 0.0286 - mae: 0.1296 - val_loss: 0.0308 - val_mse: 0.0308 - val_mae: 0.1305\n",
            "Epoch 155/300\n",
            "67/67 [==============================] - 0s 188us/sample - loss: 0.0282 - mse: 0.0282 - mae: 0.1286 - val_loss: 0.0329 - val_mse: 0.0329 - val_mae: 0.1394\n",
            "Epoch 156/300\n",
            "67/67 [==============================] - 0s 194us/sample - loss: 0.0278 - mse: 0.0278 - mae: 0.1282 - val_loss: 0.0381 - val_mse: 0.0381 - val_mae: 0.1539\n",
            "Epoch 157/300\n",
            "67/67 [==============================] - 0s 187us/sample - loss: 0.0306 - mse: 0.0306 - mae: 0.1332 - val_loss: 0.0396 - val_mse: 0.0396 - val_mae: 0.1567\n",
            "Epoch 158/300\n",
            "67/67 [==============================] - 0s 191us/sample - loss: 0.0309 - mse: 0.0309 - mae: 0.1333 - val_loss: 0.0357 - val_mse: 0.0357 - val_mae: 0.1476\n",
            "Epoch 159/300\n",
            "67/67 [==============================] - 0s 228us/sample - loss: 0.0279 - mse: 0.0279 - mae: 0.1265 - val_loss: 0.0309 - val_mse: 0.0309 - val_mae: 0.1302\n",
            "Epoch 160/300\n",
            "67/67 [==============================] - 0s 229us/sample - loss: 0.0311 - mse: 0.0311 - mae: 0.1358 - val_loss: 0.0326 - val_mse: 0.0326 - val_mae: 0.1254\n",
            "Epoch 161/300\n",
            "67/67 [==============================] - 0s 207us/sample - loss: 0.0354 - mse: 0.0354 - mae: 0.1378 - val_loss: 0.0306 - val_mse: 0.0306 - val_mae: 0.1236\n",
            "Epoch 162/300\n",
            "67/67 [==============================] - 0s 235us/sample - loss: 0.0303 - mse: 0.0303 - mae: 0.1313 - val_loss: 0.0324 - val_mse: 0.0324 - val_mae: 0.1390\n",
            "Epoch 163/300\n",
            "67/67 [==============================] - 0s 247us/sample - loss: 0.0289 - mse: 0.0289 - mae: 0.1313 - val_loss: 0.0416 - val_mse: 0.0416 - val_mae: 0.1631\n",
            "Epoch 164/300\n",
            "67/67 [==============================] - 0s 235us/sample - loss: 0.0337 - mse: 0.0337 - mae: 0.1430 - val_loss: 0.0459 - val_mse: 0.0459 - val_mae: 0.1724\n",
            "Epoch 165/300\n",
            "67/67 [==============================] - 0s 223us/sample - loss: 0.0350 - mse: 0.0350 - mae: 0.1458 - val_loss: 0.0414 - val_mse: 0.0414 - val_mae: 0.1618\n",
            "Epoch 166/300\n",
            "67/67 [==============================] - 0s 204us/sample - loss: 0.0319 - mse: 0.0319 - mae: 0.1363 - val_loss: 0.0362 - val_mse: 0.0362 - val_mae: 0.1487\n",
            "Epoch 167/300\n",
            "67/67 [==============================] - 0s 227us/sample - loss: 0.0285 - mse: 0.0285 - mae: 0.1282 - val_loss: 0.0315 - val_mse: 0.0315 - val_mae: 0.1326\n",
            "Epoch 168/300\n",
            "67/67 [==============================] - 0s 251us/sample - loss: 0.0282 - mse: 0.0282 - mae: 0.1290 - val_loss: 0.0304 - val_mse: 0.0304 - val_mae: 0.1268\n",
            "Epoch 169/300\n",
            "67/67 [==============================] - 0s 214us/sample - loss: 0.0294 - mse: 0.0294 - mae: 0.1303 - val_loss: 0.0299 - val_mse: 0.0299 - val_mae: 0.1254\n",
            "Epoch 170/300\n",
            "67/67 [==============================] - 0s 242us/sample - loss: 0.0288 - mse: 0.0288 - mae: 0.1299 - val_loss: 0.0292 - val_mse: 0.0292 - val_mae: 0.1249\n",
            "Epoch 171/300\n",
            "67/67 [==============================] - 0s 242us/sample - loss: 0.0282 - mse: 0.0282 - mae: 0.1317 - val_loss: 0.0299 - val_mse: 0.0299 - val_mae: 0.1310\n",
            "Epoch 172/300\n",
            "67/67 [==============================] - 0s 208us/sample - loss: 0.0278 - mse: 0.0278 - mae: 0.1326 - val_loss: 0.0317 - val_mse: 0.0317 - val_mae: 0.1382\n",
            "Epoch 173/300\n",
            "67/67 [==============================] - 0s 258us/sample - loss: 0.0284 - mse: 0.0284 - mae: 0.1327 - val_loss: 0.0332 - val_mse: 0.0332 - val_mae: 0.1420\n",
            "Epoch 174/300\n",
            "67/67 [==============================] - 0s 186us/sample - loss: 0.0284 - mse: 0.0284 - mae: 0.1307 - val_loss: 0.0332 - val_mse: 0.0332 - val_mae: 0.1405\n",
            "Epoch 175/300\n",
            "67/67 [==============================] - 0s 219us/sample - loss: 0.0275 - mse: 0.0275 - mae: 0.1274 - val_loss: 0.0312 - val_mse: 0.0312 - val_mae: 0.1320\n",
            "Epoch 176/300\n",
            "67/67 [==============================] - 0s 215us/sample - loss: 0.0286 - mse: 0.0286 - mae: 0.1289 - val_loss: 0.0311 - val_mse: 0.0311 - val_mae: 0.1304\n",
            "Epoch 177/300\n",
            "67/67 [==============================] - 0s 221us/sample - loss: 0.0295 - mse: 0.0295 - mae: 0.1316 - val_loss: 0.0312 - val_mse: 0.0312 - val_mae: 0.1313\n",
            "Epoch 178/300\n",
            "67/67 [==============================] - 0s 220us/sample - loss: 0.0289 - mse: 0.0289 - mae: 0.1305 - val_loss: 0.0311 - val_mse: 0.0311 - val_mae: 0.1313\n",
            "Epoch 179/300\n",
            "67/67 [==============================] - 0s 260us/sample - loss: 0.0277 - mse: 0.0277 - mae: 0.1276 - val_loss: 0.0329 - val_mse: 0.0329 - val_mae: 0.1394\n",
            "Epoch 180/300\n",
            "67/67 [==============================] - 0s 230us/sample - loss: 0.0286 - mse: 0.0286 - mae: 0.1302 - val_loss: 0.0387 - val_mse: 0.0387 - val_mae: 0.1551\n",
            "Epoch 181/300\n",
            "67/67 [==============================] - 0s 221us/sample - loss: 0.0309 - mse: 0.0309 - mae: 0.1348 - val_loss: 0.0368 - val_mse: 0.0368 - val_mae: 0.1513\n",
            "Epoch 182/300\n",
            "67/67 [==============================] - 0s 258us/sample - loss: 0.0295 - mse: 0.0295 - mae: 0.1317 - val_loss: 0.0320 - val_mse: 0.0320 - val_mae: 0.1380\n",
            "Epoch 183/300\n",
            "67/67 [==============================] - 0s 231us/sample - loss: 0.0282 - mse: 0.0282 - mae: 0.1305 - val_loss: 0.0293 - val_mse: 0.0293 - val_mae: 0.1248\n",
            "Epoch 184/300\n",
            "67/67 [==============================] - 0s 234us/sample - loss: 0.0286 - mse: 0.0286 - mae: 0.1297 - val_loss: 0.0294 - val_mse: 0.0294 - val_mae: 0.1214\n",
            "Epoch 185/300\n",
            "67/67 [==============================] - 0s 248us/sample - loss: 0.0302 - mse: 0.0302 - mae: 0.1312 - val_loss: 0.0296 - val_mse: 0.0296 - val_mae: 0.1212\n",
            "Epoch 186/300\n",
            "67/67 [==============================] - 0s 281us/sample - loss: 0.0305 - mse: 0.0305 - mae: 0.1318 - val_loss: 0.0292 - val_mse: 0.0292 - val_mae: 0.1236\n",
            "Epoch 187/300\n",
            "67/67 [==============================] - 0s 232us/sample - loss: 0.0285 - mse: 0.0285 - mae: 0.1297 - val_loss: 0.0290 - val_mse: 0.0290 - val_mae: 0.1243\n",
            "Epoch 188/300\n",
            "67/67 [==============================] - 0s 234us/sample - loss: 0.0285 - mse: 0.0285 - mae: 0.1303 - val_loss: 0.0288 - val_mse: 0.0288 - val_mae: 0.1241\n",
            "Epoch 189/300\n",
            "67/67 [==============================] - 0s 291us/sample - loss: 0.0276 - mse: 0.0276 - mae: 0.1296 - val_loss: 0.0306 - val_mse: 0.0306 - val_mae: 0.1345\n",
            "Epoch 190/300\n",
            "67/67 [==============================] - 0s 253us/sample - loss: 0.0282 - mse: 0.0282 - mae: 0.1324 - val_loss: 0.0337 - val_mse: 0.0337 - val_mae: 0.1445\n",
            "Epoch 191/300\n",
            "67/67 [==============================] - 0s 214us/sample - loss: 0.0290 - mse: 0.0290 - mae: 0.1336 - val_loss: 0.0345 - val_mse: 0.0345 - val_mae: 0.1460\n",
            "Epoch 192/300\n",
            "67/67 [==============================] - 0s 237us/sample - loss: 0.0288 - mse: 0.0288 - mae: 0.1311 - val_loss: 0.0369 - val_mse: 0.0369 - val_mae: 0.1512\n",
            "Epoch 193/300\n",
            "67/67 [==============================] - 0s 221us/sample - loss: 0.0299 - mse: 0.0299 - mae: 0.1322 - val_loss: 0.0370 - val_mse: 0.0370 - val_mae: 0.1504\n",
            "Epoch 194/300\n",
            "67/67 [==============================] - 0s 203us/sample - loss: 0.0292 - mse: 0.0292 - mae: 0.1294 - val_loss: 0.0327 - val_mse: 0.0327 - val_mae: 0.1377\n",
            "Epoch 195/300\n",
            "67/67 [==============================] - 0s 193us/sample - loss: 0.0278 - mse: 0.0278 - mae: 0.1271 - val_loss: 0.0306 - val_mse: 0.0306 - val_mae: 0.1292\n",
            "Epoch 196/300\n",
            "67/67 [==============================] - 0s 205us/sample - loss: 0.0279 - mse: 0.0279 - mae: 0.1286 - val_loss: 0.0304 - val_mse: 0.0304 - val_mae: 0.1288\n",
            "Epoch 197/300\n",
            "67/67 [==============================] - 0s 224us/sample - loss: 0.0280 - mse: 0.0280 - mae: 0.1281 - val_loss: 0.0313 - val_mse: 0.0313 - val_mae: 0.1328\n",
            "Epoch 198/300\n",
            "67/67 [==============================] - 0s 204us/sample - loss: 0.0271 - mse: 0.0271 - mae: 0.1257 - val_loss: 0.0326 - val_mse: 0.0326 - val_mae: 0.1368\n",
            "Epoch 199/300\n",
            "67/67 [==============================] - 0s 223us/sample - loss: 0.0273 - mse: 0.0273 - mae: 0.1256 - val_loss: 0.0325 - val_mse: 0.0325 - val_mae: 0.1366\n",
            "Epoch 200/300\n",
            "67/67 [==============================] - 0s 242us/sample - loss: 0.0272 - mse: 0.0272 - mae: 0.1256 - val_loss: 0.0331 - val_mse: 0.0331 - val_mae: 0.1398\n",
            "Epoch 201/300\n",
            "67/67 [==============================] - 0s 234us/sample - loss: 0.0275 - mse: 0.0275 - mae: 0.1268 - val_loss: 0.0334 - val_mse: 0.0334 - val_mae: 0.1414\n",
            "Epoch 202/300\n",
            "67/67 [==============================] - 0s 211us/sample - loss: 0.0276 - mse: 0.0276 - mae: 0.1272 - val_loss: 0.0317 - val_mse: 0.0317 - val_mae: 0.1361\n",
            "Epoch 203/300\n",
            "67/67 [==============================] - 0s 239us/sample - loss: 0.0271 - mse: 0.0271 - mae: 0.1273 - val_loss: 0.0303 - val_mse: 0.0303 - val_mae: 0.1310\n",
            "Epoch 204/300\n",
            "67/67 [==============================] - 0s 219us/sample - loss: 0.0270 - mse: 0.0270 - mae: 0.1274 - val_loss: 0.0314 - val_mse: 0.0314 - val_mae: 0.1352\n",
            "Epoch 205/300\n",
            "67/67 [==============================] - 0s 196us/sample - loss: 0.0270 - mse: 0.0270 - mae: 0.1270 - val_loss: 0.0339 - val_mse: 0.0339 - val_mae: 0.1423\n",
            "Epoch 206/300\n",
            "67/67 [==============================] - 0s 207us/sample - loss: 0.0284 - mse: 0.0284 - mae: 0.1284 - val_loss: 0.0386 - val_mse: 0.0386 - val_mae: 0.1527\n",
            "Epoch 207/300\n",
            "67/67 [==============================] - 0s 315us/sample - loss: 0.0305 - mse: 0.0305 - mae: 0.1311 - val_loss: 0.0348 - val_mse: 0.0348 - val_mae: 0.1415\n",
            "Epoch 208/300\n",
            "67/67 [==============================] - 0s 210us/sample - loss: 0.0282 - mse: 0.0282 - mae: 0.1271 - val_loss: 0.0317 - val_mse: 0.0317 - val_mae: 0.1316\n",
            "Epoch 209/300\n",
            "67/67 [==============================] - 0s 223us/sample - loss: 0.0271 - mse: 0.0271 - mae: 0.1265 - val_loss: 0.0300 - val_mse: 0.0300 - val_mae: 0.1256\n",
            "Epoch 210/300\n",
            "67/67 [==============================] - 0s 217us/sample - loss: 0.0280 - mse: 0.0280 - mae: 0.1291 - val_loss: 0.0292 - val_mse: 0.0292 - val_mae: 0.1235\n",
            "Epoch 211/300\n",
            "67/67 [==============================] - 0s 211us/sample - loss: 0.0271 - mse: 0.0271 - mae: 0.1282 - val_loss: 0.0314 - val_mse: 0.0314 - val_mae: 0.1347\n",
            "Epoch 212/300\n",
            "67/67 [==============================] - 0s 228us/sample - loss: 0.0274 - mse: 0.0274 - mae: 0.1296 - val_loss: 0.0345 - val_mse: 0.0345 - val_mae: 0.1449\n",
            "Epoch 213/300\n",
            "67/67 [==============================] - 0s 208us/sample - loss: 0.0294 - mse: 0.0294 - mae: 0.1355 - val_loss: 0.0344 - val_mse: 0.0344 - val_mae: 0.1450\n",
            "Epoch 214/300\n",
            "67/67 [==============================] - 0s 206us/sample - loss: 0.0293 - mse: 0.0293 - mae: 0.1351 - val_loss: 0.0327 - val_mse: 0.0327 - val_mae: 0.1398\n",
            "Epoch 215/300\n",
            "67/67 [==============================] - 0s 279us/sample - loss: 0.0282 - mse: 0.0282 - mae: 0.1323 - val_loss: 0.0314 - val_mse: 0.0314 - val_mae: 0.1351\n",
            "Epoch 216/300\n",
            "67/67 [==============================] - 0s 215us/sample - loss: 0.0275 - mse: 0.0275 - mae: 0.1311 - val_loss: 0.0313 - val_mse: 0.0313 - val_mae: 0.1334\n",
            "Epoch 217/300\n",
            "67/67 [==============================] - 0s 245us/sample - loss: 0.0269 - mse: 0.0269 - mae: 0.1276 - val_loss: 0.0342 - val_mse: 0.0342 - val_mae: 0.1404\n",
            "Epoch 218/300\n",
            "67/67 [==============================] - 0s 239us/sample - loss: 0.0282 - mse: 0.0282 - mae: 0.1270 - val_loss: 0.0350 - val_mse: 0.0350 - val_mae: 0.1421\n",
            "Epoch 219/300\n",
            "67/67 [==============================] - 0s 231us/sample - loss: 0.0277 - mse: 0.0277 - mae: 0.1255 - val_loss: 0.0329 - val_mse: 0.0329 - val_mae: 0.1374\n",
            "Epoch 220/300\n",
            "67/67 [==============================] - 0s 240us/sample - loss: 0.0272 - mse: 0.0272 - mae: 0.1253 - val_loss: 0.0342 - val_mse: 0.0342 - val_mae: 0.1420\n",
            "Epoch 221/300\n",
            "67/67 [==============================] - 0s 243us/sample - loss: 0.0273 - mse: 0.0273 - mae: 0.1245 - val_loss: 0.0362 - val_mse: 0.0362 - val_mae: 0.1471\n",
            "Epoch 222/300\n",
            "67/67 [==============================] - 0s 230us/sample - loss: 0.0286 - mse: 0.0286 - mae: 0.1265 - val_loss: 0.0382 - val_mse: 0.0382 - val_mae: 0.1513\n",
            "Epoch 223/300\n",
            "67/67 [==============================] - 0s 294us/sample - loss: 0.0297 - mse: 0.0297 - mae: 0.1289 - val_loss: 0.0363 - val_mse: 0.0363 - val_mae: 0.1470\n",
            "Epoch 224/300\n",
            "67/67 [==============================] - 0s 272us/sample - loss: 0.0283 - mse: 0.0283 - mae: 0.1259 - val_loss: 0.0323 - val_mse: 0.0323 - val_mae: 0.1363\n",
            "Epoch 225/300\n",
            "67/67 [==============================] - 0s 266us/sample - loss: 0.0266 - mse: 0.0266 - mae: 0.1243 - val_loss: 0.0309 - val_mse: 0.0309 - val_mae: 0.1318\n",
            "Epoch 226/300\n",
            "67/67 [==============================] - 0s 279us/sample - loss: 0.0265 - mse: 0.0265 - mae: 0.1254 - val_loss: 0.0299 - val_mse: 0.0299 - val_mae: 0.1281\n",
            "Epoch 227/300\n",
            "67/67 [==============================] - 0s 392us/sample - loss: 0.0267 - mse: 0.0267 - mae: 0.1267 - val_loss: 0.0295 - val_mse: 0.0295 - val_mae: 0.1260\n",
            "Epoch 228/300\n",
            "67/67 [==============================] - 0s 313us/sample - loss: 0.0269 - mse: 0.0269 - mae: 0.1261 - val_loss: 0.0315 - val_mse: 0.0315 - val_mae: 0.1341\n",
            "Epoch 229/300\n",
            "67/67 [==============================] - 0s 237us/sample - loss: 0.0266 - mse: 0.0266 - mae: 0.1246 - val_loss: 0.0332 - val_mse: 0.0332 - val_mae: 0.1387\n",
            "Epoch 230/300\n",
            "67/67 [==============================] - 0s 212us/sample - loss: 0.0276 - mse: 0.0276 - mae: 0.1260 - val_loss: 0.0363 - val_mse: 0.0363 - val_mae: 0.1470\n",
            "Epoch 231/300\n",
            "67/67 [==============================] - 0s 211us/sample - loss: 0.0300 - mse: 0.0300 - mae: 0.1301 - val_loss: 0.0379 - val_mse: 0.0379 - val_mae: 0.1501\n",
            "Epoch 232/300\n",
            "67/67 [==============================] - 0s 211us/sample - loss: 0.0286 - mse: 0.0286 - mae: 0.1265 - val_loss: 0.0310 - val_mse: 0.0310 - val_mae: 0.1307\n",
            "Epoch 233/300\n",
            "67/67 [==============================] - 0s 253us/sample - loss: 0.0283 - mse: 0.0283 - mae: 0.1287 - val_loss: 0.0317 - val_mse: 0.0317 - val_mae: 0.1265\n",
            "Epoch 234/300\n",
            "67/67 [==============================] - 0s 251us/sample - loss: 0.0325 - mse: 0.0325 - mae: 0.1358 - val_loss: 0.0289 - val_mse: 0.0289 - val_mae: 0.1220\n",
            "Epoch 235/300\n",
            "67/67 [==============================] - 0s 220us/sample - loss: 0.0275 - mse: 0.0275 - mae: 0.1272 - val_loss: 0.0348 - val_mse: 0.0348 - val_mae: 0.1458\n",
            "Epoch 236/300\n",
            "67/67 [==============================] - 0s 221us/sample - loss: 0.0287 - mse: 0.0287 - mae: 0.1308 - val_loss: 0.0369 - val_mse: 0.0369 - val_mae: 0.1521\n",
            "Epoch 237/300\n",
            "67/67 [==============================] - 0s 224us/sample - loss: 0.0291 - mse: 0.0291 - mae: 0.1326 - val_loss: 0.0307 - val_mse: 0.0307 - val_mae: 0.1348\n",
            "Epoch 238/300\n",
            "67/67 [==============================] - 0s 233us/sample - loss: 0.0274 - mse: 0.0274 - mae: 0.1297 - val_loss: 0.0282 - val_mse: 0.0282 - val_mae: 0.1208\n",
            "Epoch 239/300\n",
            "67/67 [==============================] - 0s 205us/sample - loss: 0.0281 - mse: 0.0281 - mae: 0.1290 - val_loss: 0.0285 - val_mse: 0.0285 - val_mae: 0.1198\n",
            "Epoch 240/300\n",
            "67/67 [==============================] - 0s 234us/sample - loss: 0.0285 - mse: 0.0285 - mae: 0.1287 - val_loss: 0.0291 - val_mse: 0.0291 - val_mae: 0.1233\n",
            "Epoch 241/300\n",
            "67/67 [==============================] - 0s 276us/sample - loss: 0.0275 - mse: 0.0275 - mae: 0.1268 - val_loss: 0.0299 - val_mse: 0.0299 - val_mae: 0.1281\n",
            "Epoch 242/300\n",
            "67/67 [==============================] - 0s 240us/sample - loss: 0.0271 - mse: 0.0271 - mae: 0.1259 - val_loss: 0.0310 - val_mse: 0.0310 - val_mae: 0.1337\n",
            "Epoch 243/300\n",
            "67/67 [==============================] - 0s 217us/sample - loss: 0.0266 - mse: 0.0266 - mae: 0.1249 - val_loss: 0.0313 - val_mse: 0.0313 - val_mae: 0.1356\n",
            "Epoch 244/300\n",
            "67/67 [==============================] - 0s 308us/sample - loss: 0.0265 - mse: 0.0265 - mae: 0.1260 - val_loss: 0.0301 - val_mse: 0.0301 - val_mae: 0.1338\n",
            "Epoch 245/300\n",
            "67/67 [==============================] - 0s 238us/sample - loss: 0.0269 - mse: 0.0269 - mae: 0.1294 - val_loss: 0.0290 - val_mse: 0.0290 - val_mae: 0.1308\n",
            "Epoch 246/300\n",
            "67/67 [==============================] - 0s 231us/sample - loss: 0.0270 - mse: 0.0270 - mae: 0.1307 - val_loss: 0.0277 - val_mse: 0.0277 - val_mae: 0.1237\n",
            "Epoch 247/300\n",
            "67/67 [==============================] - 0s 254us/sample - loss: 0.0271 - mse: 0.0271 - mae: 0.1305 - val_loss: 0.0280 - val_mse: 0.0280 - val_mae: 0.1222\n",
            "Epoch 248/300\n",
            "67/67 [==============================] - 0s 239us/sample - loss: 0.0267 - mse: 0.0267 - mae: 0.1283 - val_loss: 0.0305 - val_mse: 0.0305 - val_mae: 0.1319\n",
            "Epoch 249/300\n",
            "67/67 [==============================] - 0s 234us/sample - loss: 0.0265 - mse: 0.0265 - mae: 0.1254 - val_loss: 0.0318 - val_mse: 0.0318 - val_mae: 0.1353\n",
            "Epoch 250/300\n",
            "67/67 [==============================] - 0s 241us/sample - loss: 0.0263 - mse: 0.0263 - mae: 0.1241 - val_loss: 0.0303 - val_mse: 0.0303 - val_mae: 0.1292\n",
            "Epoch 251/300\n",
            "67/67 [==============================] - 0s 251us/sample - loss: 0.0267 - mse: 0.0267 - mae: 0.1254 - val_loss: 0.0308 - val_mse: 0.0308 - val_mae: 0.1315\n",
            "Epoch 252/300\n",
            "67/67 [==============================] - 0s 267us/sample - loss: 0.0266 - mse: 0.0266 - mae: 0.1257 - val_loss: 0.0330 - val_mse: 0.0330 - val_mae: 0.1390\n",
            "Epoch 253/300\n",
            "67/67 [==============================] - 0s 275us/sample - loss: 0.0274 - mse: 0.0274 - mae: 0.1243 - val_loss: 0.0354 - val_mse: 0.0354 - val_mae: 0.1451\n",
            "Epoch 254/300\n",
            "67/67 [==============================] - 0s 278us/sample - loss: 0.0274 - mse: 0.0274 - mae: 0.1237 - val_loss: 0.0324 - val_mse: 0.0324 - val_mae: 0.1373\n",
            "Epoch 255/300\n",
            "67/67 [==============================] - 0s 255us/sample - loss: 0.0267 - mse: 0.0267 - mae: 0.1244 - val_loss: 0.0302 - val_mse: 0.0302 - val_mae: 0.1290\n",
            "Epoch 256/300\n",
            "67/67 [==============================] - 0s 259us/sample - loss: 0.0265 - mse: 0.0265 - mae: 0.1252 - val_loss: 0.0295 - val_mse: 0.0295 - val_mae: 0.1261\n",
            "Epoch 257/300\n",
            "67/67 [==============================] - 0s 240us/sample - loss: 0.0265 - mse: 0.0265 - mae: 0.1255 - val_loss: 0.0304 - val_mse: 0.0304 - val_mae: 0.1308\n",
            "Epoch 258/300\n",
            "67/67 [==============================] - 0s 288us/sample - loss: 0.0261 - mse: 0.0261 - mae: 0.1246 - val_loss: 0.0341 - val_mse: 0.0341 - val_mae: 0.1411\n",
            "Epoch 259/300\n",
            "67/67 [==============================] - 0s 282us/sample - loss: 0.0278 - mse: 0.0278 - mae: 0.1254 - val_loss: 0.0383 - val_mse: 0.0383 - val_mae: 0.1510\n",
            "Epoch 260/300\n",
            "67/67 [==============================] - 0s 210us/sample - loss: 0.0301 - mse: 0.0301 - mae: 0.1290 - val_loss: 0.0388 - val_mse: 0.0388 - val_mae: 0.1514\n",
            "Epoch 261/300\n",
            "67/67 [==============================] - 0s 233us/sample - loss: 0.0299 - mse: 0.0299 - mae: 0.1269 - val_loss: 0.0357 - val_mse: 0.0357 - val_mae: 0.1439\n",
            "Epoch 262/300\n",
            "67/67 [==============================] - 0s 262us/sample - loss: 0.0279 - mse: 0.0279 - mae: 0.1243 - val_loss: 0.0335 - val_mse: 0.0335 - val_mae: 0.1397\n",
            "Epoch 263/300\n",
            "67/67 [==============================] - 0s 254us/sample - loss: 0.0276 - mse: 0.0276 - mae: 0.1261 - val_loss: 0.0330 - val_mse: 0.0330 - val_mae: 0.1387\n",
            "Epoch 264/300\n",
            "67/67 [==============================] - 0s 272us/sample - loss: 0.0267 - mse: 0.0267 - mae: 0.1269 - val_loss: 0.0308 - val_mse: 0.0308 - val_mae: 0.1301\n",
            "Epoch 265/300\n",
            "67/67 [==============================] - 0s 225us/sample - loss: 0.0287 - mse: 0.0287 - mae: 0.1303 - val_loss: 0.0297 - val_mse: 0.0297 - val_mae: 0.1241\n",
            "Epoch 266/300\n",
            "67/67 [==============================] - 0s 241us/sample - loss: 0.0281 - mse: 0.0281 - mae: 0.1279 - val_loss: 0.0293 - val_mse: 0.0293 - val_mae: 0.1267\n",
            "Epoch 267/300\n",
            "67/67 [==============================] - 0s 216us/sample - loss: 0.0271 - mse: 0.0271 - mae: 0.1274 - val_loss: 0.0311 - val_mse: 0.0311 - val_mae: 0.1360\n",
            "Epoch 268/300\n",
            "67/67 [==============================] - 0s 254us/sample - loss: 0.0270 - mse: 0.0270 - mae: 0.1293 - val_loss: 0.0287 - val_mse: 0.0287 - val_mae: 0.1284\n",
            "Epoch 269/300\n",
            "67/67 [==============================] - 0s 218us/sample - loss: 0.0267 - mse: 0.0267 - mae: 0.1294 - val_loss: 0.0286 - val_mse: 0.0286 - val_mae: 0.1290\n",
            "Epoch 270/300\n",
            "67/67 [==============================] - 0s 228us/sample - loss: 0.0269 - mse: 0.0269 - mae: 0.1304 - val_loss: 0.0284 - val_mse: 0.0284 - val_mae: 0.1277\n",
            "Epoch 271/300\n",
            "67/67 [==============================] - 0s 247us/sample - loss: 0.0266 - mse: 0.0266 - mae: 0.1294 - val_loss: 0.0285 - val_mse: 0.0285 - val_mae: 0.1275\n",
            "Epoch 272/300\n",
            "67/67 [==============================] - 0s 234us/sample - loss: 0.0264 - mse: 0.0264 - mae: 0.1279 - val_loss: 0.0294 - val_mse: 0.0294 - val_mae: 0.1302\n",
            "Epoch 273/300\n",
            "67/67 [==============================] - 0s 233us/sample - loss: 0.0261 - mse: 0.0261 - mae: 0.1261 - val_loss: 0.0309 - val_mse: 0.0309 - val_mae: 0.1340\n",
            "Epoch 274/300\n",
            "67/67 [==============================] - 0s 199us/sample - loss: 0.0261 - mse: 0.0261 - mae: 0.1239 - val_loss: 0.0324 - val_mse: 0.0324 - val_mae: 0.1377\n",
            "Epoch 275/300\n",
            "67/67 [==============================] - 0s 200us/sample - loss: 0.0265 - mse: 0.0265 - mae: 0.1235 - val_loss: 0.0314 - val_mse: 0.0314 - val_mae: 0.1342\n",
            "Epoch 276/300\n",
            "67/67 [==============================] - 0s 224us/sample - loss: 0.0291 - mse: 0.0291 - mae: 0.1313 - val_loss: 0.0314 - val_mse: 0.0314 - val_mae: 0.1306\n",
            "Epoch 277/300\n",
            "67/67 [==============================] - 0s 228us/sample - loss: 0.0292 - mse: 0.0292 - mae: 0.1322 - val_loss: 0.0307 - val_mse: 0.0307 - val_mae: 0.1312\n",
            "Epoch 278/300\n",
            "67/67 [==============================] - 0s 263us/sample - loss: 0.0260 - mse: 0.0260 - mae: 0.1232 - val_loss: 0.0355 - val_mse: 0.0355 - val_mae: 0.1451\n",
            "Epoch 279/300\n",
            "67/67 [==============================] - 0s 178us/sample - loss: 0.0282 - mse: 0.0282 - mae: 0.1265 - val_loss: 0.0397 - val_mse: 0.0397 - val_mae: 0.1564\n",
            "Epoch 280/300\n",
            "67/67 [==============================] - 0s 233us/sample - loss: 0.0310 - mse: 0.0310 - mae: 0.1332 - val_loss: 0.0354 - val_mse: 0.0354 - val_mae: 0.1449\n",
            "Epoch 281/300\n",
            "67/67 [==============================] - 0s 218us/sample - loss: 0.0282 - mse: 0.0282 - mae: 0.1259 - val_loss: 0.0295 - val_mse: 0.0295 - val_mae: 0.1264\n",
            "Epoch 282/300\n",
            "67/67 [==============================] - 0s 183us/sample - loss: 0.0262 - mse: 0.0262 - mae: 0.1246 - val_loss: 0.0290 - val_mse: 0.0290 - val_mae: 0.1237\n",
            "Epoch 283/300\n",
            "67/67 [==============================] - 0s 243us/sample - loss: 0.0267 - mse: 0.0267 - mae: 0.1256 - val_loss: 0.0305 - val_mse: 0.0305 - val_mae: 0.1303\n",
            "Epoch 284/300\n",
            "67/67 [==============================] - 0s 201us/sample - loss: 0.0264 - mse: 0.0264 - mae: 0.1236 - val_loss: 0.0314 - val_mse: 0.0314 - val_mae: 0.1330\n",
            "Epoch 285/300\n",
            "67/67 [==============================] - 0s 201us/sample - loss: 0.0262 - mse: 0.0262 - mae: 0.1237 - val_loss: 0.0295 - val_mse: 0.0295 - val_mae: 0.1257\n",
            "Epoch 286/300\n",
            "67/67 [==============================] - 0s 205us/sample - loss: 0.0274 - mse: 0.0274 - mae: 0.1272 - val_loss: 0.0302 - val_mse: 0.0302 - val_mae: 0.1248\n",
            "Epoch 287/300\n",
            "67/67 [==============================] - 0s 205us/sample - loss: 0.0310 - mse: 0.0310 - mae: 0.1340 - val_loss: 0.0294 - val_mse: 0.0294 - val_mae: 0.1223\n",
            "Epoch 288/300\n",
            "67/67 [==============================] - 0s 211us/sample - loss: 0.0275 - mse: 0.0275 - mae: 0.1275 - val_loss: 0.0302 - val_mse: 0.0302 - val_mae: 0.1309\n",
            "Epoch 289/300\n",
            "67/67 [==============================] - 0s 326us/sample - loss: 0.0283 - mse: 0.0283 - mae: 0.1290 - val_loss: 0.0424 - val_mse: 0.0424 - val_mae: 0.1650\n",
            "Epoch 290/300\n",
            "67/67 [==============================] - 0s 235us/sample - loss: 0.0352 - mse: 0.0352 - mae: 0.1467 - val_loss: 0.0416 - val_mse: 0.0416 - val_mae: 0.1632\n",
            "Epoch 291/300\n",
            "67/67 [==============================] - 0s 227us/sample - loss: 0.0324 - mse: 0.0324 - mae: 0.1405 - val_loss: 0.0288 - val_mse: 0.0288 - val_mae: 0.1253\n",
            "Epoch 292/300\n",
            "67/67 [==============================] - 0s 250us/sample - loss: 0.0259 - mse: 0.0259 - mae: 0.1244 - val_loss: 0.0307 - val_mse: 0.0307 - val_mae: 0.1225\n",
            "Epoch 293/300\n",
            "67/67 [==============================] - 0s 214us/sample - loss: 0.0337 - mse: 0.0337 - mae: 0.1390 - val_loss: 0.0327 - val_mse: 0.0327 - val_mae: 0.1257\n",
            "Epoch 294/300\n",
            "67/67 [==============================] - 0s 215us/sample - loss: 0.0342 - mse: 0.0342 - mae: 0.1388 - val_loss: 0.0289 - val_mse: 0.0289 - val_mae: 0.1200\n",
            "Epoch 295/300\n",
            "67/67 [==============================] - 0s 186us/sample - loss: 0.0282 - mse: 0.0282 - mae: 0.1276 - val_loss: 0.0287 - val_mse: 0.0287 - val_mae: 0.1232\n",
            "Epoch 296/300\n",
            "67/67 [==============================] - 0s 217us/sample - loss: 0.0262 - mse: 0.0262 - mae: 0.1252 - val_loss: 0.0296 - val_mse: 0.0296 - val_mae: 0.1292\n",
            "Epoch 297/300\n",
            "67/67 [==============================] - 0s 200us/sample - loss: 0.0261 - mse: 0.0261 - mae: 0.1254 - val_loss: 0.0315 - val_mse: 0.0315 - val_mae: 0.1351\n",
            "Epoch 298/300\n",
            "67/67 [==============================] - 0s 239us/sample - loss: 0.0269 - mse: 0.0269 - mae: 0.1259 - val_loss: 0.0321 - val_mse: 0.0321 - val_mae: 0.1354\n",
            "Epoch 299/300\n",
            "67/67 [==============================] - 0s 229us/sample - loss: 0.0265 - mse: 0.0265 - mae: 0.1238 - val_loss: 0.0309 - val_mse: 0.0309 - val_mae: 0.1306\n",
            "Epoch 300/300\n",
            "67/67 [==============================] - 0s 285us/sample - loss: 0.0268 - mse: 0.0268 - mae: 0.1256 - val_loss: 0.0302 - val_mse: 0.0302 - val_mae: 0.1269\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v5aNfodbA8xs",
        "colab_type": "code",
        "outputId": "9faeab74-0db6-4731-a09f-3c3593dfb710",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "model.evaluate(X_test, y_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0824 09:07:58.203101 140072041154432 training.py:510] Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "38/38 [==============================] - 0s 632us/sample - loss: 0.0547 - mse: 0.0547 - mae: 0.1658\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.05473129686556364, 0.054731295, 0.16584389]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8vSB68BWBCFk",
        "colab_type": "code",
        "outputId": "1aa5c0ed-2def-44ec-812c-c5a84ccdeae7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "y_pred = model.predict_classes(X_test[0:3])\n",
        "y_pred"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0824 09:07:58.467851 140072041154432 training.py:510] Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1],\n",
              "       [1],\n",
              "       [1]], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FgT_AsS6Bhum",
        "colab_type": "code",
        "outputId": "d3a82565-3705-4b65-f160-18b9444a4dba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        }
      },
      "source": [
        "y_pred,y_test[0:3]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[1],\n",
              "        [1],\n",
              "        [1]], dtype=int32), 71     1.3\n",
              " 104    2.2\n",
              " 128    2.1\n",
              " Name: 3, dtype: float64)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DywDmLzTCgof",
        "colab_type": "code",
        "outputId": "b12f37f0-3ce4-420f-8b95-a173f74e693a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "source": [
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model Loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Training','Testing'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEWCAYAAAB1xKBvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O7WD7TnbD2d9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g7E1bt2uY6g3",
        "colab_type": "text"
      },
      "source": [
        "# Logistic regression with TF.Keras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TGElhy9UZEYr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "86IHz_epY9ak",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = load_iris()\n",
        "x_train, x_test, y_train, y_test= train_test_split(data.data, data.target, test_size=0.3, random_state=42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xMYkTh9aZJ-5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Flatten(),\n",
        "    #tf.keras.layers.Dense(500, activation=tf.nn.relu),\n",
        "    #tf.keras.layers.Dropout(0.2),\n",
        "    tf.keras.layers.Dense(3, activation=tf.nn.softmax)\n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vNjF5jd4ZWLL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer='sgd',\n",
        "loss='sparse_categorical_crossentropy',\n",
        "metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YoJrTy0UZZfw",
        "colab_type": "code",
        "outputId": "c8c47937-237e-47d7-e966-15649de7d4bd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.fit(x_train, y_train, epochs=800)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 105 samples\n",
            "Epoch 1/800\n",
            "105/105 [==============================] - 0s 2ms/sample - loss: 4.0168 - accuracy: 0.3524\n",
            "Epoch 2/800\n",
            "105/105 [==============================] - 0s 87us/sample - loss: 2.6058 - accuracy: 0.3524\n",
            "Epoch 3/800\n",
            "105/105 [==============================] - 0s 67us/sample - loss: 1.5835 - accuracy: 0.3524\n",
            "Epoch 4/800\n",
            "105/105 [==============================] - 0s 94us/sample - loss: 1.1925 - accuracy: 0.5810\n",
            "Epoch 5/800\n",
            "105/105 [==============================] - 0s 77us/sample - loss: 1.0130 - accuracy: 0.5143\n",
            "Epoch 6/800\n",
            "105/105 [==============================] - 0s 78us/sample - loss: 0.8966 - accuracy: 0.5714\n",
            "Epoch 7/800\n",
            "105/105 [==============================] - 0s 76us/sample - loss: 0.8071 - accuracy: 0.5429\n",
            "Epoch 8/800\n",
            "105/105 [==============================] - 0s 79us/sample - loss: 0.7582 - accuracy: 0.4667\n",
            "Epoch 9/800\n",
            "105/105 [==============================] - 0s 85us/sample - loss: 0.7200 - accuracy: 0.6667\n",
            "Epoch 10/800\n",
            "105/105 [==============================] - 0s 86us/sample - loss: 0.7008 - accuracy: 0.7619\n",
            "Epoch 11/800\n",
            "105/105 [==============================] - 0s 80us/sample - loss: 0.6979 - accuracy: 0.6667\n",
            "Epoch 12/800\n",
            "105/105 [==============================] - 0s 86us/sample - loss: 0.6889 - accuracy: 0.6571\n",
            "Epoch 13/800\n",
            "105/105 [==============================] - 0s 80us/sample - loss: 0.6783 - accuracy: 0.6952\n",
            "Epoch 14/800\n",
            "105/105 [==============================] - 0s 85us/sample - loss: 0.6772 - accuracy: 0.6857\n",
            "Epoch 15/800\n",
            "105/105 [==============================] - 0s 80us/sample - loss: 0.6654 - accuracy: 0.8762\n",
            "Epoch 16/800\n",
            "105/105 [==============================] - 0s 74us/sample - loss: 0.6621 - accuracy: 0.8667\n",
            "Epoch 17/800\n",
            "105/105 [==============================] - 0s 83us/sample - loss: 0.6585 - accuracy: 0.8381\n",
            "Epoch 18/800\n",
            "105/105 [==============================] - 0s 69us/sample - loss: 0.6506 - accuracy: 0.7810\n",
            "Epoch 19/800\n",
            "105/105 [==============================] - 0s 91us/sample - loss: 0.6444 - accuracy: 0.7143\n",
            "Epoch 20/800\n",
            "105/105 [==============================] - 0s 74us/sample - loss: 0.6417 - accuracy: 0.6952\n",
            "Epoch 21/800\n",
            "105/105 [==============================] - 0s 76us/sample - loss: 0.6332 - accuracy: 0.7810\n",
            "Epoch 22/800\n",
            "105/105 [==============================] - 0s 70us/sample - loss: 0.6317 - accuracy: 0.7429\n",
            "Epoch 23/800\n",
            "105/105 [==============================] - 0s 84us/sample - loss: 0.6293 - accuracy: 0.9143\n",
            "Epoch 24/800\n",
            "105/105 [==============================] - 0s 104us/sample - loss: 0.6237 - accuracy: 0.9143\n",
            "Epoch 25/800\n",
            "105/105 [==============================] - 0s 91us/sample - loss: 0.6183 - accuracy: 0.7238\n",
            "Epoch 26/800\n",
            "105/105 [==============================] - 0s 87us/sample - loss: 0.6153 - accuracy: 0.8000\n",
            "Epoch 27/800\n",
            "105/105 [==============================] - 0s 85us/sample - loss: 0.6119 - accuracy: 0.9143\n",
            "Epoch 28/800\n",
            "105/105 [==============================] - 0s 84us/sample - loss: 0.6081 - accuracy: 0.9048\n",
            "Epoch 29/800\n",
            "105/105 [==============================] - 0s 74us/sample - loss: 0.6051 - accuracy: 0.8095\n",
            "Epoch 30/800\n",
            "105/105 [==============================] - 0s 72us/sample - loss: 0.6079 - accuracy: 0.8762\n",
            "Epoch 31/800\n",
            "105/105 [==============================] - 0s 83us/sample - loss: 0.5961 - accuracy: 0.8857\n",
            "Epoch 32/800\n",
            "105/105 [==============================] - 0s 79us/sample - loss: 0.5982 - accuracy: 0.8286\n",
            "Epoch 33/800\n",
            "105/105 [==============================] - 0s 70us/sample - loss: 0.5929 - accuracy: 0.8381\n",
            "Epoch 34/800\n",
            "105/105 [==============================] - 0s 90us/sample - loss: 0.5959 - accuracy: 0.8857\n",
            "Epoch 35/800\n",
            "105/105 [==============================] - 0s 77us/sample - loss: 0.5837 - accuracy: 0.8190\n",
            "Epoch 36/800\n",
            "105/105 [==============================] - 0s 78us/sample - loss: 0.5802 - accuracy: 0.8571\n",
            "Epoch 37/800\n",
            "105/105 [==============================] - 0s 82us/sample - loss: 0.5787 - accuracy: 0.8000\n",
            "Epoch 38/800\n",
            "105/105 [==============================] - 0s 80us/sample - loss: 0.5759 - accuracy: 0.7905\n",
            "Epoch 39/800\n",
            "105/105 [==============================] - 0s 77us/sample - loss: 0.5747 - accuracy: 0.8571\n",
            "Epoch 40/800\n",
            "105/105 [==============================] - 0s 74us/sample - loss: 0.5692 - accuracy: 0.8286\n",
            "Epoch 41/800\n",
            "105/105 [==============================] - 0s 96us/sample - loss: 0.5670 - accuracy: 0.8286\n",
            "Epoch 42/800\n",
            "105/105 [==============================] - 0s 75us/sample - loss: 0.5647 - accuracy: 0.8476\n",
            "Epoch 43/800\n",
            "105/105 [==============================] - 0s 89us/sample - loss: 0.5648 - accuracy: 0.7905\n",
            "Epoch 44/800\n",
            "105/105 [==============================] - 0s 85us/sample - loss: 0.5603 - accuracy: 0.8286\n",
            "Epoch 45/800\n",
            "105/105 [==============================] - 0s 90us/sample - loss: 0.5588 - accuracy: 0.7238\n",
            "Epoch 46/800\n",
            "105/105 [==============================] - 0s 75us/sample - loss: 0.5560 - accuracy: 0.8000\n",
            "Epoch 47/800\n",
            "105/105 [==============================] - 0s 73us/sample - loss: 0.5545 - accuracy: 0.8476\n",
            "Epoch 48/800\n",
            "105/105 [==============================] - 0s 81us/sample - loss: 0.5512 - accuracy: 0.9238\n",
            "Epoch 49/800\n",
            "105/105 [==============================] - 0s 85us/sample - loss: 0.5503 - accuracy: 0.8667\n",
            "Epoch 50/800\n",
            "105/105 [==============================] - 0s 76us/sample - loss: 0.5496 - accuracy: 0.7048\n",
            "Epoch 51/800\n",
            "105/105 [==============================] - 0s 86us/sample - loss: 0.5461 - accuracy: 0.9048\n",
            "Epoch 52/800\n",
            "105/105 [==============================] - 0s 80us/sample - loss: 0.5443 - accuracy: 0.9333\n",
            "Epoch 53/800\n",
            "105/105 [==============================] - 0s 79us/sample - loss: 0.5412 - accuracy: 0.9238\n",
            "Epoch 54/800\n",
            "105/105 [==============================] - 0s 94us/sample - loss: 0.5401 - accuracy: 0.8476\n",
            "Epoch 55/800\n",
            "105/105 [==============================] - 0s 87us/sample - loss: 0.5385 - accuracy: 0.9429\n",
            "Epoch 56/800\n",
            "105/105 [==============================] - 0s 80us/sample - loss: 0.5383 - accuracy: 0.8000\n",
            "Epoch 57/800\n",
            "105/105 [==============================] - 0s 80us/sample - loss: 0.5364 - accuracy: 0.9048\n",
            "Epoch 58/800\n",
            "105/105 [==============================] - 0s 87us/sample - loss: 0.5371 - accuracy: 0.9048\n",
            "Epoch 59/800\n",
            "105/105 [==============================] - 0s 132us/sample - loss: 0.5279 - accuracy: 0.8762\n",
            "Epoch 60/800\n",
            "105/105 [==============================] - 0s 100us/sample - loss: 0.5264 - accuracy: 0.9333\n",
            "Epoch 61/800\n",
            "105/105 [==============================] - 0s 79us/sample - loss: 0.5277 - accuracy: 0.8095\n",
            "Epoch 62/800\n",
            "105/105 [==============================] - 0s 73us/sample - loss: 0.5280 - accuracy: 0.9524\n",
            "Epoch 63/800\n",
            "105/105 [==============================] - 0s 68us/sample - loss: 0.5217 - accuracy: 0.9429\n",
            "Epoch 64/800\n",
            "105/105 [==============================] - 0s 95us/sample - loss: 0.5240 - accuracy: 0.8190\n",
            "Epoch 65/800\n",
            "105/105 [==============================] - 0s 78us/sample - loss: 0.5257 - accuracy: 0.7143\n",
            "Epoch 66/800\n",
            "105/105 [==============================] - 0s 92us/sample - loss: 0.5212 - accuracy: 0.8190\n",
            "Epoch 67/800\n",
            "105/105 [==============================] - 0s 92us/sample - loss: 0.5180 - accuracy: 0.9048\n",
            "Epoch 68/800\n",
            "105/105 [==============================] - 0s 104us/sample - loss: 0.5130 - accuracy: 0.9429\n",
            "Epoch 69/800\n",
            "105/105 [==============================] - 0s 114us/sample - loss: 0.5124 - accuracy: 0.8381\n",
            "Epoch 70/800\n",
            "105/105 [==============================] - 0s 74us/sample - loss: 0.5152 - accuracy: 0.8095\n",
            "Epoch 71/800\n",
            "105/105 [==============================] - 0s 130us/sample - loss: 0.5091 - accuracy: 0.9524\n",
            "Epoch 72/800\n",
            "105/105 [==============================] - 0s 79us/sample - loss: 0.5108 - accuracy: 0.8857\n",
            "Epoch 73/800\n",
            "105/105 [==============================] - 0s 72us/sample - loss: 0.5060 - accuracy: 0.9524\n",
            "Epoch 74/800\n",
            "105/105 [==============================] - 0s 88us/sample - loss: 0.5037 - accuracy: 0.9238\n",
            "Epoch 75/800\n",
            "105/105 [==============================] - 0s 79us/sample - loss: 0.5062 - accuracy: 0.9619\n",
            "Epoch 76/800\n",
            "105/105 [==============================] - 0s 80us/sample - loss: 0.5002 - accuracy: 0.9143\n",
            "Epoch 77/800\n",
            "105/105 [==============================] - 0s 88us/sample - loss: 0.5008 - accuracy: 0.9333\n",
            "Epoch 78/800\n",
            "105/105 [==============================] - 0s 86us/sample - loss: 0.4968 - accuracy: 0.9238\n",
            "Epoch 79/800\n",
            "105/105 [==============================] - 0s 83us/sample - loss: 0.4958 - accuracy: 0.9429\n",
            "Epoch 80/800\n",
            "105/105 [==============================] - 0s 90us/sample - loss: 0.4981 - accuracy: 0.8571\n",
            "Epoch 81/800\n",
            "105/105 [==============================] - 0s 93us/sample - loss: 0.4982 - accuracy: 0.8381\n",
            "Epoch 82/800\n",
            "105/105 [==============================] - 0s 87us/sample - loss: 0.4915 - accuracy: 0.9143\n",
            "Epoch 83/800\n",
            "105/105 [==============================] - 0s 117us/sample - loss: 0.4900 - accuracy: 0.8952\n",
            "Epoch 84/800\n",
            "105/105 [==============================] - 0s 88us/sample - loss: 0.4921 - accuracy: 0.8476\n",
            "Epoch 85/800\n",
            "105/105 [==============================] - 0s 88us/sample - loss: 0.4975 - accuracy: 0.9143\n",
            "Epoch 86/800\n",
            "105/105 [==============================] - 0s 129us/sample - loss: 0.4880 - accuracy: 0.9429\n",
            "Epoch 87/800\n",
            "105/105 [==============================] - 0s 116us/sample - loss: 0.4917 - accuracy: 0.8857\n",
            "Epoch 88/800\n",
            "105/105 [==============================] - 0s 104us/sample - loss: 0.4833 - accuracy: 0.8857\n",
            "Epoch 89/800\n",
            "105/105 [==============================] - 0s 87us/sample - loss: 0.4840 - accuracy: 0.9524\n",
            "Epoch 90/800\n",
            "105/105 [==============================] - 0s 106us/sample - loss: 0.4845 - accuracy: 0.9143\n",
            "Epoch 91/800\n",
            "105/105 [==============================] - 0s 103us/sample - loss: 0.4835 - accuracy: 0.8667\n",
            "Epoch 92/800\n",
            "105/105 [==============================] - 0s 89us/sample - loss: 0.4804 - accuracy: 0.8667\n",
            "Epoch 93/800\n",
            "105/105 [==============================] - 0s 70us/sample - loss: 0.4781 - accuracy: 0.9238\n",
            "Epoch 94/800\n",
            "105/105 [==============================] - 0s 63us/sample - loss: 0.4800 - accuracy: 0.8571\n",
            "Epoch 95/800\n",
            "105/105 [==============================] - 0s 67us/sample - loss: 0.4783 - accuracy: 0.9429\n",
            "Epoch 96/800\n",
            "105/105 [==============================] - 0s 89us/sample - loss: 0.4761 - accuracy: 0.9333\n",
            "Epoch 97/800\n",
            "105/105 [==============================] - 0s 106us/sample - loss: 0.4743 - accuracy: 0.8857\n",
            "Epoch 98/800\n",
            "105/105 [==============================] - 0s 81us/sample - loss: 0.4717 - accuracy: 0.9333\n",
            "Epoch 99/800\n",
            "105/105 [==============================] - 0s 94us/sample - loss: 0.4732 - accuracy: 0.9524\n",
            "Epoch 100/800\n",
            "105/105 [==============================] - 0s 75us/sample - loss: 0.4705 - accuracy: 0.8952\n",
            "Epoch 101/800\n",
            "105/105 [==============================] - 0s 86us/sample - loss: 0.4701 - accuracy: 0.9429\n",
            "Epoch 102/800\n",
            "105/105 [==============================] - 0s 77us/sample - loss: 0.4715 - accuracy: 0.9429\n",
            "Epoch 103/800\n",
            "105/105 [==============================] - 0s 99us/sample - loss: 0.4657 - accuracy: 0.9333\n",
            "Epoch 104/800\n",
            "105/105 [==============================] - 0s 85us/sample - loss: 0.4644 - accuracy: 0.9333\n",
            "Epoch 105/800\n",
            "105/105 [==============================] - 0s 78us/sample - loss: 0.4643 - accuracy: 0.8952\n",
            "Epoch 106/800\n",
            "105/105 [==============================] - 0s 93us/sample - loss: 0.4630 - accuracy: 0.9333\n",
            "Epoch 107/800\n",
            "105/105 [==============================] - 0s 88us/sample - loss: 0.4649 - accuracy: 0.8762\n",
            "Epoch 108/800\n",
            "105/105 [==============================] - 0s 97us/sample - loss: 0.4616 - accuracy: 0.9048\n",
            "Epoch 109/800\n",
            "105/105 [==============================] - 0s 107us/sample - loss: 0.4614 - accuracy: 0.9143\n",
            "Epoch 110/800\n",
            "105/105 [==============================] - 0s 104us/sample - loss: 0.4590 - accuracy: 0.9333\n",
            "Epoch 111/800\n",
            "105/105 [==============================] - 0s 103us/sample - loss: 0.4582 - accuracy: 0.9810\n",
            "Epoch 112/800\n",
            "105/105 [==============================] - 0s 111us/sample - loss: 0.4558 - accuracy: 0.9429\n",
            "Epoch 113/800\n",
            "105/105 [==============================] - 0s 90us/sample - loss: 0.4566 - accuracy: 0.9429\n",
            "Epoch 114/800\n",
            "105/105 [==============================] - 0s 96us/sample - loss: 0.4563 - accuracy: 0.9524\n",
            "Epoch 115/800\n",
            "105/105 [==============================] - 0s 110us/sample - loss: 0.4545 - accuracy: 0.9143\n",
            "Epoch 116/800\n",
            "105/105 [==============================] - 0s 69us/sample - loss: 0.4571 - accuracy: 0.9048\n",
            "Epoch 117/800\n",
            "105/105 [==============================] - 0s 81us/sample - loss: 0.4538 - accuracy: 0.9048\n",
            "Epoch 118/800\n",
            "105/105 [==============================] - 0s 64us/sample - loss: 0.4528 - accuracy: 0.8476\n",
            "Epoch 119/800\n",
            "105/105 [==============================] - 0s 66us/sample - loss: 0.4500 - accuracy: 0.9714\n",
            "Epoch 120/800\n",
            "105/105 [==============================] - 0s 72us/sample - loss: 0.4512 - accuracy: 0.8857\n",
            "Epoch 121/800\n",
            "105/105 [==============================] - 0s 71us/sample - loss: 0.4488 - accuracy: 0.8857\n",
            "Epoch 122/800\n",
            "105/105 [==============================] - 0s 75us/sample - loss: 0.4531 - accuracy: 0.9048\n",
            "Epoch 123/800\n",
            "105/105 [==============================] - 0s 77us/sample - loss: 0.4527 - accuracy: 0.9524\n",
            "Epoch 124/800\n",
            "105/105 [==============================] - 0s 84us/sample - loss: 0.4470 - accuracy: 0.9429\n",
            "Epoch 125/800\n",
            "105/105 [==============================] - 0s 70us/sample - loss: 0.4488 - accuracy: 0.8476\n",
            "Epoch 126/800\n",
            "105/105 [==============================] - 0s 70us/sample - loss: 0.4422 - accuracy: 0.9429\n",
            "Epoch 127/800\n",
            "105/105 [==============================] - 0s 62us/sample - loss: 0.4443 - accuracy: 0.9333\n",
            "Epoch 128/800\n",
            "105/105 [==============================] - 0s 68us/sample - loss: 0.4415 - accuracy: 0.9429\n",
            "Epoch 129/800\n",
            "105/105 [==============================] - 0s 65us/sample - loss: 0.4428 - accuracy: 0.9524\n",
            "Epoch 130/800\n",
            "105/105 [==============================] - 0s 84us/sample - loss: 0.4403 - accuracy: 0.9143\n",
            "Epoch 131/800\n",
            "105/105 [==============================] - 0s 59us/sample - loss: 0.4384 - accuracy: 0.9429\n",
            "Epoch 132/800\n",
            "105/105 [==============================] - 0s 80us/sample - loss: 0.4378 - accuracy: 0.9333\n",
            "Epoch 133/800\n",
            "105/105 [==============================] - 0s 63us/sample - loss: 0.4390 - accuracy: 0.9429\n",
            "Epoch 134/800\n",
            "105/105 [==============================] - 0s 75us/sample - loss: 0.4368 - accuracy: 0.9810\n",
            "Epoch 135/800\n",
            "105/105 [==============================] - 0s 90us/sample - loss: 0.4374 - accuracy: 0.9429\n",
            "Epoch 136/800\n",
            "105/105 [==============================] - 0s 69us/sample - loss: 0.4378 - accuracy: 0.8952\n",
            "Epoch 137/800\n",
            "105/105 [==============================] - 0s 74us/sample - loss: 0.4365 - accuracy: 0.8667\n",
            "Epoch 138/800\n",
            "105/105 [==============================] - 0s 76us/sample - loss: 0.4351 - accuracy: 0.9429\n",
            "Epoch 139/800\n",
            "105/105 [==============================] - 0s 70us/sample - loss: 0.4326 - accuracy: 0.9714\n",
            "Epoch 140/800\n",
            "105/105 [==============================] - 0s 110us/sample - loss: 0.4305 - accuracy: 0.9619\n",
            "Epoch 141/800\n",
            "105/105 [==============================] - 0s 82us/sample - loss: 0.4304 - accuracy: 0.9524\n",
            "Epoch 142/800\n",
            "105/105 [==============================] - 0s 69us/sample - loss: 0.4314 - accuracy: 0.9810\n",
            "Epoch 143/800\n",
            "105/105 [==============================] - 0s 80us/sample - loss: 0.4308 - accuracy: 0.9524\n",
            "Epoch 144/800\n",
            "105/105 [==============================] - 0s 92us/sample - loss: 0.4272 - accuracy: 0.9429\n",
            "Epoch 145/800\n",
            "105/105 [==============================] - 0s 108us/sample - loss: 0.4268 - accuracy: 0.9524\n",
            "Epoch 146/800\n",
            "105/105 [==============================] - 0s 72us/sample - loss: 0.4271 - accuracy: 0.9619\n",
            "Epoch 147/800\n",
            "105/105 [==============================] - 0s 61us/sample - loss: 0.4296 - accuracy: 0.9619\n",
            "Epoch 148/800\n",
            "105/105 [==============================] - 0s 55us/sample - loss: 0.4248 - accuracy: 0.9429\n",
            "Epoch 149/800\n",
            "105/105 [==============================] - 0s 68us/sample - loss: 0.4239 - accuracy: 0.9714\n",
            "Epoch 150/800\n",
            "105/105 [==============================] - 0s 67us/sample - loss: 0.4226 - accuracy: 0.9333\n",
            "Epoch 151/800\n",
            "105/105 [==============================] - 0s 67us/sample - loss: 0.4217 - accuracy: 0.9619\n",
            "Epoch 152/800\n",
            "105/105 [==============================] - 0s 68us/sample - loss: 0.4299 - accuracy: 0.9333\n",
            "Epoch 153/800\n",
            "105/105 [==============================] - 0s 66us/sample - loss: 0.4211 - accuracy: 0.9524\n",
            "Epoch 154/800\n",
            "105/105 [==============================] - 0s 90us/sample - loss: 0.4280 - accuracy: 0.9429\n",
            "Epoch 155/800\n",
            "105/105 [==============================] - 0s 97us/sample - loss: 0.4182 - accuracy: 0.9714\n",
            "Epoch 156/800\n",
            "105/105 [==============================] - 0s 97us/sample - loss: 0.4194 - accuracy: 0.9429\n",
            "Epoch 157/800\n",
            "105/105 [==============================] - 0s 103us/sample - loss: 0.4218 - accuracy: 0.8952\n",
            "Epoch 158/800\n",
            "105/105 [==============================] - 0s 100us/sample - loss: 0.4172 - accuracy: 0.9333\n",
            "Epoch 159/800\n",
            "105/105 [==============================] - 0s 95us/sample - loss: 0.4169 - accuracy: 0.8857\n",
            "Epoch 160/800\n",
            "105/105 [==============================] - 0s 113us/sample - loss: 0.4180 - accuracy: 0.9143\n",
            "Epoch 161/800\n",
            "105/105 [==============================] - 0s 89us/sample - loss: 0.4156 - accuracy: 0.9810\n",
            "Epoch 162/800\n",
            "105/105 [==============================] - 0s 94us/sample - loss: 0.4164 - accuracy: 0.9810\n",
            "Epoch 163/800\n",
            "105/105 [==============================] - 0s 84us/sample - loss: 0.4161 - accuracy: 0.9524\n",
            "Epoch 164/800\n",
            "105/105 [==============================] - 0s 86us/sample - loss: 0.4113 - accuracy: 0.9429\n",
            "Epoch 165/800\n",
            "105/105 [==============================] - 0s 86us/sample - loss: 0.4125 - accuracy: 0.9714\n",
            "Epoch 166/800\n",
            "105/105 [==============================] - 0s 91us/sample - loss: 0.4129 - accuracy: 0.9714\n",
            "Epoch 167/800\n",
            "105/105 [==============================] - 0s 100us/sample - loss: 0.4100 - accuracy: 0.9714\n",
            "Epoch 168/800\n",
            "105/105 [==============================] - 0s 86us/sample - loss: 0.4117 - accuracy: 0.9333\n",
            "Epoch 169/800\n",
            "105/105 [==============================] - 0s 94us/sample - loss: 0.4134 - accuracy: 0.8857\n",
            "Epoch 170/800\n",
            "105/105 [==============================] - 0s 94us/sample - loss: 0.4092 - accuracy: 0.9238\n",
            "Epoch 171/800\n",
            "105/105 [==============================] - 0s 92us/sample - loss: 0.4068 - accuracy: 0.9429\n",
            "Epoch 172/800\n",
            "105/105 [==============================] - 0s 91us/sample - loss: 0.4055 - accuracy: 0.9619\n",
            "Epoch 173/800\n",
            "105/105 [==============================] - 0s 91us/sample - loss: 0.4073 - accuracy: 0.9714\n",
            "Epoch 174/800\n",
            "105/105 [==============================] - 0s 91us/sample - loss: 0.4053 - accuracy: 0.9619\n",
            "Epoch 175/800\n",
            "105/105 [==============================] - 0s 90us/sample - loss: 0.4084 - accuracy: 0.9524\n",
            "Epoch 176/800\n",
            "105/105 [==============================] - 0s 82us/sample - loss: 0.4057 - accuracy: 0.9810\n",
            "Epoch 177/800\n",
            "105/105 [==============================] - 0s 79us/sample - loss: 0.4021 - accuracy: 0.9619\n",
            "Epoch 178/800\n",
            "105/105 [==============================] - 0s 84us/sample - loss: 0.4081 - accuracy: 0.9429\n",
            "Epoch 179/800\n",
            "105/105 [==============================] - 0s 87us/sample - loss: 0.4030 - accuracy: 0.9619\n",
            "Epoch 180/800\n",
            "105/105 [==============================] - 0s 71us/sample - loss: 0.4062 - accuracy: 0.9524\n",
            "Epoch 181/800\n",
            "105/105 [==============================] - 0s 73us/sample - loss: 0.4007 - accuracy: 0.9238\n",
            "Epoch 182/800\n",
            "105/105 [==============================] - 0s 81us/sample - loss: 0.4005 - accuracy: 0.9333\n",
            "Epoch 183/800\n",
            "105/105 [==============================] - 0s 90us/sample - loss: 0.3992 - accuracy: 0.9714\n",
            "Epoch 184/800\n",
            "105/105 [==============================] - 0s 88us/sample - loss: 0.3988 - accuracy: 0.9333\n",
            "Epoch 185/800\n",
            "105/105 [==============================] - 0s 90us/sample - loss: 0.3973 - accuracy: 0.9429\n",
            "Epoch 186/800\n",
            "105/105 [==============================] - 0s 88us/sample - loss: 0.3959 - accuracy: 0.9714\n",
            "Epoch 187/800\n",
            "105/105 [==============================] - 0s 86us/sample - loss: 0.3984 - accuracy: 0.9429\n",
            "Epoch 188/800\n",
            "105/105 [==============================] - 0s 96us/sample - loss: 0.3975 - accuracy: 0.9714\n",
            "Epoch 189/800\n",
            "105/105 [==============================] - 0s 77us/sample - loss: 0.3965 - accuracy: 0.9524\n",
            "Epoch 190/800\n",
            "105/105 [==============================] - 0s 95us/sample - loss: 0.3940 - accuracy: 0.9524\n",
            "Epoch 191/800\n",
            "105/105 [==============================] - 0s 85us/sample - loss: 0.3962 - accuracy: 0.9524\n",
            "Epoch 192/800\n",
            "105/105 [==============================] - 0s 97us/sample - loss: 0.3958 - accuracy: 0.9619\n",
            "Epoch 193/800\n",
            "105/105 [==============================] - 0s 85us/sample - loss: 0.3924 - accuracy: 0.9619\n",
            "Epoch 194/800\n",
            "105/105 [==============================] - 0s 76us/sample - loss: 0.3919 - accuracy: 0.9714\n",
            "Epoch 195/800\n",
            "105/105 [==============================] - 0s 76us/sample - loss: 0.3916 - accuracy: 0.9714\n",
            "Epoch 196/800\n",
            "105/105 [==============================] - 0s 79us/sample - loss: 0.3913 - accuracy: 0.9238\n",
            "Epoch 197/800\n",
            "105/105 [==============================] - 0s 104us/sample - loss: 0.3908 - accuracy: 0.9619\n",
            "Epoch 198/800\n",
            "105/105 [==============================] - 0s 75us/sample - loss: 0.3886 - accuracy: 0.9714\n",
            "Epoch 199/800\n",
            "105/105 [==============================] - 0s 81us/sample - loss: 0.3895 - accuracy: 0.9333\n",
            "Epoch 200/800\n",
            "105/105 [==============================] - 0s 91us/sample - loss: 0.3897 - accuracy: 0.9333\n",
            "Epoch 201/800\n",
            "105/105 [==============================] - 0s 86us/sample - loss: 0.3921 - accuracy: 0.9524\n",
            "Epoch 202/800\n",
            "105/105 [==============================] - 0s 86us/sample - loss: 0.3872 - accuracy: 0.9619\n",
            "Epoch 203/800\n",
            "105/105 [==============================] - 0s 82us/sample - loss: 0.3865 - accuracy: 0.9619\n",
            "Epoch 204/800\n",
            "105/105 [==============================] - 0s 84us/sample - loss: 0.3908 - accuracy: 0.9524\n",
            "Epoch 205/800\n",
            "105/105 [==============================] - 0s 88us/sample - loss: 0.3850 - accuracy: 0.9619\n",
            "Epoch 206/800\n",
            "105/105 [==============================] - 0s 91us/sample - loss: 0.3852 - accuracy: 0.9714\n",
            "Epoch 207/800\n",
            "105/105 [==============================] - 0s 94us/sample - loss: 0.3831 - accuracy: 0.9714\n",
            "Epoch 208/800\n",
            "105/105 [==============================] - 0s 101us/sample - loss: 0.3826 - accuracy: 0.9429\n",
            "Epoch 209/800\n",
            "105/105 [==============================] - 0s 83us/sample - loss: 0.3840 - accuracy: 0.9714\n",
            "Epoch 210/800\n",
            "105/105 [==============================] - 0s 120us/sample - loss: 0.3818 - accuracy: 0.9714\n",
            "Epoch 211/800\n",
            "105/105 [==============================] - 0s 104us/sample - loss: 0.3807 - accuracy: 0.9714\n",
            "Epoch 212/800\n",
            "105/105 [==============================] - 0s 77us/sample - loss: 0.3809 - accuracy: 0.9429\n",
            "Epoch 213/800\n",
            "105/105 [==============================] - 0s 78us/sample - loss: 0.3790 - accuracy: 0.9619\n",
            "Epoch 214/800\n",
            "105/105 [==============================] - 0s 117us/sample - loss: 0.3801 - accuracy: 0.9619\n",
            "Epoch 215/800\n",
            "105/105 [==============================] - 0s 84us/sample - loss: 0.3796 - accuracy: 0.9333\n",
            "Epoch 216/800\n",
            "105/105 [==============================] - 0s 87us/sample - loss: 0.3785 - accuracy: 0.9714\n",
            "Epoch 217/800\n",
            "105/105 [==============================] - 0s 82us/sample - loss: 0.3771 - accuracy: 0.9619\n",
            "Epoch 218/800\n",
            "105/105 [==============================] - 0s 83us/sample - loss: 0.3781 - accuracy: 0.9714\n",
            "Epoch 219/800\n",
            "105/105 [==============================] - 0s 85us/sample - loss: 0.3766 - accuracy: 0.9619\n",
            "Epoch 220/800\n",
            "105/105 [==============================] - 0s 96us/sample - loss: 0.3778 - accuracy: 0.9429\n",
            "Epoch 221/800\n",
            "105/105 [==============================] - 0s 88us/sample - loss: 0.3757 - accuracy: 0.9333\n",
            "Epoch 222/800\n",
            "105/105 [==============================] - 0s 88us/sample - loss: 0.3766 - accuracy: 0.9619\n",
            "Epoch 223/800\n",
            "105/105 [==============================] - 0s 80us/sample - loss: 0.3775 - accuracy: 0.9810\n",
            "Epoch 224/800\n",
            "105/105 [==============================] - 0s 89us/sample - loss: 0.3738 - accuracy: 0.9714\n",
            "Epoch 225/800\n",
            "105/105 [==============================] - 0s 88us/sample - loss: 0.3734 - accuracy: 0.9714\n",
            "Epoch 226/800\n",
            "105/105 [==============================] - 0s 109us/sample - loss: 0.3753 - accuracy: 0.9429\n",
            "Epoch 227/800\n",
            "105/105 [==============================] - 0s 96us/sample - loss: 0.3719 - accuracy: 0.9524\n",
            "Epoch 228/800\n",
            "105/105 [==============================] - 0s 100us/sample - loss: 0.3721 - accuracy: 0.9619\n",
            "Epoch 229/800\n",
            "105/105 [==============================] - 0s 99us/sample - loss: 0.3720 - accuracy: 0.9619\n",
            "Epoch 230/800\n",
            "105/105 [==============================] - 0s 101us/sample - loss: 0.3702 - accuracy: 0.9619\n",
            "Epoch 231/800\n",
            "105/105 [==============================] - 0s 98us/sample - loss: 0.3703 - accuracy: 0.9714\n",
            "Epoch 232/800\n",
            "105/105 [==============================] - 0s 85us/sample - loss: 0.3730 - accuracy: 0.9619\n",
            "Epoch 233/800\n",
            "105/105 [==============================] - 0s 60us/sample - loss: 0.3688 - accuracy: 0.9524\n",
            "Epoch 234/800\n",
            "105/105 [==============================] - 0s 76us/sample - loss: 0.3697 - accuracy: 0.9619\n",
            "Epoch 235/800\n",
            "105/105 [==============================] - 0s 81us/sample - loss: 0.3677 - accuracy: 0.9524\n",
            "Epoch 236/800\n",
            "105/105 [==============================] - 0s 83us/sample - loss: 0.3709 - accuracy: 0.9619\n",
            "Epoch 237/800\n",
            "105/105 [==============================] - 0s 84us/sample - loss: 0.3681 - accuracy: 0.9524\n",
            "Epoch 238/800\n",
            "105/105 [==============================] - 0s 89us/sample - loss: 0.3658 - accuracy: 0.9714\n",
            "Epoch 239/800\n",
            "105/105 [==============================] - 0s 91us/sample - loss: 0.3647 - accuracy: 0.9619\n",
            "Epoch 240/800\n",
            "105/105 [==============================] - 0s 82us/sample - loss: 0.3642 - accuracy: 0.9714\n",
            "Epoch 241/800\n",
            "105/105 [==============================] - 0s 125us/sample - loss: 0.3641 - accuracy: 0.9714\n",
            "Epoch 242/800\n",
            "105/105 [==============================] - 0s 84us/sample - loss: 0.3670 - accuracy: 0.9619\n",
            "Epoch 243/800\n",
            "105/105 [==============================] - 0s 152us/sample - loss: 0.3662 - accuracy: 0.9714\n",
            "Epoch 244/800\n",
            "105/105 [==============================] - 0s 95us/sample - loss: 0.3628 - accuracy: 0.9714\n",
            "Epoch 245/800\n",
            "105/105 [==============================] - 0s 101us/sample - loss: 0.3631 - accuracy: 0.9714\n",
            "Epoch 246/800\n",
            "105/105 [==============================] - 0s 105us/sample - loss: 0.3621 - accuracy: 0.9714\n",
            "Epoch 247/800\n",
            "105/105 [==============================] - 0s 119us/sample - loss: 0.3617 - accuracy: 0.9619\n",
            "Epoch 248/800\n",
            "105/105 [==============================] - 0s 107us/sample - loss: 0.3627 - accuracy: 0.9429\n",
            "Epoch 249/800\n",
            "105/105 [==============================] - 0s 90us/sample - loss: 0.3603 - accuracy: 0.9524\n",
            "Epoch 250/800\n",
            "105/105 [==============================] - 0s 84us/sample - loss: 0.3629 - accuracy: 0.9714\n",
            "Epoch 251/800\n",
            "105/105 [==============================] - 0s 85us/sample - loss: 0.3595 - accuracy: 0.9619\n",
            "Epoch 252/800\n",
            "105/105 [==============================] - 0s 108us/sample - loss: 0.3621 - accuracy: 0.9714\n",
            "Epoch 253/800\n",
            "105/105 [==============================] - 0s 84us/sample - loss: 0.3605 - accuracy: 0.9714\n",
            "Epoch 254/800\n",
            "105/105 [==============================] - 0s 84us/sample - loss: 0.3576 - accuracy: 0.9619\n",
            "Epoch 255/800\n",
            "105/105 [==============================] - 0s 89us/sample - loss: 0.3576 - accuracy: 0.9714\n",
            "Epoch 256/800\n",
            "105/105 [==============================] - 0s 91us/sample - loss: 0.3598 - accuracy: 0.9619\n",
            "Epoch 257/800\n",
            "105/105 [==============================] - 0s 113us/sample - loss: 0.3579 - accuracy: 0.9714\n",
            "Epoch 258/800\n",
            "105/105 [==============================] - 0s 69us/sample - loss: 0.3575 - accuracy: 0.9619\n",
            "Epoch 259/800\n",
            "105/105 [==============================] - 0s 72us/sample - loss: 0.3562 - accuracy: 0.9714\n",
            "Epoch 260/800\n",
            "105/105 [==============================] - 0s 69us/sample - loss: 0.3573 - accuracy: 0.9524\n",
            "Epoch 261/800\n",
            "105/105 [==============================] - 0s 70us/sample - loss: 0.3552 - accuracy: 0.9714\n",
            "Epoch 262/800\n",
            "105/105 [==============================] - 0s 69us/sample - loss: 0.3549 - accuracy: 0.9619\n",
            "Epoch 263/800\n",
            "105/105 [==============================] - 0s 80us/sample - loss: 0.3531 - accuracy: 0.9714\n",
            "Epoch 264/800\n",
            "105/105 [==============================] - 0s 122us/sample - loss: 0.3536 - accuracy: 0.9714\n",
            "Epoch 265/800\n",
            "105/105 [==============================] - 0s 111us/sample - loss: 0.3545 - accuracy: 0.9524\n",
            "Epoch 266/800\n",
            "105/105 [==============================] - 0s 76us/sample - loss: 0.3534 - accuracy: 0.9619\n",
            "Epoch 267/800\n",
            "105/105 [==============================] - 0s 92us/sample - loss: 0.3528 - accuracy: 0.9524\n",
            "Epoch 268/800\n",
            "105/105 [==============================] - 0s 82us/sample - loss: 0.3531 - accuracy: 0.9524\n",
            "Epoch 269/800\n",
            "105/105 [==============================] - 0s 91us/sample - loss: 0.3510 - accuracy: 0.9524\n",
            "Epoch 270/800\n",
            "105/105 [==============================] - 0s 94us/sample - loss: 0.3509 - accuracy: 0.9619\n",
            "Epoch 271/800\n",
            "105/105 [==============================] - 0s 75us/sample - loss: 0.3502 - accuracy: 0.9714\n",
            "Epoch 272/800\n",
            "105/105 [==============================] - 0s 84us/sample - loss: 0.3508 - accuracy: 0.9619\n",
            "Epoch 273/800\n",
            "105/105 [==============================] - 0s 82us/sample - loss: 0.3496 - accuracy: 0.9619\n",
            "Epoch 274/800\n",
            "105/105 [==============================] - 0s 70us/sample - loss: 0.3519 - accuracy: 0.9714\n",
            "Epoch 275/800\n",
            "105/105 [==============================] - 0s 78us/sample - loss: 0.3502 - accuracy: 0.9714\n",
            "Epoch 276/800\n",
            "105/105 [==============================] - 0s 85us/sample - loss: 0.3471 - accuracy: 0.9714\n",
            "Epoch 277/800\n",
            "105/105 [==============================] - 0s 83us/sample - loss: 0.3465 - accuracy: 0.9619\n",
            "Epoch 278/800\n",
            "105/105 [==============================] - 0s 114us/sample - loss: 0.3464 - accuracy: 0.9714\n",
            "Epoch 279/800\n",
            "105/105 [==============================] - 0s 85us/sample - loss: 0.3461 - accuracy: 0.9714\n",
            "Epoch 280/800\n",
            "105/105 [==============================] - 0s 92us/sample - loss: 0.3458 - accuracy: 0.9619\n",
            "Epoch 281/800\n",
            "105/105 [==============================] - 0s 84us/sample - loss: 0.3487 - accuracy: 0.9524\n",
            "Epoch 282/800\n",
            "105/105 [==============================] - 0s 85us/sample - loss: 0.3482 - accuracy: 0.9714\n",
            "Epoch 283/800\n",
            "105/105 [==============================] - 0s 96us/sample - loss: 0.3475 - accuracy: 0.9619\n",
            "Epoch 284/800\n",
            "105/105 [==============================] - 0s 84us/sample - loss: 0.3434 - accuracy: 0.9714\n",
            "Epoch 285/800\n",
            "105/105 [==============================] - 0s 77us/sample - loss: 0.3448 - accuracy: 0.9619\n",
            "Epoch 286/800\n",
            "105/105 [==============================] - 0s 79us/sample - loss: 0.3433 - accuracy: 0.9714\n",
            "Epoch 287/800\n",
            "105/105 [==============================] - 0s 92us/sample - loss: 0.3423 - accuracy: 0.9524\n",
            "Epoch 288/800\n",
            "105/105 [==============================] - 0s 80us/sample - loss: 0.3418 - accuracy: 0.9714\n",
            "Epoch 289/800\n",
            "105/105 [==============================] - 0s 100us/sample - loss: 0.3414 - accuracy: 0.9619\n",
            "Epoch 290/800\n",
            "105/105 [==============================] - 0s 91us/sample - loss: 0.3431 - accuracy: 0.9714\n",
            "Epoch 291/800\n",
            "105/105 [==============================] - 0s 80us/sample - loss: 0.3440 - accuracy: 0.9619\n",
            "Epoch 292/800\n",
            "105/105 [==============================] - 0s 98us/sample - loss: 0.3404 - accuracy: 0.9619\n",
            "Epoch 293/800\n",
            "105/105 [==============================] - 0s 77us/sample - loss: 0.3404 - accuracy: 0.9524\n",
            "Epoch 294/800\n",
            "105/105 [==============================] - 0s 85us/sample - loss: 0.3402 - accuracy: 0.9714\n",
            "Epoch 295/800\n",
            "105/105 [==============================] - 0s 87us/sample - loss: 0.3392 - accuracy: 0.9714\n",
            "Epoch 296/800\n",
            "105/105 [==============================] - 0s 89us/sample - loss: 0.3418 - accuracy: 0.9524\n",
            "Epoch 297/800\n",
            "105/105 [==============================] - 0s 85us/sample - loss: 0.3390 - accuracy: 0.9524\n",
            "Epoch 298/800\n",
            "105/105 [==============================] - 0s 79us/sample - loss: 0.3371 - accuracy: 0.9714\n",
            "Epoch 299/800\n",
            "105/105 [==============================] - 0s 80us/sample - loss: 0.3370 - accuracy: 0.9714\n",
            "Epoch 300/800\n",
            "105/105 [==============================] - 0s 88us/sample - loss: 0.3377 - accuracy: 0.9714\n",
            "Epoch 301/800\n",
            "105/105 [==============================] - 0s 83us/sample - loss: 0.3389 - accuracy: 0.9619\n",
            "Epoch 302/800\n",
            "105/105 [==============================] - 0s 75us/sample - loss: 0.3365 - accuracy: 0.9619\n",
            "Epoch 303/800\n",
            "105/105 [==============================] - 0s 97us/sample - loss: 0.3357 - accuracy: 0.9714\n",
            "Epoch 304/800\n",
            "105/105 [==============================] - 0s 90us/sample - loss: 0.3351 - accuracy: 0.9714\n",
            "Epoch 305/800\n",
            "105/105 [==============================] - 0s 93us/sample - loss: 0.3373 - accuracy: 0.9619\n",
            "Epoch 306/800\n",
            "105/105 [==============================] - 0s 71us/sample - loss: 0.3341 - accuracy: 0.9524\n",
            "Epoch 307/800\n",
            "105/105 [==============================] - 0s 66us/sample - loss: 0.3344 - accuracy: 0.9619\n",
            "Epoch 308/800\n",
            "105/105 [==============================] - 0s 90us/sample - loss: 0.3360 - accuracy: 0.9619\n",
            "Epoch 309/800\n",
            "105/105 [==============================] - 0s 90us/sample - loss: 0.3331 - accuracy: 0.9524\n",
            "Epoch 310/800\n",
            "105/105 [==============================] - 0s 95us/sample - loss: 0.3328 - accuracy: 0.9714\n",
            "Epoch 311/800\n",
            "105/105 [==============================] - 0s 84us/sample - loss: 0.3324 - accuracy: 0.9714\n",
            "Epoch 312/800\n",
            "105/105 [==============================] - 0s 86us/sample - loss: 0.3326 - accuracy: 0.9619\n",
            "Epoch 313/800\n",
            "105/105 [==============================] - 0s 78us/sample - loss: 0.3352 - accuracy: 0.9619\n",
            "Epoch 314/800\n",
            "105/105 [==============================] - 0s 93us/sample - loss: 0.3321 - accuracy: 0.9714\n",
            "Epoch 315/800\n",
            "105/105 [==============================] - 0s 87us/sample - loss: 0.3327 - accuracy: 0.9714\n",
            "Epoch 316/800\n",
            "105/105 [==============================] - 0s 84us/sample - loss: 0.3315 - accuracy: 0.9714\n",
            "Epoch 317/800\n",
            "105/105 [==============================] - 0s 87us/sample - loss: 0.3316 - accuracy: 0.9714\n",
            "Epoch 318/800\n",
            "105/105 [==============================] - 0s 80us/sample - loss: 0.3349 - accuracy: 0.9619\n",
            "Epoch 319/800\n",
            "105/105 [==============================] - 0s 75us/sample - loss: 0.3287 - accuracy: 0.9714\n",
            "Epoch 320/800\n",
            "105/105 [==============================] - 0s 85us/sample - loss: 0.3285 - accuracy: 0.9714\n",
            "Epoch 321/800\n",
            "105/105 [==============================] - 0s 91us/sample - loss: 0.3298 - accuracy: 0.9714\n",
            "Epoch 322/800\n",
            "105/105 [==============================] - 0s 83us/sample - loss: 0.3306 - accuracy: 0.9524\n",
            "Epoch 323/800\n",
            "105/105 [==============================] - 0s 85us/sample - loss: 0.3294 - accuracy: 0.9619\n",
            "Epoch 324/800\n",
            "105/105 [==============================] - 0s 87us/sample - loss: 0.3287 - accuracy: 0.9524\n",
            "Epoch 325/800\n",
            "105/105 [==============================] - 0s 79us/sample - loss: 0.3273 - accuracy: 0.9524\n",
            "Epoch 326/800\n",
            "105/105 [==============================] - 0s 84us/sample - loss: 0.3282 - accuracy: 0.9714\n",
            "Epoch 327/800\n",
            "105/105 [==============================] - 0s 93us/sample - loss: 0.3322 - accuracy: 0.9333\n",
            "Epoch 328/800\n",
            "105/105 [==============================] - 0s 81us/sample - loss: 0.3261 - accuracy: 0.9714\n",
            "Epoch 329/800\n",
            "105/105 [==============================] - 0s 90us/sample - loss: 0.3268 - accuracy: 0.9619\n",
            "Epoch 330/800\n",
            "105/105 [==============================] - 0s 86us/sample - loss: 0.3251 - accuracy: 0.9714\n",
            "Epoch 331/800\n",
            "105/105 [==============================] - 0s 77us/sample - loss: 0.3281 - accuracy: 0.9714\n",
            "Epoch 332/800\n",
            "105/105 [==============================] - 0s 85us/sample - loss: 0.3260 - accuracy: 0.9524\n",
            "Epoch 333/800\n",
            "105/105 [==============================] - 0s 84us/sample - loss: 0.3276 - accuracy: 0.9714\n",
            "Epoch 334/800\n",
            "105/105 [==============================] - 0s 89us/sample - loss: 0.3272 - accuracy: 0.9524\n",
            "Epoch 335/800\n",
            "105/105 [==============================] - 0s 102us/sample - loss: 0.3273 - accuracy: 0.9714\n",
            "Epoch 336/800\n",
            "105/105 [==============================] - 0s 82us/sample - loss: 0.3285 - accuracy: 0.9524\n",
            "Epoch 337/800\n",
            "105/105 [==============================] - 0s 78us/sample - loss: 0.3262 - accuracy: 0.9714\n",
            "Epoch 338/800\n",
            "105/105 [==============================] - 0s 73us/sample - loss: 0.3262 - accuracy: 0.9619\n",
            "Epoch 339/800\n",
            "105/105 [==============================] - 0s 69us/sample - loss: 0.3233 - accuracy: 0.9524\n",
            "Epoch 340/800\n",
            "105/105 [==============================] - 0s 84us/sample - loss: 0.3215 - accuracy: 0.9714\n",
            "Epoch 341/800\n",
            "105/105 [==============================] - 0s 84us/sample - loss: 0.3215 - accuracy: 0.9524\n",
            "Epoch 342/800\n",
            "105/105 [==============================] - 0s 92us/sample - loss: 0.3214 - accuracy: 0.9619\n",
            "Epoch 343/800\n",
            "105/105 [==============================] - 0s 104us/sample - loss: 0.3212 - accuracy: 0.9524\n",
            "Epoch 344/800\n",
            "105/105 [==============================] - 0s 88us/sample - loss: 0.3203 - accuracy: 0.9619\n",
            "Epoch 345/800\n",
            "105/105 [==============================] - 0s 86us/sample - loss: 0.3199 - accuracy: 0.9714\n",
            "Epoch 346/800\n",
            "105/105 [==============================] - 0s 77us/sample - loss: 0.3204 - accuracy: 0.9524\n",
            "Epoch 347/800\n",
            "105/105 [==============================] - 0s 78us/sample - loss: 0.3185 - accuracy: 0.9714\n",
            "Epoch 348/800\n",
            "105/105 [==============================] - 0s 74us/sample - loss: 0.3181 - accuracy: 0.9714\n",
            "Epoch 349/800\n",
            "105/105 [==============================] - 0s 102us/sample - loss: 0.3183 - accuracy: 0.9619\n",
            "Epoch 350/800\n",
            "105/105 [==============================] - 0s 103us/sample - loss: 0.3209 - accuracy: 0.9619\n",
            "Epoch 351/800\n",
            "105/105 [==============================] - 0s 70us/sample - loss: 0.3188 - accuracy: 0.9619\n",
            "Epoch 352/800\n",
            "105/105 [==============================] - 0s 104us/sample - loss: 0.3169 - accuracy: 0.9619\n",
            "Epoch 353/800\n",
            "105/105 [==============================] - 0s 82us/sample - loss: 0.3163 - accuracy: 0.9619\n",
            "Epoch 354/800\n",
            "105/105 [==============================] - 0s 85us/sample - loss: 0.3156 - accuracy: 0.9714\n",
            "Epoch 355/800\n",
            "105/105 [==============================] - 0s 76us/sample - loss: 0.3150 - accuracy: 0.9714\n",
            "Epoch 356/800\n",
            "105/105 [==============================] - 0s 77us/sample - loss: 0.3166 - accuracy: 0.9714\n",
            "Epoch 357/800\n",
            "105/105 [==============================] - 0s 74us/sample - loss: 0.3148 - accuracy: 0.9714\n",
            "Epoch 358/800\n",
            "105/105 [==============================] - 0s 78us/sample - loss: 0.3157 - accuracy: 0.9619\n",
            "Epoch 359/800\n",
            "105/105 [==============================] - 0s 74us/sample - loss: 0.3147 - accuracy: 0.9524\n",
            "Epoch 360/800\n",
            "105/105 [==============================] - 0s 72us/sample - loss: 0.3139 - accuracy: 0.9714\n",
            "Epoch 361/800\n",
            "105/105 [==============================] - 0s 80us/sample - loss: 0.3139 - accuracy: 0.9714\n",
            "Epoch 362/800\n",
            "105/105 [==============================] - 0s 74us/sample - loss: 0.3133 - accuracy: 0.9714\n",
            "Epoch 363/800\n",
            "105/105 [==============================] - 0s 84us/sample - loss: 0.3142 - accuracy: 0.9619\n",
            "Epoch 364/800\n",
            "105/105 [==============================] - 0s 114us/sample - loss: 0.3129 - accuracy: 0.9619\n",
            "Epoch 365/800\n",
            "105/105 [==============================] - 0s 108us/sample - loss: 0.3129 - accuracy: 0.9524\n",
            "Epoch 366/800\n",
            "105/105 [==============================] - 0s 78us/sample - loss: 0.3127 - accuracy: 0.9714\n",
            "Epoch 367/800\n",
            "105/105 [==============================] - 0s 76us/sample - loss: 0.3110 - accuracy: 0.9714\n",
            "Epoch 368/800\n",
            "105/105 [==============================] - 0s 74us/sample - loss: 0.3118 - accuracy: 0.9619\n",
            "Epoch 369/800\n",
            "105/105 [==============================] - 0s 91us/sample - loss: 0.3130 - accuracy: 0.9619\n",
            "Epoch 370/800\n",
            "105/105 [==============================] - 0s 86us/sample - loss: 0.3151 - accuracy: 0.9429\n",
            "Epoch 371/800\n",
            "105/105 [==============================] - 0s 89us/sample - loss: 0.3109 - accuracy: 0.9619\n",
            "Epoch 372/800\n",
            "105/105 [==============================] - 0s 82us/sample - loss: 0.3097 - accuracy: 0.9619\n",
            "Epoch 373/800\n",
            "105/105 [==============================] - 0s 72us/sample - loss: 0.3119 - accuracy: 0.9619\n",
            "Epoch 374/800\n",
            "105/105 [==============================] - 0s 84us/sample - loss: 0.3101 - accuracy: 0.9619\n",
            "Epoch 375/800\n",
            "105/105 [==============================] - 0s 87us/sample - loss: 0.3099 - accuracy: 0.9619\n",
            "Epoch 376/800\n",
            "105/105 [==============================] - 0s 80us/sample - loss: 0.3099 - accuracy: 0.9714\n",
            "Epoch 377/800\n",
            "105/105 [==============================] - 0s 78us/sample - loss: 0.3087 - accuracy: 0.9714\n",
            "Epoch 378/800\n",
            "105/105 [==============================] - 0s 73us/sample - loss: 0.3113 - accuracy: 0.9524\n",
            "Epoch 379/800\n",
            "105/105 [==============================] - 0s 70us/sample - loss: 0.3086 - accuracy: 0.9619\n",
            "Epoch 380/800\n",
            "105/105 [==============================] - 0s 81us/sample - loss: 0.3081 - accuracy: 0.9619\n",
            "Epoch 381/800\n",
            "105/105 [==============================] - 0s 79us/sample - loss: 0.3094 - accuracy: 0.9524\n",
            "Epoch 382/800\n",
            "105/105 [==============================] - 0s 80us/sample - loss: 0.3100 - accuracy: 0.9619\n",
            "Epoch 383/800\n",
            "105/105 [==============================] - 0s 83us/sample - loss: 0.3064 - accuracy: 0.9714\n",
            "Epoch 384/800\n",
            "105/105 [==============================] - 0s 87us/sample - loss: 0.3074 - accuracy: 0.9524\n",
            "Epoch 385/800\n",
            "105/105 [==============================] - 0s 78us/sample - loss: 0.3054 - accuracy: 0.9714\n",
            "Epoch 386/800\n",
            "105/105 [==============================] - 0s 83us/sample - loss: 0.3058 - accuracy: 0.9714\n",
            "Epoch 387/800\n",
            "105/105 [==============================] - 0s 81us/sample - loss: 0.3090 - accuracy: 0.9524\n",
            "Epoch 388/800\n",
            "105/105 [==============================] - 0s 86us/sample - loss: 0.3089 - accuracy: 0.9524\n",
            "Epoch 389/800\n",
            "105/105 [==============================] - 0s 91us/sample - loss: 0.3052 - accuracy: 0.9619\n",
            "Epoch 390/800\n",
            "105/105 [==============================] - 0s 89us/sample - loss: 0.3055 - accuracy: 0.9714\n",
            "Epoch 391/800\n",
            "105/105 [==============================] - 0s 86us/sample - loss: 0.3035 - accuracy: 0.9619\n",
            "Epoch 392/800\n",
            "105/105 [==============================] - 0s 84us/sample - loss: 0.3036 - accuracy: 0.9619\n",
            "Epoch 393/800\n",
            "105/105 [==============================] - 0s 88us/sample - loss: 0.3041 - accuracy: 0.9619\n",
            "Epoch 394/800\n",
            "105/105 [==============================] - 0s 65us/sample - loss: 0.3029 - accuracy: 0.9714\n",
            "Epoch 395/800\n",
            "105/105 [==============================] - 0s 82us/sample - loss: 0.3034 - accuracy: 0.9619\n",
            "Epoch 396/800\n",
            "105/105 [==============================] - 0s 86us/sample - loss: 0.3027 - accuracy: 0.9619\n",
            "Epoch 397/800\n",
            "105/105 [==============================] - 0s 73us/sample - loss: 0.3030 - accuracy: 0.9714\n",
            "Epoch 398/800\n",
            "105/105 [==============================] - 0s 64us/sample - loss: 0.3017 - accuracy: 0.9714\n",
            "Epoch 399/800\n",
            "105/105 [==============================] - 0s 72us/sample - loss: 0.3012 - accuracy: 0.9619\n",
            "Epoch 400/800\n",
            "105/105 [==============================] - 0s 73us/sample - loss: 0.3022 - accuracy: 0.9714\n",
            "Epoch 401/800\n",
            "105/105 [==============================] - 0s 82us/sample - loss: 0.3010 - accuracy: 0.9619\n",
            "Epoch 402/800\n",
            "105/105 [==============================] - 0s 80us/sample - loss: 0.3003 - accuracy: 0.9714\n",
            "Epoch 403/800\n",
            "105/105 [==============================] - 0s 105us/sample - loss: 0.3018 - accuracy: 0.9619\n",
            "Epoch 404/800\n",
            "105/105 [==============================] - 0s 98us/sample - loss: 0.3032 - accuracy: 0.9714\n",
            "Epoch 405/800\n",
            "105/105 [==============================] - 0s 91us/sample - loss: 0.2997 - accuracy: 0.9619\n",
            "Epoch 406/800\n",
            "105/105 [==============================] - 0s 98us/sample - loss: 0.2999 - accuracy: 0.9619\n",
            "Epoch 407/800\n",
            "105/105 [==============================] - 0s 116us/sample - loss: 0.3018 - accuracy: 0.9524\n",
            "Epoch 408/800\n",
            "105/105 [==============================] - 0s 77us/sample - loss: 0.3017 - accuracy: 0.9524\n",
            "Epoch 409/800\n",
            "105/105 [==============================] - 0s 76us/sample - loss: 0.2981 - accuracy: 0.9619\n",
            "Epoch 410/800\n",
            "105/105 [==============================] - 0s 85us/sample - loss: 0.2981 - accuracy: 0.9714\n",
            "Epoch 411/800\n",
            "105/105 [==============================] - 0s 80us/sample - loss: 0.2986 - accuracy: 0.9619\n",
            "Epoch 412/800\n",
            "105/105 [==============================] - 0s 81us/sample - loss: 0.3000 - accuracy: 0.9619\n",
            "Epoch 413/800\n",
            "105/105 [==============================] - 0s 84us/sample - loss: 0.2987 - accuracy: 0.9619\n",
            "Epoch 414/800\n",
            "105/105 [==============================] - 0s 87us/sample - loss: 0.2966 - accuracy: 0.9714\n",
            "Epoch 415/800\n",
            "105/105 [==============================] - 0s 82us/sample - loss: 0.2971 - accuracy: 0.9619\n",
            "Epoch 416/800\n",
            "105/105 [==============================] - 0s 90us/sample - loss: 0.2959 - accuracy: 0.9714\n",
            "Epoch 417/800\n",
            "105/105 [==============================] - 0s 81us/sample - loss: 0.2964 - accuracy: 0.9619\n",
            "Epoch 418/800\n",
            "105/105 [==============================] - 0s 92us/sample - loss: 0.2963 - accuracy: 0.9714\n",
            "Epoch 419/800\n",
            "105/105 [==============================] - 0s 115us/sample - loss: 0.2960 - accuracy: 0.9714\n",
            "Epoch 420/800\n",
            "105/105 [==============================] - 0s 79us/sample - loss: 0.2947 - accuracy: 0.9619\n",
            "Epoch 421/800\n",
            "105/105 [==============================] - 0s 81us/sample - loss: 0.2945 - accuracy: 0.9714\n",
            "Epoch 422/800\n",
            "105/105 [==============================] - 0s 92us/sample - loss: 0.2956 - accuracy: 0.9619\n",
            "Epoch 423/800\n",
            "105/105 [==============================] - 0s 98us/sample - loss: 0.2938 - accuracy: 0.9714\n",
            "Epoch 424/800\n",
            "105/105 [==============================] - 0s 83us/sample - loss: 0.2946 - accuracy: 0.9714\n",
            "Epoch 425/800\n",
            "105/105 [==============================] - 0s 108us/sample - loss: 0.2942 - accuracy: 0.9714\n",
            "Epoch 426/800\n",
            "105/105 [==============================] - 0s 89us/sample - loss: 0.2952 - accuracy: 0.9619\n",
            "Epoch 427/800\n",
            "105/105 [==============================] - 0s 76us/sample - loss: 0.2965 - accuracy: 0.9524\n",
            "Epoch 428/800\n",
            "105/105 [==============================] - 0s 77us/sample - loss: 0.2950 - accuracy: 0.9619\n",
            "Epoch 429/800\n",
            "105/105 [==============================] - 0s 86us/sample - loss: 0.2950 - accuracy: 0.9619\n",
            "Epoch 430/800\n",
            "105/105 [==============================] - 0s 73us/sample - loss: 0.2961 - accuracy: 0.9619\n",
            "Epoch 431/800\n",
            "105/105 [==============================] - 0s 84us/sample - loss: 0.2947 - accuracy: 0.9429\n",
            "Epoch 432/800\n",
            "105/105 [==============================] - 0s 119us/sample - loss: 0.2942 - accuracy: 0.9524\n",
            "Epoch 433/800\n",
            "105/105 [==============================] - 0s 88us/sample - loss: 0.2928 - accuracy: 0.9619\n",
            "Epoch 434/800\n",
            "105/105 [==============================] - 0s 93us/sample - loss: 0.2913 - accuracy: 0.9619\n",
            "Epoch 435/800\n",
            "105/105 [==============================] - 0s 100us/sample - loss: 0.2920 - accuracy: 0.9714\n",
            "Epoch 436/800\n",
            "105/105 [==============================] - 0s 89us/sample - loss: 0.2909 - accuracy: 0.9714\n",
            "Epoch 437/800\n",
            "105/105 [==============================] - 0s 98us/sample - loss: 0.2902 - accuracy: 0.9619\n",
            "Epoch 438/800\n",
            "105/105 [==============================] - 0s 123us/sample - loss: 0.2915 - accuracy: 0.9619\n",
            "Epoch 439/800\n",
            "105/105 [==============================] - 0s 81us/sample - loss: 0.2897 - accuracy: 0.9619\n",
            "Epoch 440/800\n",
            "105/105 [==============================] - 0s 94us/sample - loss: 0.2891 - accuracy: 0.9714\n",
            "Epoch 441/800\n",
            "105/105 [==============================] - 0s 78us/sample - loss: 0.2915 - accuracy: 0.9714\n",
            "Epoch 442/800\n",
            "105/105 [==============================] - 0s 103us/sample - loss: 0.2905 - accuracy: 0.9619\n",
            "Epoch 443/800\n",
            "105/105 [==============================] - 0s 88us/sample - loss: 0.2902 - accuracy: 0.9619\n",
            "Epoch 444/800\n",
            "105/105 [==============================] - 0s 78us/sample - loss: 0.2882 - accuracy: 0.9619\n",
            "Epoch 445/800\n",
            "105/105 [==============================] - 0s 77us/sample - loss: 0.2890 - accuracy: 0.9714\n",
            "Epoch 446/800\n",
            "105/105 [==============================] - 0s 78us/sample - loss: 0.2879 - accuracy: 0.9619\n",
            "Epoch 447/800\n",
            "105/105 [==============================] - 0s 83us/sample - loss: 0.2882 - accuracy: 0.9619\n",
            "Epoch 448/800\n",
            "105/105 [==============================] - 0s 81us/sample - loss: 0.2876 - accuracy: 0.9714\n",
            "Epoch 449/800\n",
            "105/105 [==============================] - 0s 80us/sample - loss: 0.2896 - accuracy: 0.9619\n",
            "Epoch 450/800\n",
            "105/105 [==============================] - 0s 95us/sample - loss: 0.2887 - accuracy: 0.9619\n",
            "Epoch 451/800\n",
            "105/105 [==============================] - 0s 106us/sample - loss: 0.2886 - accuracy: 0.9619\n",
            "Epoch 452/800\n",
            "105/105 [==============================] - 0s 109us/sample - loss: 0.2862 - accuracy: 0.9619\n",
            "Epoch 453/800\n",
            "105/105 [==============================] - 0s 90us/sample - loss: 0.2943 - accuracy: 0.9524\n",
            "Epoch 454/800\n",
            "105/105 [==============================] - 0s 89us/sample - loss: 0.2855 - accuracy: 0.9714\n",
            "Epoch 455/800\n",
            "105/105 [==============================] - 0s 87us/sample - loss: 0.2860 - accuracy: 0.9714\n",
            "Epoch 456/800\n",
            "105/105 [==============================] - 0s 88us/sample - loss: 0.2848 - accuracy: 0.9714\n",
            "Epoch 457/800\n",
            "105/105 [==============================] - 0s 84us/sample - loss: 0.2866 - accuracy: 0.9619\n",
            "Epoch 458/800\n",
            "105/105 [==============================] - 0s 80us/sample - loss: 0.2848 - accuracy: 0.9714\n",
            "Epoch 459/800\n",
            "105/105 [==============================] - 0s 91us/sample - loss: 0.2839 - accuracy: 0.9714\n",
            "Epoch 460/800\n",
            "105/105 [==============================] - 0s 132us/sample - loss: 0.2837 - accuracy: 0.9714\n",
            "Epoch 461/800\n",
            "105/105 [==============================] - 0s 111us/sample - loss: 0.2839 - accuracy: 0.9714\n",
            "Epoch 462/800\n",
            "105/105 [==============================] - 0s 108us/sample - loss: 0.2851 - accuracy: 0.9619\n",
            "Epoch 463/800\n",
            "105/105 [==============================] - 0s 97us/sample - loss: 0.2851 - accuracy: 0.9619\n",
            "Epoch 464/800\n",
            "105/105 [==============================] - 0s 101us/sample - loss: 0.2830 - accuracy: 0.9619\n",
            "Epoch 465/800\n",
            "105/105 [==============================] - 0s 84us/sample - loss: 0.2822 - accuracy: 0.9619\n",
            "Epoch 466/800\n",
            "105/105 [==============================] - 0s 72us/sample - loss: 0.2841 - accuracy: 0.9714\n",
            "Epoch 467/800\n",
            "105/105 [==============================] - 0s 69us/sample - loss: 0.2820 - accuracy: 0.9714\n",
            "Epoch 468/800\n",
            "105/105 [==============================] - 0s 81us/sample - loss: 0.2826 - accuracy: 0.9714\n",
            "Epoch 469/800\n",
            "105/105 [==============================] - 0s 86us/sample - loss: 0.2820 - accuracy: 0.9714\n",
            "Epoch 470/800\n",
            "105/105 [==============================] - 0s 86us/sample - loss: 0.2834 - accuracy: 0.9714\n",
            "Epoch 471/800\n",
            "105/105 [==============================] - 0s 83us/sample - loss: 0.2822 - accuracy: 0.9714\n",
            "Epoch 472/800\n",
            "105/105 [==============================] - 0s 77us/sample - loss: 0.2825 - accuracy: 0.9619\n",
            "Epoch 473/800\n",
            "105/105 [==============================] - 0s 70us/sample - loss: 0.2809 - accuracy: 0.9714\n",
            "Epoch 474/800\n",
            "105/105 [==============================] - 0s 66us/sample - loss: 0.2803 - accuracy: 0.9714\n",
            "Epoch 475/800\n",
            "105/105 [==============================] - 0s 76us/sample - loss: 0.2821 - accuracy: 0.9619\n",
            "Epoch 476/800\n",
            "105/105 [==============================] - 0s 83us/sample - loss: 0.2852 - accuracy: 0.9524\n",
            "Epoch 477/800\n",
            "105/105 [==============================] - 0s 91us/sample - loss: 0.2798 - accuracy: 0.9714\n",
            "Epoch 478/800\n",
            "105/105 [==============================] - 0s 55us/sample - loss: 0.2805 - accuracy: 0.9714\n",
            "Epoch 479/800\n",
            "105/105 [==============================] - 0s 81us/sample - loss: 0.2811 - accuracy: 0.9619\n",
            "Epoch 480/800\n",
            "105/105 [==============================] - 0s 84us/sample - loss: 0.2806 - accuracy: 0.9619\n",
            "Epoch 481/800\n",
            "105/105 [==============================] - 0s 73us/sample - loss: 0.2796 - accuracy: 0.9714\n",
            "Epoch 482/800\n",
            "105/105 [==============================] - 0s 96us/sample - loss: 0.2788 - accuracy: 0.9714\n",
            "Epoch 483/800\n",
            "105/105 [==============================] - 0s 107us/sample - loss: 0.2780 - accuracy: 0.9714\n",
            "Epoch 484/800\n",
            "105/105 [==============================] - 0s 78us/sample - loss: 0.2778 - accuracy: 0.9619\n",
            "Epoch 485/800\n",
            "105/105 [==============================] - 0s 77us/sample - loss: 0.2807 - accuracy: 0.9714\n",
            "Epoch 486/800\n",
            "105/105 [==============================] - 0s 69us/sample - loss: 0.2806 - accuracy: 0.9714\n",
            "Epoch 487/800\n",
            "105/105 [==============================] - 0s 72us/sample - loss: 0.2840 - accuracy: 0.9524\n",
            "Epoch 488/800\n",
            "105/105 [==============================] - 0s 69us/sample - loss: 0.2799 - accuracy: 0.9619\n",
            "Epoch 489/800\n",
            "105/105 [==============================] - 0s 77us/sample - loss: 0.2782 - accuracy: 0.9619\n",
            "Epoch 490/800\n",
            "105/105 [==============================] - 0s 86us/sample - loss: 0.2795 - accuracy: 0.9619\n",
            "Epoch 491/800\n",
            "105/105 [==============================] - 0s 79us/sample - loss: 0.2774 - accuracy: 0.9619\n",
            "Epoch 492/800\n",
            "105/105 [==============================] - 0s 137us/sample - loss: 0.2780 - accuracy: 0.9429\n",
            "Epoch 493/800\n",
            "105/105 [==============================] - 0s 93us/sample - loss: 0.2757 - accuracy: 0.9619\n",
            "Epoch 494/800\n",
            "105/105 [==============================] - 0s 71us/sample - loss: 0.2765 - accuracy: 0.9714\n",
            "Epoch 495/800\n",
            "105/105 [==============================] - 0s 82us/sample - loss: 0.2804 - accuracy: 0.9619\n",
            "Epoch 496/800\n",
            "105/105 [==============================] - 0s 74us/sample - loss: 0.2756 - accuracy: 0.9619\n",
            "Epoch 497/800\n",
            "105/105 [==============================] - 0s 79us/sample - loss: 0.2755 - accuracy: 0.9619\n",
            "Epoch 498/800\n",
            "105/105 [==============================] - 0s 69us/sample - loss: 0.2744 - accuracy: 0.9619\n",
            "Epoch 499/800\n",
            "105/105 [==============================] - 0s 80us/sample - loss: 0.2752 - accuracy: 0.9619\n",
            "Epoch 500/800\n",
            "105/105 [==============================] - 0s 72us/sample - loss: 0.2758 - accuracy: 0.9714\n",
            "Epoch 501/800\n",
            "105/105 [==============================] - 0s 102us/sample - loss: 0.2748 - accuracy: 0.9619\n",
            "Epoch 502/800\n",
            "105/105 [==============================] - 0s 75us/sample - loss: 0.2782 - accuracy: 0.9714\n",
            "Epoch 503/800\n",
            "105/105 [==============================] - 0s 91us/sample - loss: 0.2781 - accuracy: 0.9619\n",
            "Epoch 504/800\n",
            "105/105 [==============================] - 0s 77us/sample - loss: 0.2741 - accuracy: 0.9714\n",
            "Epoch 505/800\n",
            "105/105 [==============================] - 0s 79us/sample - loss: 0.2742 - accuracy: 0.9619\n",
            "Epoch 506/800\n",
            "105/105 [==============================] - 0s 73us/sample - loss: 0.2723 - accuracy: 0.9619\n",
            "Epoch 507/800\n",
            "105/105 [==============================] - 0s 91us/sample - loss: 0.2730 - accuracy: 0.9714\n",
            "Epoch 508/800\n",
            "105/105 [==============================] - 0s 82us/sample - loss: 0.2735 - accuracy: 0.9619\n",
            "Epoch 509/800\n",
            "105/105 [==============================] - 0s 89us/sample - loss: 0.2735 - accuracy: 0.9619\n",
            "Epoch 510/800\n",
            "105/105 [==============================] - 0s 80us/sample - loss: 0.2740 - accuracy: 0.9619\n",
            "Epoch 511/800\n",
            "105/105 [==============================] - 0s 78us/sample - loss: 0.2712 - accuracy: 0.9619\n",
            "Epoch 512/800\n",
            "105/105 [==============================] - 0s 68us/sample - loss: 0.2714 - accuracy: 0.9714\n",
            "Epoch 513/800\n",
            "105/105 [==============================] - 0s 76us/sample - loss: 0.2709 - accuracy: 0.9714\n",
            "Epoch 514/800\n",
            "105/105 [==============================] - 0s 71us/sample - loss: 0.2711 - accuracy: 0.9714\n",
            "Epoch 515/800\n",
            "105/105 [==============================] - 0s 71us/sample - loss: 0.2709 - accuracy: 0.9714\n",
            "Epoch 516/800\n",
            "105/105 [==============================] - 0s 82us/sample - loss: 0.2727 - accuracy: 0.9619\n",
            "Epoch 517/800\n",
            "105/105 [==============================] - 0s 80us/sample - loss: 0.2751 - accuracy: 0.9714\n",
            "Epoch 518/800\n",
            "105/105 [==============================] - 0s 83us/sample - loss: 0.2724 - accuracy: 0.9524\n",
            "Epoch 519/800\n",
            "105/105 [==============================] - 0s 93us/sample - loss: 0.2712 - accuracy: 0.9524\n",
            "Epoch 520/800\n",
            "105/105 [==============================] - 0s 76us/sample - loss: 0.2719 - accuracy: 0.9524\n",
            "Epoch 521/800\n",
            "105/105 [==============================] - 0s 114us/sample - loss: 0.2703 - accuracy: 0.9619\n",
            "Epoch 522/800\n",
            "105/105 [==============================] - 0s 97us/sample - loss: 0.2702 - accuracy: 0.9619\n",
            "Epoch 523/800\n",
            "105/105 [==============================] - 0s 79us/sample - loss: 0.2685 - accuracy: 0.9714\n",
            "Epoch 524/800\n",
            "105/105 [==============================] - 0s 89us/sample - loss: 0.2728 - accuracy: 0.9524\n",
            "Epoch 525/800\n",
            "105/105 [==============================] - 0s 81us/sample - loss: 0.2692 - accuracy: 0.9714\n",
            "Epoch 526/800\n",
            "105/105 [==============================] - 0s 79us/sample - loss: 0.2701 - accuracy: 0.9619\n",
            "Epoch 527/800\n",
            "105/105 [==============================] - 0s 91us/sample - loss: 0.2697 - accuracy: 0.9714\n",
            "Epoch 528/800\n",
            "105/105 [==============================] - 0s 104us/sample - loss: 0.2699 - accuracy: 0.9619\n",
            "Epoch 529/800\n",
            "105/105 [==============================] - 0s 86us/sample - loss: 0.2677 - accuracy: 0.9619\n",
            "Epoch 530/800\n",
            "105/105 [==============================] - 0s 88us/sample - loss: 0.2674 - accuracy: 0.9619\n",
            "Epoch 531/800\n",
            "105/105 [==============================] - 0s 90us/sample - loss: 0.2673 - accuracy: 0.9714\n",
            "Epoch 532/800\n",
            "105/105 [==============================] - 0s 83us/sample - loss: 0.2700 - accuracy: 0.9714\n",
            "Epoch 533/800\n",
            "105/105 [==============================] - 0s 114us/sample - loss: 0.2692 - accuracy: 0.9619\n",
            "Epoch 534/800\n",
            "105/105 [==============================] - 0s 90us/sample - loss: 0.2701 - accuracy: 0.9619\n",
            "Epoch 535/800\n",
            "105/105 [==============================] - 0s 104us/sample - loss: 0.2710 - accuracy: 0.9714\n",
            "Epoch 536/800\n",
            "105/105 [==============================] - 0s 102us/sample - loss: 0.2675 - accuracy: 0.9619\n",
            "Epoch 537/800\n",
            "105/105 [==============================] - 0s 112us/sample - loss: 0.2657 - accuracy: 0.9714\n",
            "Epoch 538/800\n",
            "105/105 [==============================] - 0s 85us/sample - loss: 0.2662 - accuracy: 0.9714\n",
            "Epoch 539/800\n",
            "105/105 [==============================] - 0s 84us/sample - loss: 0.2655 - accuracy: 0.9619\n",
            "Epoch 540/800\n",
            "105/105 [==============================] - 0s 85us/sample - loss: 0.2648 - accuracy: 0.9714\n",
            "Epoch 541/800\n",
            "105/105 [==============================] - 0s 86us/sample - loss: 0.2652 - accuracy: 0.9714\n",
            "Epoch 542/800\n",
            "105/105 [==============================] - 0s 79us/sample - loss: 0.2644 - accuracy: 0.9714\n",
            "Epoch 543/800\n",
            "105/105 [==============================] - 0s 88us/sample - loss: 0.2674 - accuracy: 0.9619\n",
            "Epoch 544/800\n",
            "105/105 [==============================] - 0s 74us/sample - loss: 0.2643 - accuracy: 0.9619\n",
            "Epoch 545/800\n",
            "105/105 [==============================] - 0s 89us/sample - loss: 0.2639 - accuracy: 0.9619\n",
            "Epoch 546/800\n",
            "105/105 [==============================] - 0s 92us/sample - loss: 0.2640 - accuracy: 0.9714\n",
            "Epoch 547/800\n",
            "105/105 [==============================] - 0s 94us/sample - loss: 0.2642 - accuracy: 0.9524\n",
            "Epoch 548/800\n",
            "105/105 [==============================] - 0s 90us/sample - loss: 0.2648 - accuracy: 0.9619\n",
            "Epoch 549/800\n",
            "105/105 [==============================] - 0s 80us/sample - loss: 0.2630 - accuracy: 0.9714\n",
            "Epoch 550/800\n",
            "105/105 [==============================] - 0s 98us/sample - loss: 0.2676 - accuracy: 0.9429\n",
            "Epoch 551/800\n",
            "105/105 [==============================] - 0s 88us/sample - loss: 0.2635 - accuracy: 0.9619\n",
            "Epoch 552/800\n",
            "105/105 [==============================] - 0s 82us/sample - loss: 0.2625 - accuracy: 0.9714\n",
            "Epoch 553/800\n",
            "105/105 [==============================] - 0s 112us/sample - loss: 0.2632 - accuracy: 0.9714\n",
            "Epoch 554/800\n",
            "105/105 [==============================] - 0s 95us/sample - loss: 0.2628 - accuracy: 0.9714\n",
            "Epoch 555/800\n",
            "105/105 [==============================] - 0s 79us/sample - loss: 0.2641 - accuracy: 0.9619\n",
            "Epoch 556/800\n",
            "105/105 [==============================] - 0s 92us/sample - loss: 0.2639 - accuracy: 0.9619\n",
            "Epoch 557/800\n",
            "105/105 [==============================] - 0s 77us/sample - loss: 0.2632 - accuracy: 0.9619\n",
            "Epoch 558/800\n",
            "105/105 [==============================] - 0s 112us/sample - loss: 0.2634 - accuracy: 0.9619\n",
            "Epoch 559/800\n",
            "105/105 [==============================] - 0s 96us/sample - loss: 0.2637 - accuracy: 0.9524\n",
            "Epoch 560/800\n",
            "105/105 [==============================] - 0s 82us/sample - loss: 0.2616 - accuracy: 0.9619\n",
            "Epoch 561/800\n",
            "105/105 [==============================] - 0s 147us/sample - loss: 0.2616 - accuracy: 0.9619\n",
            "Epoch 562/800\n",
            "105/105 [==============================] - 0s 92us/sample - loss: 0.2641 - accuracy: 0.9619\n",
            "Epoch 563/800\n",
            "105/105 [==============================] - 0s 95us/sample - loss: 0.2601 - accuracy: 0.9714\n",
            "Epoch 564/800\n",
            "105/105 [==============================] - 0s 88us/sample - loss: 0.2611 - accuracy: 0.9714\n",
            "Epoch 565/800\n",
            "105/105 [==============================] - 0s 94us/sample - loss: 0.2599 - accuracy: 0.9714\n",
            "Epoch 566/800\n",
            "105/105 [==============================] - 0s 86us/sample - loss: 0.2596 - accuracy: 0.9714\n",
            "Epoch 567/800\n",
            "105/105 [==============================] - 0s 99us/sample - loss: 0.2595 - accuracy: 0.9714\n",
            "Epoch 568/800\n",
            "105/105 [==============================] - 0s 88us/sample - loss: 0.2591 - accuracy: 0.9714\n",
            "Epoch 569/800\n",
            "105/105 [==============================] - 0s 80us/sample - loss: 0.2597 - accuracy: 0.9619\n",
            "Epoch 570/800\n",
            "105/105 [==============================] - 0s 76us/sample - loss: 0.2600 - accuracy: 0.9619\n",
            "Epoch 571/800\n",
            "105/105 [==============================] - 0s 88us/sample - loss: 0.2596 - accuracy: 0.9619\n",
            "Epoch 572/800\n",
            "105/105 [==============================] - 0s 85us/sample - loss: 0.2595 - accuracy: 0.9619\n",
            "Epoch 573/800\n",
            "105/105 [==============================] - 0s 91us/sample - loss: 0.2610 - accuracy: 0.9619\n",
            "Epoch 574/800\n",
            "105/105 [==============================] - 0s 101us/sample - loss: 0.2608 - accuracy: 0.9619\n",
            "Epoch 575/800\n",
            "105/105 [==============================] - 0s 78us/sample - loss: 0.2580 - accuracy: 0.9619\n",
            "Epoch 576/800\n",
            "105/105 [==============================] - 0s 82us/sample - loss: 0.2583 - accuracy: 0.9619\n",
            "Epoch 577/800\n",
            "105/105 [==============================] - 0s 80us/sample - loss: 0.2575 - accuracy: 0.9714\n",
            "Epoch 578/800\n",
            "105/105 [==============================] - 0s 81us/sample - loss: 0.2601 - accuracy: 0.9524\n",
            "Epoch 579/800\n",
            "105/105 [==============================] - 0s 92us/sample - loss: 0.2575 - accuracy: 0.9619\n",
            "Epoch 580/800\n",
            "105/105 [==============================] - 0s 97us/sample - loss: 0.2568 - accuracy: 0.9714\n",
            "Epoch 581/800\n",
            "105/105 [==============================] - 0s 87us/sample - loss: 0.2583 - accuracy: 0.9619\n",
            "Epoch 582/800\n",
            "105/105 [==============================] - 0s 75us/sample - loss: 0.2582 - accuracy: 0.9619\n",
            "Epoch 583/800\n",
            "105/105 [==============================] - 0s 68us/sample - loss: 0.2582 - accuracy: 0.9619\n",
            "Epoch 584/800\n",
            "105/105 [==============================] - 0s 85us/sample - loss: 0.2570 - accuracy: 0.9524\n",
            "Epoch 585/800\n",
            "105/105 [==============================] - 0s 106us/sample - loss: 0.2561 - accuracy: 0.9714\n",
            "Epoch 586/800\n",
            "105/105 [==============================] - 0s 88us/sample - loss: 0.2558 - accuracy: 0.9714\n",
            "Epoch 587/800\n",
            "105/105 [==============================] - 0s 95us/sample - loss: 0.2562 - accuracy: 0.9714\n",
            "Epoch 588/800\n",
            "105/105 [==============================] - 0s 80us/sample - loss: 0.2591 - accuracy: 0.9524\n",
            "Epoch 589/800\n",
            "105/105 [==============================] - 0s 70us/sample - loss: 0.2561 - accuracy: 0.9524\n",
            "Epoch 590/800\n",
            "105/105 [==============================] - 0s 76us/sample - loss: 0.2578 - accuracy: 0.9619\n",
            "Epoch 591/800\n",
            "105/105 [==============================] - 0s 78us/sample - loss: 0.2553 - accuracy: 0.9714\n",
            "Epoch 592/800\n",
            "105/105 [==============================] - 0s 84us/sample - loss: 0.2579 - accuracy: 0.9714\n",
            "Epoch 593/800\n",
            "105/105 [==============================] - 0s 80us/sample - loss: 0.2552 - accuracy: 0.9524\n",
            "Epoch 594/800\n",
            "105/105 [==============================] - 0s 79us/sample - loss: 0.2561 - accuracy: 0.9619\n",
            "Epoch 595/800\n",
            "105/105 [==============================] - 0s 69us/sample - loss: 0.2593 - accuracy: 0.9714\n",
            "Epoch 596/800\n",
            "105/105 [==============================] - 0s 67us/sample - loss: 0.2537 - accuracy: 0.9714\n",
            "Epoch 597/800\n",
            "105/105 [==============================] - 0s 70us/sample - loss: 0.2537 - accuracy: 0.9619\n",
            "Epoch 598/800\n",
            "105/105 [==============================] - 0s 79us/sample - loss: 0.2545 - accuracy: 0.9619\n",
            "Epoch 599/800\n",
            "105/105 [==============================] - 0s 81us/sample - loss: 0.2549 - accuracy: 0.9714\n",
            "Epoch 600/800\n",
            "105/105 [==============================] - 0s 78us/sample - loss: 0.2532 - accuracy: 0.9524\n",
            "Epoch 601/800\n",
            "105/105 [==============================] - 0s 78us/sample - loss: 0.2525 - accuracy: 0.9714\n",
            "Epoch 602/800\n",
            "105/105 [==============================] - 0s 70us/sample - loss: 0.2537 - accuracy: 0.9619\n",
            "Epoch 603/800\n",
            "105/105 [==============================] - 0s 73us/sample - loss: 0.2524 - accuracy: 0.9714\n",
            "Epoch 604/800\n",
            "105/105 [==============================] - 0s 89us/sample - loss: 0.2527 - accuracy: 0.9714\n",
            "Epoch 605/800\n",
            "105/105 [==============================] - 0s 84us/sample - loss: 0.2519 - accuracy: 0.9714\n",
            "Epoch 606/800\n",
            "105/105 [==============================] - 0s 92us/sample - loss: 0.2527 - accuracy: 0.9619\n",
            "Epoch 607/800\n",
            "105/105 [==============================] - 0s 100us/sample - loss: 0.2526 - accuracy: 0.9619\n",
            "Epoch 608/800\n",
            "105/105 [==============================] - 0s 81us/sample - loss: 0.2526 - accuracy: 0.9619\n",
            "Epoch 609/800\n",
            "105/105 [==============================] - 0s 64us/sample - loss: 0.2525 - accuracy: 0.9714\n",
            "Epoch 610/800\n",
            "105/105 [==============================] - 0s 90us/sample - loss: 0.2510 - accuracy: 0.9714\n",
            "Epoch 611/800\n",
            "105/105 [==============================] - 0s 98us/sample - loss: 0.2514 - accuracy: 0.9619\n",
            "Epoch 612/800\n",
            "105/105 [==============================] - 0s 94us/sample - loss: 0.2505 - accuracy: 0.9714\n",
            "Epoch 613/800\n",
            "105/105 [==============================] - 0s 91us/sample - loss: 0.2520 - accuracy: 0.9619\n",
            "Epoch 614/800\n",
            "105/105 [==============================] - 0s 77us/sample - loss: 0.2513 - accuracy: 0.9619\n",
            "Epoch 615/800\n",
            "105/105 [==============================] - 0s 78us/sample - loss: 0.2501 - accuracy: 0.9619\n",
            "Epoch 616/800\n",
            "105/105 [==============================] - 0s 69us/sample - loss: 0.2512 - accuracy: 0.9619\n",
            "Epoch 617/800\n",
            "105/105 [==============================] - 0s 68us/sample - loss: 0.2499 - accuracy: 0.9619\n",
            "Epoch 618/800\n",
            "105/105 [==============================] - 0s 72us/sample - loss: 0.2507 - accuracy: 0.9619\n",
            "Epoch 619/800\n",
            "105/105 [==============================] - 0s 82us/sample - loss: 0.2497 - accuracy: 0.9714\n",
            "Epoch 620/800\n",
            "105/105 [==============================] - 0s 86us/sample - loss: 0.2510 - accuracy: 0.9619\n",
            "Epoch 621/800\n",
            "105/105 [==============================] - 0s 85us/sample - loss: 0.2498 - accuracy: 0.9619\n",
            "Epoch 622/800\n",
            "105/105 [==============================] - 0s 86us/sample - loss: 0.2489 - accuracy: 0.9714\n",
            "Epoch 623/800\n",
            "105/105 [==============================] - 0s 86us/sample - loss: 0.2491 - accuracy: 0.9714\n",
            "Epoch 624/800\n",
            "105/105 [==============================] - 0s 82us/sample - loss: 0.2495 - accuracy: 0.9714\n",
            "Epoch 625/800\n",
            "105/105 [==============================] - 0s 113us/sample - loss: 0.2514 - accuracy: 0.9619\n",
            "Epoch 626/800\n",
            "105/105 [==============================] - 0s 105us/sample - loss: 0.2483 - accuracy: 0.9714\n",
            "Epoch 627/800\n",
            "105/105 [==============================] - 0s 87us/sample - loss: 0.2492 - accuracy: 0.9619\n",
            "Epoch 628/800\n",
            "105/105 [==============================] - 0s 112us/sample - loss: 0.2492 - accuracy: 0.9619\n",
            "Epoch 629/800\n",
            "105/105 [==============================] - 0s 84us/sample - loss: 0.2486 - accuracy: 0.9714\n",
            "Epoch 630/800\n",
            "105/105 [==============================] - 0s 96us/sample - loss: 0.2479 - accuracy: 0.9619\n",
            "Epoch 631/800\n",
            "105/105 [==============================] - 0s 90us/sample - loss: 0.2480 - accuracy: 0.9714\n",
            "Epoch 632/800\n",
            "105/105 [==============================] - 0s 102us/sample - loss: 0.2478 - accuracy: 0.9619\n",
            "Epoch 633/800\n",
            "105/105 [==============================] - 0s 81us/sample - loss: 0.2484 - accuracy: 0.9619\n",
            "Epoch 634/800\n",
            "105/105 [==============================] - 0s 95us/sample - loss: 0.2483 - accuracy: 0.9619\n",
            "Epoch 635/800\n",
            "105/105 [==============================] - 0s 99us/sample - loss: 0.2478 - accuracy: 0.9619\n",
            "Epoch 636/800\n",
            "105/105 [==============================] - 0s 95us/sample - loss: 0.2465 - accuracy: 0.9619\n",
            "Epoch 637/800\n",
            "105/105 [==============================] - 0s 98us/sample - loss: 0.2475 - accuracy: 0.9619\n",
            "Epoch 638/800\n",
            "105/105 [==============================] - 0s 83us/sample - loss: 0.2479 - accuracy: 0.9619\n",
            "Epoch 639/800\n",
            "105/105 [==============================] - 0s 75us/sample - loss: 0.2479 - accuracy: 0.9619\n",
            "Epoch 640/800\n",
            "105/105 [==============================] - 0s 97us/sample - loss: 0.2459 - accuracy: 0.9714\n",
            "Epoch 641/800\n",
            "105/105 [==============================] - 0s 87us/sample - loss: 0.2478 - accuracy: 0.9619\n",
            "Epoch 642/800\n",
            "105/105 [==============================] - 0s 86us/sample - loss: 0.2459 - accuracy: 0.9619\n",
            "Epoch 643/800\n",
            "105/105 [==============================] - 0s 94us/sample - loss: 0.2459 - accuracy: 0.9619\n",
            "Epoch 644/800\n",
            "105/105 [==============================] - 0s 96us/sample - loss: 0.2453 - accuracy: 0.9619\n",
            "Epoch 645/800\n",
            "105/105 [==============================] - 0s 83us/sample - loss: 0.2449 - accuracy: 0.9619\n",
            "Epoch 646/800\n",
            "105/105 [==============================] - 0s 78us/sample - loss: 0.2456 - accuracy: 0.9714\n",
            "Epoch 647/800\n",
            "105/105 [==============================] - 0s 74us/sample - loss: 0.2462 - accuracy: 0.9619\n",
            "Epoch 648/800\n",
            "105/105 [==============================] - 0s 76us/sample - loss: 0.2469 - accuracy: 0.9619\n",
            "Epoch 649/800\n",
            "105/105 [==============================] - 0s 98us/sample - loss: 0.2444 - accuracy: 0.9714\n",
            "Epoch 650/800\n",
            "105/105 [==============================] - 0s 73us/sample - loss: 0.2442 - accuracy: 0.9619\n",
            "Epoch 651/800\n",
            "105/105 [==============================] - 0s 80us/sample - loss: 0.2445 - accuracy: 0.9619\n",
            "Epoch 652/800\n",
            "105/105 [==============================] - 0s 96us/sample - loss: 0.2458 - accuracy: 0.9619\n",
            "Epoch 653/800\n",
            "105/105 [==============================] - 0s 93us/sample - loss: 0.2447 - accuracy: 0.9619\n",
            "Epoch 654/800\n",
            "105/105 [==============================] - 0s 95us/sample - loss: 0.2450 - accuracy: 0.9619\n",
            "Epoch 655/800\n",
            "105/105 [==============================] - 0s 83us/sample - loss: 0.2432 - accuracy: 0.9714\n",
            "Epoch 656/800\n",
            "105/105 [==============================] - 0s 70us/sample - loss: 0.2434 - accuracy: 0.9619\n",
            "Epoch 657/800\n",
            "105/105 [==============================] - 0s 91us/sample - loss: 0.2438 - accuracy: 0.9619\n",
            "Epoch 658/800\n",
            "105/105 [==============================] - 0s 112us/sample - loss: 0.2453 - accuracy: 0.9619\n",
            "Epoch 659/800\n",
            "105/105 [==============================] - 0s 103us/sample - loss: 0.2428 - accuracy: 0.9619\n",
            "Epoch 660/800\n",
            "105/105 [==============================] - 0s 83us/sample - loss: 0.2422 - accuracy: 0.9714\n",
            "Epoch 661/800\n",
            "105/105 [==============================] - 0s 77us/sample - loss: 0.2438 - accuracy: 0.9619\n",
            "Epoch 662/800\n",
            "105/105 [==============================] - 0s 82us/sample - loss: 0.2425 - accuracy: 0.9619\n",
            "Epoch 663/800\n",
            "105/105 [==============================] - 0s 76us/sample - loss: 0.2421 - accuracy: 0.9714\n",
            "Epoch 664/800\n",
            "105/105 [==============================] - 0s 84us/sample - loss: 0.2423 - accuracy: 0.9714\n",
            "Epoch 665/800\n",
            "105/105 [==============================] - 0s 162us/sample - loss: 0.2415 - accuracy: 0.9714\n",
            "Epoch 666/800\n",
            "105/105 [==============================] - 0s 83us/sample - loss: 0.2419 - accuracy: 0.9619\n",
            "Epoch 667/800\n",
            "105/105 [==============================] - 0s 80us/sample - loss: 0.2420 - accuracy: 0.9714\n",
            "Epoch 668/800\n",
            "105/105 [==============================] - 0s 102us/sample - loss: 0.2423 - accuracy: 0.9619\n",
            "Epoch 669/800\n",
            "105/105 [==============================] - 0s 84us/sample - loss: 0.2411 - accuracy: 0.9619\n",
            "Epoch 670/800\n",
            "105/105 [==============================] - 0s 91us/sample - loss: 0.2409 - accuracy: 0.9714\n",
            "Epoch 671/800\n",
            "105/105 [==============================] - 0s 91us/sample - loss: 0.2420 - accuracy: 0.9619\n",
            "Epoch 672/800\n",
            "105/105 [==============================] - 0s 86us/sample - loss: 0.2402 - accuracy: 0.9714\n",
            "Epoch 673/800\n",
            "105/105 [==============================] - 0s 83us/sample - loss: 0.2406 - accuracy: 0.9619\n",
            "Epoch 674/800\n",
            "105/105 [==============================] - 0s 84us/sample - loss: 0.2422 - accuracy: 0.9619\n",
            "Epoch 675/800\n",
            "105/105 [==============================] - 0s 77us/sample - loss: 0.2401 - accuracy: 0.9619\n",
            "Epoch 676/800\n",
            "105/105 [==============================] - 0s 83us/sample - loss: 0.2401 - accuracy: 0.9714\n",
            "Epoch 677/800\n",
            "105/105 [==============================] - 0s 83us/sample - loss: 0.2397 - accuracy: 0.9714\n",
            "Epoch 678/800\n",
            "105/105 [==============================] - 0s 95us/sample - loss: 0.2427 - accuracy: 0.9714\n",
            "Epoch 679/800\n",
            "105/105 [==============================] - 0s 93us/sample - loss: 0.2417 - accuracy: 0.9619\n",
            "Epoch 680/800\n",
            "105/105 [==============================] - 0s 89us/sample - loss: 0.2396 - accuracy: 0.9619\n",
            "Epoch 681/800\n",
            "105/105 [==============================] - 0s 81us/sample - loss: 0.2394 - accuracy: 0.9714\n",
            "Epoch 682/800\n",
            "105/105 [==============================] - 0s 90us/sample - loss: 0.2396 - accuracy: 0.9619\n",
            "Epoch 683/800\n",
            "105/105 [==============================] - 0s 89us/sample - loss: 0.2394 - accuracy: 0.9619\n",
            "Epoch 684/800\n",
            "105/105 [==============================] - 0s 75us/sample - loss: 0.2396 - accuracy: 0.9619\n",
            "Epoch 685/800\n",
            "105/105 [==============================] - 0s 75us/sample - loss: 0.2388 - accuracy: 0.9619\n",
            "Epoch 686/800\n",
            "105/105 [==============================] - 0s 77us/sample - loss: 0.2390 - accuracy: 0.9619\n",
            "Epoch 687/800\n",
            "105/105 [==============================] - 0s 76us/sample - loss: 0.2389 - accuracy: 0.9714\n",
            "Epoch 688/800\n",
            "105/105 [==============================] - 0s 100us/sample - loss: 0.2395 - accuracy: 0.9619\n",
            "Epoch 689/800\n",
            "105/105 [==============================] - 0s 72us/sample - loss: 0.2377 - accuracy: 0.9619\n",
            "Epoch 690/800\n",
            "105/105 [==============================] - 0s 82us/sample - loss: 0.2383 - accuracy: 0.9714\n",
            "Epoch 691/800\n",
            "105/105 [==============================] - 0s 87us/sample - loss: 0.2382 - accuracy: 0.9619\n",
            "Epoch 692/800\n",
            "105/105 [==============================] - 0s 92us/sample - loss: 0.2385 - accuracy: 0.9619\n",
            "Epoch 693/800\n",
            "105/105 [==============================] - 0s 84us/sample - loss: 0.2370 - accuracy: 0.9619\n",
            "Epoch 694/800\n",
            "105/105 [==============================] - 0s 125us/sample - loss: 0.2376 - accuracy: 0.9619\n",
            "Epoch 695/800\n",
            "105/105 [==============================] - 0s 81us/sample - loss: 0.2366 - accuracy: 0.9619\n",
            "Epoch 696/800\n",
            "105/105 [==============================] - 0s 78us/sample - loss: 0.2365 - accuracy: 0.9714\n",
            "Epoch 697/800\n",
            "105/105 [==============================] - 0s 118us/sample - loss: 0.2365 - accuracy: 0.9714\n",
            "Epoch 698/800\n",
            "105/105 [==============================] - 0s 92us/sample - loss: 0.2369 - accuracy: 0.9619\n",
            "Epoch 699/800\n",
            "105/105 [==============================] - 0s 122us/sample - loss: 0.2384 - accuracy: 0.9714\n",
            "Epoch 700/800\n",
            "105/105 [==============================] - 0s 96us/sample - loss: 0.2365 - accuracy: 0.9619\n",
            "Epoch 701/800\n",
            "105/105 [==============================] - 0s 79us/sample - loss: 0.2361 - accuracy: 0.9714\n",
            "Epoch 702/800\n",
            "105/105 [==============================] - 0s 89us/sample - loss: 0.2365 - accuracy: 0.9619\n",
            "Epoch 703/800\n",
            "105/105 [==============================] - 0s 82us/sample - loss: 0.2364 - accuracy: 0.9619\n",
            "Epoch 704/800\n",
            "105/105 [==============================] - 0s 86us/sample - loss: 0.2359 - accuracy: 0.9619\n",
            "Epoch 705/800\n",
            "105/105 [==============================] - 0s 79us/sample - loss: 0.2389 - accuracy: 0.9714\n",
            "Epoch 706/800\n",
            "105/105 [==============================] - 0s 79us/sample - loss: 0.2359 - accuracy: 0.9619\n",
            "Epoch 707/800\n",
            "105/105 [==============================] - 0s 98us/sample - loss: 0.2352 - accuracy: 0.9714\n",
            "Epoch 708/800\n",
            "105/105 [==============================] - 0s 83us/sample - loss: 0.2360 - accuracy: 0.9619\n",
            "Epoch 709/800\n",
            "105/105 [==============================] - 0s 86us/sample - loss: 0.2371 - accuracy: 0.9619\n",
            "Epoch 710/800\n",
            "105/105 [==============================] - 0s 86us/sample - loss: 0.2365 - accuracy: 0.9619\n",
            "Epoch 711/800\n",
            "105/105 [==============================] - 0s 75us/sample - loss: 0.2362 - accuracy: 0.9619\n",
            "Epoch 712/800\n",
            "105/105 [==============================] - 0s 148us/sample - loss: 0.2359 - accuracy: 0.9619\n",
            "Epoch 713/800\n",
            "105/105 [==============================] - 0s 74us/sample - loss: 0.2350 - accuracy: 0.9714\n",
            "Epoch 714/800\n",
            "105/105 [==============================] - 0s 80us/sample - loss: 0.2339 - accuracy: 0.9714\n",
            "Epoch 715/800\n",
            "105/105 [==============================] - 0s 73us/sample - loss: 0.2344 - accuracy: 0.9619\n",
            "Epoch 716/800\n",
            "105/105 [==============================] - 0s 87us/sample - loss: 0.2340 - accuracy: 0.9619\n",
            "Epoch 717/800\n",
            "105/105 [==============================] - 0s 109us/sample - loss: 0.2337 - accuracy: 0.9619\n",
            "Epoch 718/800\n",
            "105/105 [==============================] - 0s 111us/sample - loss: 0.2342 - accuracy: 0.9619\n",
            "Epoch 719/800\n",
            "105/105 [==============================] - 0s 93us/sample - loss: 0.2352 - accuracy: 0.9619\n",
            "Epoch 720/800\n",
            "105/105 [==============================] - 0s 87us/sample - loss: 0.2331 - accuracy: 0.9619\n",
            "Epoch 721/800\n",
            "105/105 [==============================] - 0s 96us/sample - loss: 0.2337 - accuracy: 0.9619\n",
            "Epoch 722/800\n",
            "105/105 [==============================] - 0s 101us/sample - loss: 0.2343 - accuracy: 0.9619\n",
            "Epoch 723/800\n",
            "105/105 [==============================] - 0s 86us/sample - loss: 0.2338 - accuracy: 0.9619\n",
            "Epoch 724/800\n",
            "105/105 [==============================] - 0s 103us/sample - loss: 0.2335 - accuracy: 0.9619\n",
            "Epoch 725/800\n",
            "105/105 [==============================] - 0s 102us/sample - loss: 0.2325 - accuracy: 0.9619\n",
            "Epoch 726/800\n",
            "105/105 [==============================] - 0s 83us/sample - loss: 0.2320 - accuracy: 0.9619\n",
            "Epoch 727/800\n",
            "105/105 [==============================] - 0s 114us/sample - loss: 0.2321 - accuracy: 0.9524\n",
            "Epoch 728/800\n",
            "105/105 [==============================] - 0s 114us/sample - loss: 0.2327 - accuracy: 0.9714\n",
            "Epoch 729/800\n",
            "105/105 [==============================] - 0s 82us/sample - loss: 0.2325 - accuracy: 0.9619\n",
            "Epoch 730/800\n",
            "105/105 [==============================] - 0s 90us/sample - loss: 0.2322 - accuracy: 0.9714\n",
            "Epoch 731/800\n",
            "105/105 [==============================] - 0s 94us/sample - loss: 0.2335 - accuracy: 0.9619\n",
            "Epoch 732/800\n",
            "105/105 [==============================] - 0s 91us/sample - loss: 0.2317 - accuracy: 0.9714\n",
            "Epoch 733/800\n",
            "105/105 [==============================] - 0s 108us/sample - loss: 0.2313 - accuracy: 0.9714\n",
            "Epoch 734/800\n",
            "105/105 [==============================] - 0s 93us/sample - loss: 0.2324 - accuracy: 0.9714\n",
            "Epoch 735/800\n",
            "105/105 [==============================] - 0s 95us/sample - loss: 0.2309 - accuracy: 0.9714\n",
            "Epoch 736/800\n",
            "105/105 [==============================] - 0s 69us/sample - loss: 0.2313 - accuracy: 0.9714\n",
            "Epoch 737/800\n",
            "105/105 [==============================] - 0s 100us/sample - loss: 0.2312 - accuracy: 0.9619\n",
            "Epoch 738/800\n",
            "105/105 [==============================] - 0s 99us/sample - loss: 0.2311 - accuracy: 0.9810\n",
            "Epoch 739/800\n",
            "105/105 [==============================] - 0s 91us/sample - loss: 0.2305 - accuracy: 0.9714\n",
            "Epoch 740/800\n",
            "105/105 [==============================] - 0s 107us/sample - loss: 0.2310 - accuracy: 0.9619\n",
            "Epoch 741/800\n",
            "105/105 [==============================] - 0s 88us/sample - loss: 0.2316 - accuracy: 0.9619\n",
            "Epoch 742/800\n",
            "105/105 [==============================] - 0s 115us/sample - loss: 0.2307 - accuracy: 0.9714\n",
            "Epoch 743/800\n",
            "105/105 [==============================] - 0s 131us/sample - loss: 0.2321 - accuracy: 0.9619\n",
            "Epoch 744/800\n",
            "105/105 [==============================] - 0s 90us/sample - loss: 0.2299 - accuracy: 0.9714\n",
            "Epoch 745/800\n",
            "105/105 [==============================] - 0s 111us/sample - loss: 0.2300 - accuracy: 0.9714\n",
            "Epoch 746/800\n",
            "105/105 [==============================] - 0s 95us/sample - loss: 0.2303 - accuracy: 0.9714\n",
            "Epoch 747/800\n",
            "105/105 [==============================] - 0s 94us/sample - loss: 0.2300 - accuracy: 0.9619\n",
            "Epoch 748/800\n",
            "105/105 [==============================] - 0s 108us/sample - loss: 0.2305 - accuracy: 0.9714\n",
            "Epoch 749/800\n",
            "105/105 [==============================] - 0s 91us/sample - loss: 0.2295 - accuracy: 0.9619\n",
            "Epoch 750/800\n",
            "105/105 [==============================] - 0s 125us/sample - loss: 0.2298 - accuracy: 0.9714\n",
            "Epoch 751/800\n",
            "105/105 [==============================] - 0s 84us/sample - loss: 0.2287 - accuracy: 0.9619\n",
            "Epoch 752/800\n",
            "105/105 [==============================] - 0s 85us/sample - loss: 0.2292 - accuracy: 0.9619\n",
            "Epoch 753/800\n",
            "105/105 [==============================] - 0s 88us/sample - loss: 0.2291 - accuracy: 0.9619\n",
            "Epoch 754/800\n",
            "105/105 [==============================] - 0s 93us/sample - loss: 0.2296 - accuracy: 0.9619\n",
            "Epoch 755/800\n",
            "105/105 [==============================] - 0s 90us/sample - loss: 0.2286 - accuracy: 0.9619\n",
            "Epoch 756/800\n",
            "105/105 [==============================] - 0s 92us/sample - loss: 0.2286 - accuracy: 0.9714\n",
            "Epoch 757/800\n",
            "105/105 [==============================] - 0s 91us/sample - loss: 0.2299 - accuracy: 0.9714\n",
            "Epoch 758/800\n",
            "105/105 [==============================] - 0s 89us/sample - loss: 0.2291 - accuracy: 0.9619\n",
            "Epoch 759/800\n",
            "105/105 [==============================] - 0s 91us/sample - loss: 0.2272 - accuracy: 0.9714\n",
            "Epoch 760/800\n",
            "105/105 [==============================] - 0s 80us/sample - loss: 0.2275 - accuracy: 0.9714\n",
            "Epoch 761/800\n",
            "105/105 [==============================] - 0s 84us/sample - loss: 0.2291 - accuracy: 0.9619\n",
            "Epoch 762/800\n",
            "105/105 [==============================] - 0s 79us/sample - loss: 0.2277 - accuracy: 0.9619\n",
            "Epoch 763/800\n",
            "105/105 [==============================] - 0s 87us/sample - loss: 0.2278 - accuracy: 0.9714\n",
            "Epoch 764/800\n",
            "105/105 [==============================] - 0s 83us/sample - loss: 0.2268 - accuracy: 0.9619\n",
            "Epoch 765/800\n",
            "105/105 [==============================] - 0s 86us/sample - loss: 0.2270 - accuracy: 0.9619\n",
            "Epoch 766/800\n",
            "105/105 [==============================] - 0s 82us/sample - loss: 0.2273 - accuracy: 0.9619\n",
            "Epoch 767/800\n",
            "105/105 [==============================] - 0s 79us/sample - loss: 0.2271 - accuracy: 0.9714\n",
            "Epoch 768/800\n",
            "105/105 [==============================] - 0s 89us/sample - loss: 0.2265 - accuracy: 0.9619\n",
            "Epoch 769/800\n",
            "105/105 [==============================] - 0s 85us/sample - loss: 0.2267 - accuracy: 0.9619\n",
            "Epoch 770/800\n",
            "105/105 [==============================] - 0s 84us/sample - loss: 0.2272 - accuracy: 0.9714\n",
            "Epoch 771/800\n",
            "105/105 [==============================] - 0s 87us/sample - loss: 0.2270 - accuracy: 0.9714\n",
            "Epoch 772/800\n",
            "105/105 [==============================] - 0s 91us/sample - loss: 0.2265 - accuracy: 0.9619\n",
            "Epoch 773/800\n",
            "105/105 [==============================] - 0s 91us/sample - loss: 0.2259 - accuracy: 0.9714\n",
            "Epoch 774/800\n",
            "105/105 [==============================] - 0s 92us/sample - loss: 0.2258 - accuracy: 0.9714\n",
            "Epoch 775/800\n",
            "105/105 [==============================] - 0s 87us/sample - loss: 0.2257 - accuracy: 0.9619\n",
            "Epoch 776/800\n",
            "105/105 [==============================] - 0s 72us/sample - loss: 0.2250 - accuracy: 0.9619\n",
            "Epoch 777/800\n",
            "105/105 [==============================] - 0s 74us/sample - loss: 0.2260 - accuracy: 0.9714\n",
            "Epoch 778/800\n",
            "105/105 [==============================] - 0s 87us/sample - loss: 0.2253 - accuracy: 0.9619\n",
            "Epoch 779/800\n",
            "105/105 [==============================] - 0s 98us/sample - loss: 0.2253 - accuracy: 0.9619\n",
            "Epoch 780/800\n",
            "105/105 [==============================] - 0s 112us/sample - loss: 0.2290 - accuracy: 0.9429\n",
            "Epoch 781/800\n",
            "105/105 [==============================] - 0s 90us/sample - loss: 0.2278 - accuracy: 0.9619\n",
            "Epoch 782/800\n",
            "105/105 [==============================] - 0s 89us/sample - loss: 0.2281 - accuracy: 0.9619\n",
            "Epoch 783/800\n",
            "105/105 [==============================] - 0s 82us/sample - loss: 0.2251 - accuracy: 0.9619\n",
            "Epoch 784/800\n",
            "105/105 [==============================] - 0s 89us/sample - loss: 0.2243 - accuracy: 0.9714\n",
            "Epoch 785/800\n",
            "105/105 [==============================] - 0s 101us/sample - loss: 0.2239 - accuracy: 0.9714\n",
            "Epoch 786/800\n",
            "105/105 [==============================] - 0s 83us/sample - loss: 0.2241 - accuracy: 0.9619\n",
            "Epoch 787/800\n",
            "105/105 [==============================] - 0s 123us/sample - loss: 0.2238 - accuracy: 0.9714\n",
            "Epoch 788/800\n",
            "105/105 [==============================] - 0s 78us/sample - loss: 0.2244 - accuracy: 0.9619\n",
            "Epoch 789/800\n",
            "105/105 [==============================] - 0s 88us/sample - loss: 0.2255 - accuracy: 0.9714\n",
            "Epoch 790/800\n",
            "105/105 [==============================] - 0s 91us/sample - loss: 0.2266 - accuracy: 0.9619\n",
            "Epoch 791/800\n",
            "105/105 [==============================] - 0s 90us/sample - loss: 0.2254 - accuracy: 0.9619\n",
            "Epoch 792/800\n",
            "105/105 [==============================] - 0s 95us/sample - loss: 0.2249 - accuracy: 0.9619\n",
            "Epoch 793/800\n",
            "105/105 [==============================] - 0s 94us/sample - loss: 0.2235 - accuracy: 0.9619\n",
            "Epoch 794/800\n",
            "105/105 [==============================] - 0s 118us/sample - loss: 0.2229 - accuracy: 0.9714\n",
            "Epoch 795/800\n",
            "105/105 [==============================] - 0s 78us/sample - loss: 0.2233 - accuracy: 0.9714\n",
            "Epoch 796/800\n",
            "105/105 [==============================] - 0s 87us/sample - loss: 0.2255 - accuracy: 0.9619\n",
            "Epoch 797/800\n",
            "105/105 [==============================] - 0s 80us/sample - loss: 0.2229 - accuracy: 0.9714\n",
            "Epoch 798/800\n",
            "105/105 [==============================] - 0s 81us/sample - loss: 0.2224 - accuracy: 0.9714\n",
            "Epoch 799/800\n",
            "105/105 [==============================] - 0s 78us/sample - loss: 0.2223 - accuracy: 0.9619\n",
            "Epoch 800/800\n",
            "105/105 [==============================] - 0s 91us/sample - loss: 0.2230 - accuracy: 0.9810\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f64c5b77ef0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vcUGCk63Zbxl",
        "colab_type": "code",
        "outputId": "f0537c3a-4268-4336-e86c-481403271298",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "model.evaluate(x_test, y_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r45/1 [======================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 0s 1ms/sample - loss: 0.1471 - accuracy: 1.0000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.1750778113802274, 1.0]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OQ8WiUqkONkt",
        "colab_type": "text"
      },
      "source": [
        "# Pansas dummy variable"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_1u4ysI7OLrS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import sklearn\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lneHeF11OgR9",
        "colab_type": "code",
        "outputId": "39e1f021-5de9-4bad-8749-748a04795e97",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        }
      },
      "source": [
        "d = {'col1': [1, 2], 'col2': [3, 4],  'col3': [\"Jerry\", \"Bill\"]}\n",
        "df = pd.DataFrame(data=d)\n",
        "df"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>col1</th>\n",
              "      <th>col2</th>\n",
              "      <th>col3</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>Jerry</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>Bill</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   col1  col2   col3\n",
              "0     1     3  Jerry\n",
              "1     2     4   Bill"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8T94M_DJOsQy",
        "colab_type": "code",
        "outputId": "84133ce1-990c-4e77-c497-d68c96fa4215",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        }
      },
      "source": [
        "#get_dummies\n",
        "#適合有順序尺度\n",
        "dummies=pd.get_dummies(df[\"col3\"])\n",
        "dummies"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Bill</th>\n",
              "      <th>Jerry</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Bill  Jerry\n",
              "0     0      1\n",
              "1     1      0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QNKRJepaPJD3",
        "colab_type": "code",
        "outputId": "8311e2a2-0820-4f3d-ecdd-f70dcfa98f2b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        }
      },
      "source": [
        "pd.concat([df,dummies],axis='columns')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>col1</th>\n",
              "      <th>col2</th>\n",
              "      <th>col3</th>\n",
              "      <th>Bill</th>\n",
              "      <th>Jerry</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>Jerry</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>Bill</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   col1  col2   col3  Bill  Jerry\n",
              "0     1     3  Jerry     0      1\n",
              "1     2     4   Bill     1      0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ujEs-iVQsBv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "Label = LabelEncoder()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "55phb4V7RHF_",
        "colab_type": "code",
        "outputId": "a684b308-4cfe-493b-f8f8-3eee112e3476",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "Label.fit_transform(df.col3)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CoBCJIbkRVhq",
        "colab_type": "code",
        "outputId": "d6c6b04b-bc04-4775-d165-65c4ddb4d8f3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171
        }
      },
      "source": [
        "#OneHotEncoder (針對數字)\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "onehot = OneHotEncoder(categorical_features=[0])\n",
        "X = df[[\"col1\",\"col2\"]].values\n",
        "onehot.fit_transform(X).toarray()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
            "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
            "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_encoders.py:451: DeprecationWarning: The 'categorical_features' keyword is deprecated in version 0.20 and will be removed in 0.22. You can use the ColumnTransformer instead.\n",
            "  \"use the ColumnTransformer instead.\", DeprecationWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1., 0., 3.],\n",
              "       [0., 1., 4.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hIS22REaVQzL",
        "colab_type": "code",
        "outputId": "ef366f61-c1fc-42b9-8382-aa4e609011b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "#LabelEncoder()-> OneHotEncoder() \n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "encoder = LabelBinarizer()\n",
        "X = df[[\"col3\"]].values\n",
        "df_cat_1hot = encoder.fit_transform(X)\n",
        "df_cat_1hot"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1],\n",
              "       [0]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r3dfVgLq52MF",
        "colab_type": "text"
      },
      "source": [
        "# One-Hot"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ze2imk-J6x_k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#適合名目尺度\n",
        "#    cat mat on sat the\n",
        "#the 0   0   0  0   1\n",
        "#cat 1   0   0  0   0\n",
        "#sat 0   0   0  1   0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SX8-BnJvdMur",
        "colab_type": "code",
        "outputId": "69dc5271-4cd8-4bce-fffa-0d36f27abc3b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "indices = [0, 1, 2]\n",
        "depth = 5\n",
        "tf.one_hot(indices, depth)  # output: [3 x 3]\n",
        "# [[1., 0., 0.],\n",
        "#  [0., 1., 0.],\n",
        "#  [0., 0., 1.]]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor 'one_hot_1:0' shape=(3, 5) dtype=float32>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_2zgRRbH5-PG",
        "colab_type": "code",
        "outputId": "626e1262-2ba1-4e32-8fd6-ab66e0477ac3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        }
      },
      "source": [
        "indices = [0, 2, -1, 1]\n",
        "depth = 4\n",
        "tf.one_hot(indices, depth,\n",
        "           on_value=5.0, off_value=0.0,\n",
        "           axis=-1)  # output: [4 x 3]\n",
        "# [[5.0, 0.0, 0.0],  # one_hot(0)\n",
        "#  [0.0, 0.0, 5.0],  # one_hot(2)\n",
        "#  [0.0, 0.0, 0.0],  # one_hot(-1)\n",
        "#  [0.0, 5.0, 0.0]]  # one_hot(1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: id=50145, shape=(4, 4), dtype=float32, numpy=\n",
              "array([[5., 0., 0., 0.],\n",
              "       [0., 0., 5., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 5., 0., 0.]], dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JxFUdwFK5-1T",
        "colab_type": "code",
        "outputId": "198cd825-3f36-4222-dd06-6e7e2b533203",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        }
      },
      "source": [
        "indices = [[0, 2], [1, -1]]\n",
        "depth = 3\n",
        "tf.one_hot(indices, depth,\n",
        "           on_value=1.0, off_value=0.0,\n",
        "           axis=-1)  # output: [2 x 2 x 3]\n",
        "# [[[1.0, 0.0, 0.0],   # one_hot(0)\n",
        "#   [0.0, 0.0, 1.0]],  # one_hot(2)\n",
        "#  [[0.0, 1.0, 0.0],   # one_hot(1)\n",
        "#   [0.0, 0.0, 0.0]]]  # one_hot(-1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: id=50069, shape=(2, 2, 3), dtype=float32, numpy=\n",
              "array([[[1., 0., 0.],\n",
              "        [0., 0., 1.]],\n",
              "\n",
              "       [[0., 1., 0.],\n",
              "        [0., 0., 0.]]], dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_dkWxDfo-f4V",
        "colab_type": "text"
      },
      "source": [
        "# Numeric column"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mPGP6oOF6E5Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "numeric_feature_column = tf.feature_column.numeric_column(key=\"SepalLength\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JCQ0il_U9mQd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "numeric_feature_column = tf.feature_column.numeric_column(key=\"SepalLength\",\n",
        "                                                          dtype=tf.float64)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gw6gad6c93E3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vector_feature_column = tf.feature_column.numeric_column(key=\"Jerry\",\n",
        "                                                         shape=10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IQSYYgpj97Im",
        "colab_type": "code",
        "outputId": "88fe553e-5586-4784-f17f-84cafcfecfb6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(vector_feature_column)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NumericColumn(key='Jerry', shape=(10,), default_value=None, dtype=tf.float32, normalizer_fn=None)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ytrs_J7D-EQW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "matrix_feature_column = tf.feature_column.numeric_column(key=\"MyMatrix\",\n",
        "                                                         shape=[10,5])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LnqwILSS-din",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vector_feature_column2 = tf.feature_column.numeric_column(key=\"Terry\",\n",
        "                                                         shape=10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VcGFz8rW_u8u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##https://www.tensorflow.org/guide/feature_columns"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}